{"category":"closed_qa","context":"","instruction":"Databricks でどんな問題を解決できますか？","response":"ビジネスでは、全てのデータに金塊が含まれていることが分かっています。チームの仕事はそれを見つけ出すことです。しかし、扱いにくいツールや設定の難しいインフラに囲まれている状態では、その仕事をうまくこなすことはできません。本当はビジネスで現在何が起こっているのかを解明してヒーローになりたいのに、時間はツールとの格闘にばかり費やされています。\n Databricks はビッグデータを簡素化するために構築されました。Apache Spark™ は、データパイプラインを構築するための統合フレームワークを提供することで、この使命を達成するための大きな一歩を踏み出しました。Databricksは、Spark を中心に構築されたゼロ管理クラウドプラットフォームを提供することで、これをさらに推進します。Spark では、1) 完全マネージド型の Spark クラスタ、2) 調査および視覚化のための対話型ワークスペース、3) 運用パイプラインスケジューラー、4) お使いの Spark ベースのアプリケーションをk強化するプラットフォームを提供しています。もうデータの追跡に頭を悩ませる必要はありません。これからはビジネスに直結する知見を得るために注力できるようになります。"}
{"category":"closed_qa","context":"","instruction":"Databricks に適した利用者とは？","response":"データサイエンティストやエンジニアから、開発者やデータアナリストに至るまで、ビッグデータから迅速かつ効率的に価値を引き出したいユーザーに最適です。対話型のワークスペースの提供により、Spark のネイティブな R、Scala、Python、SQL、インターフェースや、リモートプログラムアクセス用 REST API、オフラインで開発された任意の Spark ジョブを実行する機能、BI やドメイン固有のツールといったサードパーティ製アプリケーションに対するシームレスなサポートが可能になりました。このように Databricks により、ユーザーは、最も使いやすいインターフェースを通じてデータおよび情報を活用できます。"}
{"category":"closed_qa","context":"","instruction":"Databricks はどのように使用されていますか？","response":"Databricks は、金融サービス、医療、小売、メディア＆エンターテインメント、公益事業など、さまざまな分野の企業で使用されています。コア ETL、データの発見と深掘り、データウェアハウス、データ製品のデプロイ、ダッシュボードを使用した社内外向けの情報の公開など、幅広いユースケースで、当社のプラットフォームをご利用いただいています。"}
{"category":"closed_qa","context":"","instruction":"Databricks を使用することで、企業は Apache Spark を容易に導入できるようになりますか？","response":"そのとおりです。企業は大量のデータを蓄積していますが、ビッグデータの分析プロセスには、インフラ管理、プロビジョニング上のボトルネック、取得や管理にかかる高額のコストなど、多くの障壁が存在します。Databricks は、これら全ての障害を取り除くように設計されています。私たちはビッグデータを企業にとって扱いやすいものにし、ビジネスアプリケーションの Excel のように、一般的な存在にしたいと考えています。"}
{"category":"closed_qa","context":"","instruction":"Databricks と Apache Spark はどのように関係していますか？","response":"Databricks は、バークレー大学で Spark の研究プロジェクトを開始したチームによって開発され、後に Apache Spark™ になりました。Databricks は、オープンソースコミュニティと連携してプロジェクトの拡大を続けています。当社は、他のどの企業よりも多くのコードを Spark に提供してきました。また、Spark の開発者、システムインテグレーター、アプリケーション、ディストリビューター、トレーナー向けに、Databricks 認定プログラムも提供しています。さらに当社では、データサイエンス、エンジニアリング、ビジネスを統合してイノベーションを加速する、統合分析プラットフォームの Databricks も開発しました。"}
{"category":"closed_qa","context":"","instruction":"Databricks の料金設定を教えてください。","response":"Databricks の料金については、料金ページでご確認ください。https://www.databricks.com/jp/product/pricing"}
{"category":"closed_qa","context":"","instruction":"Databricks は、現在利用可能ですか？","response":"はい、利用可能です。現在、業種やユースケースを問わず多くの企業が Databricks を活用して、大規模な運用ジョブを実行しています。こちらから開始してください。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark アプリケーションの開発者が Databricks を利用することはできますか？","response":"はい。Databricks なら、Apache Spark アプリケーションの開発、テスト、およびデプロイを簡単に実施できます。また ODBC/JDBC 接続、標準の Spark API、およびサードパーティ製アプリケーション用のネイティブ REST API が提供されています。"}
{"category":"closed_qa","context":"","instruction":"Databricks にデータを取り込むにはどうすればよいですか？","response":"Databricks は現在、ブラウザベースのファイルアップロード、Azure Blob Storage、AWS S3、Azure SQL Data Warehouse、Azure Data Lake Store、Cosmos DB などの NoSQL データストア、Cassandra、Elasticsearch 、JDBC データソース、HDFS、Sqoop、および Apache Spark によってネイティブにサポートされている他のさまざまなデータソースをサポートしています。"}
{"category":"closed_qa","context":"","instruction":"Databricks には、どのようなセキュリティ機能が含まれていますか？","response":"セキュリティと耐障害性は Databricks にとって最優先事項であり、Databricks の製品は、適切な認証および隔離メカニズムによってゼロから構築されています。詳しくは、セキュリティページをご覧ください。https://www.databricks.com/jp/trust"}
{"category":"closed_qa","context":"","instruction":"Databricks は、他の Apache Spark との相互運用が可能ですか？","response":"Databricks は 100％ Apache Spark を実行しています。そのため、そのうえで開発されたコードやアプリケーションは、Apache Spark に対応する任意のディストリビューション (例: Databricks 認定ディストリビューション) で実行可能です。"}
{"category":"closed_qa","context":"","instruction":"Databricks は、他のクラウドプラットフォーム上で利用できますか。","response":"現在、Databricks は Microsoft Azure、Amazon AWS、Google Cloud で利用可能です。"}
{"category":"closed_qa","context":"","instruction":"Databricks を自分のアカウントに導入する際、隔離メカニズムは提供されますか。","response":"はい。Databricks は各アカウントの VPC に全てがデプロイされ、セキュリティと隔離の追加レイヤーが提供されます。"}
{"category":"closed_qa","context":"","instruction":"Databricks を自社のデータセンターやクラスターで実行できますか？","response":"現在のところ、実行できません。ただし、Databricks では他のデプロイメントシナリオも継続的に研究しており、そこにはオンプレミスのクラスターも含まれています。"}
{"category":"closed_qa","context":"","instruction":"Databricks はユーザーのデータを保持しますか？","response":"Databricks のユーザーは、それぞれの認証情報を使用して、自分のデータストアからのデータの読み取りやデータストアへのデータの保持を実施します。"}
{"category":"closed_qa","context":"","instruction":"データを Databricks の AWS アカウントに転送する必要がありますか？","response":"いいえ、その必要はありません。ほとんどの場合、現在のデータソースからデータにアクセスできます。"}
{"category":"closed_qa","context":"","instruction":"私のデータやノートブックにアクセスできるのは誰ですか？","response":"Databricks アカウントにユーザーを追加することで、データやノートブックへのアクセスを管理できます。Databricks アカウントに追加されると、プラットフォームにアクセスできるようになります。"}
{"category":"closed_qa","context":"","instruction":"Databricks のセキュリティの詳細については、どこで確認できますか？","response":"詳しい情報は、セキュリティページをご覧ください。https://www.databricks.com/jp/trust"}
{"category":"closed_qa","context":"","instruction":"顧客データに対するクラウドプロバイダーの権利を教えてください。","response":"Databricks のお客さまは、データの制御と所有の権利を有しています。詳しくは、Databricks のサービスチームにお問い合わせください。またプライバシーポリシーもご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricks は、業界のどのようなセキュリティ管理やプラクティスに従っていますか。","response":"Databricks は、既に業界のベストプラクティスに基づいて独自のセキュリティアーキテクチャを実装しています。さらに SANS のインターネットセキュリティ対策トップ 20、コンセンサス監査ガイドライン、NIST ガイドライン、そしてインターネット標準といった、より高い基準を達成するよう継続的に取り組んでいます。また Databricks では、Databricks の完全性に影響を及ぼす可能性があるアプリケーションまたはネットワークレベルのセキュリティ問題を特定する、セキュリティ機能を標準で保持しています。"}
{"category":"closed_qa","context":"","instruction":"Databricks には、事業継続計画が用意されていますか？","response":"AWS は、事業継続プログラム（AWS 事業継続とディザスタリカバリ）を提供しています。Databricks は、複数の地域、複数のアベイラビリティーゾーン、あるいは複数のデータセンターで実行できるように設計されています。"}
{"category":"closed_qa","context":"","instruction":"Databricks のセキュリティについて詳しく知る方法はありますか？","response":"Databricks では、包括的なエンタープライズセキュリティを提供しています。また、sales@databricks.com 宛てにメールでお問い合わせいただくこともできます。"}
{"category":"closed_qa","context":"","instruction":"ゲノミクスとは？","response":"分析に関する遺伝学の一分野です。その主な役割は、DNA のシーケンス全体、または DNA を構成する原子の組成、および DNA 原子間の化学結合を決定することです。ゲノミクスの分野は、全体構造としてのゲノムに重点を置いており、生物の完全な遺伝物質の研究として定義することができます。DNA は 1869 年に初めて単離されましたが、ゲノミクスは、科学者が単純な生物の DNA シーケンスを決定した 1970 年代に始まったばかりです。ゲノミクスの分野で最大のブレークスルーは、自由生活性有機体であるインフルエンザ菌の全ゲノムシーケンスが完成した 1995 年に起こりました。ヒトゲノム計画は、ヒトゲノムの 30 億文字全てをシーケンシングすることを目的として 1990 年に開始されました。染色体 22 は、1999 年にこの計画の一環としてシーケンシングされた最初の染色体です。国際ヒトゲノム配列コンソーシアムは、2001 年 2 月にヒトゲノムの最初の草稿を学術誌ネイチャーに発表し、ゲノム全体の 30億塩基対のシーケンスの約 90% を完了しました。全てのシーケンシングは 2003 年 4 月に完了し、公開されました。進化するゲノミクスの分野には、一般的に次に挙げるいくつかの重要な研究分野が含まれています。"}
{"category":"closed_qa","context":"","instruction":"デジタルツインとは？","response":"IBM によると、デジタルツインの従来の定義は、「物理オブジェクトを正確に反映するように設計された仮想モデル」です。デジタルツインは、離散的または連続的な製造プロセスにおいて、さまざまな IoT センサー（OT：運用技術データ）やエンタープライズデータ（IT：情報技術）を用いてシステムやプロセスの状態データを収集し、仮想モデルを形成します。このモデルは、シミュレーションの実行、性能の問題の調査、知見の抽出に使用できます。\n デジタルツインの概念は、特に新しいものではありません。実際、最初の申請は、25 年以上前に行われました。ロンドンにあるヒースローエクスプレスの鉄道施設における基礎工事とコファダム工事の初期段階で、基礎のボアホールグラウティングを監視・予測するためのものであったと報告されています。この最初の申請から数年後、エッジコンピューティングや AI、データ接続性、5G 通信、モノのインターネット（IoT）の改善や機能向上により、デジタルツインは費用対効果が高い手段となり、現在のデータドリブンなビジネス環境において必須となっています。"}
{"category":"closed_qa","context":"","instruction":"データセットとは？","response":"Datasetとは、Java および Scala 用のタイプセーフなSparkの構造化APIです。Python および R は動的型付け言語であるため、この API の使用はできませんが、Scala や Java で大規模なアプリケーションを作成するためには強力なツールです。DataFrame は、Row 型のオブジェクトの分散型コレクションであり、さまざまなタイプの表形式データを保持できます。Dataset API を使用すると、データフレーム内のレコードに Java クラスを割り当て、Java の ArrayList や Scala の Seq と同様に、型付きオブジェクトのコレクションとして処理できます。Dataset で使用できるAPIはタイプセーフで、Dataset 内のオブジェクトを、最初に配置したクラスとは別のクラスのものとして誤って表示することはありません。このため、明確に定義されたインターフェースを介してやりとりする必要のある大規模なアプリケーションを作成する場合、Dataset はソフトウェアエンジニアにとって魅力的です。Dataset クラスは、内部に含まれるオブジェクトの型でパラメータ化されます。Java では Dataset、Scala では Dataset[T]です。Spark 2.0では、サポートされるT型は Java の JavaBean パターンに従う全てのクラスと、Scala のケースクラスで、Spark が自動的にT型を分析し、Dataset内の表形式データに適切なスキーマを作成する必要があるため、これらの型は制限されます。"}
{"category":"closed_qa","context":"","instruction":"データボルトとは？","response":"Data Vault（データボルト）とは、データモデリングのデザインパターンで、エンタープライズ規模の分析向けのデータウェアハウスを構築する際に使用されます。データボルトには、ハブ、リンク、サテライトの 3 種類のエンティティがあります。ハブは、ビジネスの中核となるコンセプトを、リンクは、ハブ間のリレーションシップを表します。サテライトは、ハブに属する情報やハブ間のリレーションシップに関するデータを格納します。データボルトは、レイクハウスのパラダイムを採用する組織に適したデータモデルです。"}
{"category":"closed_qa","context":"","instruction":"データボルトのメリットは何ですか？","response":"データボルトに様々なメリットがあります。1. アジャイル, 2. 構造化され、リファクタリングに柔軟に対応, 3. ペタバイト規模にも対応する優れたスケーラビリティ, 4. ETL コード生成をサポートするパターンを使用, 5. データレイヤー、ETL、スタースキーマなど、現行のアーキテクチャを使用可能。\n データボルトは、アジャイルな方法と技術に基づいているため、急速に変化するビジネス要件に適応できます。データボルトの手法を利用する大きなメリットは、モデルが変更された場合に、ETL ジョブのリファクタリングが少なくて済むことです。"}
{"category":"closed_qa","context":"","instruction":"データマートとは？","response":"データマートは、テーブルのセットを含むキュレートされたデータベースです。単一のデータチームやコミュニティ、マーケティングやエンジニアリング部門といった基幹業務の特定のニーズに対応できるよう設計されています。データマートは通常、データウェアハウスよりも小規模で、特定の目的に特化しています。一般的には、大規模なエンタープライズのデータウェアハウスのサブセットとして扱われ、分析や BI（ビジネスインテリジェンス）、レポーティングに使用されます。データマートは、中央データウェアハウスとデータレイクの物理的現実における最初の進化的ステップでした。エーシーニールセンは、1970 年代初頭に、情報をデジタル保存し販促活動を促進するための最初のデータマートを顧客に提供しました。"}
{"category":"closed_qa","context":"","instruction":"データマートの特徴は何？","response":"データマートの特徴は以下６点あります。1. 通常、データマートの構築や管理は企業のデータチームが行いますが、各事業部署の専門家が組織的に行うことも可能です。2. 企業のデータスチュワードがデータマートを管理し、エンドユーザーは読み取り専用の権限が設定されます。これは、技術に精通していないユーザーによる重要なビジネスデータの誤った削除、変更を防ぐためです。3. 一般的には、ディメンションモデルとスタースキーマが使用されます。4. データマートは、大規模なデータウェアハウスからキュレートされたデータのサブセットを含んでいます。データは高度に構造化され、データチームによってデータクレンジングと適合化が施されて容易に理解、参照できるようになっています。5. データマートは、特定の基幹業務やユースケースに特有のニーズにあわせて設計されています。6. 基本的に、ユーザーは、SQL コマンドを使用してデータを参照します。"}
{"category":"closed_qa","context":"","instruction":"データマートは何種類がありますか？","response":"データマートには、基本的に 3 つの種類があります。1. 独立型データマート：データウェアハウスの一部ではなく、エーシーニールセンが提供した最初のデータマートと非常によく似ています。通常、1 つの領域または対象領域に特化しています。データソースには、外部ソースと内部ソースの両方を含めることができます。データは変換、処理、データマートにロードされ、必要になるまで保存されます。\n 2. 従属型データマート：既存のデータウェアハウスに従属する形で存在するデータマートです。トップダウンのアプローチを採用し、あらゆるデータを一元的に保存できます。何かしらの目的でデータを使用する際は、明確に定義されたデータのセクションを選択できます。\n 3. ハイブリッド型データマート：データウェアハウスだけではなく、それ以外の独自のデータソースから取得したデータを組み合わせて使用します。これは、組織に追加された新たなグループや製品をアドホックに統合する場合など、さまざまな状況で役立ちます。ハイブリッド型データマートは、複数のデータベースが存在する環境に適しており、短期間で導入できます。このようなシステムはデータクレンジングを容易にし、小規模なデータセントリックのアプリケーションと相性が良いとされています。"}
{"category":"closed_qa","context":"","instruction":"データマートのメリットは何ですか？","response":"データマートのメリットは、基本的に２つあります。1. 信頼できる唯一の情報源（SSOT）：データマートは、特定の基幹業務の単一のデータソースとして機能します。業務に関わる全ての人が同一の事実とデータに基づいて作業することを可能にします。2. 業務効率化をサポートするシンプルさ：ユーザーは、必要なデータを見つけるためにデータウェアハウス全体を調べたりテーブルを結合したりする必要はなく、あらかじめ精選されたデータマートで、必要なデータに容易にアクセスできます。"}
{"category":"closed_qa","context":"","instruction":"データマートにおける課題は何ですか？","response":"エンタープライズデータウェアハウスは、その企業における全てのデータ管理ニーズに対応するために善意で作成されています。しかし、各事業部署によってデータのニーズや目的が異なるため、あらゆるユーザーのニーズを満たすことはできません。そのため、各部門ではセルフサービスでの分析や、部門のレポーティングのニーズを満たすべく、時には IT 部門のサポートを受けながら、特定のデータウェアハウスの対象領域を拡張し、データをコピーして、独自のデータマートを作成しています。その結果、各部門のニーズを満たしたとしても、時間が経つにつれて、データのサイロ化やシャドーコピーの原因になります。また、多くの部署でこのようなことが行われている場合、組織内で信頼できる単一のデータソースが存在しなくなってしまいます。"}
{"category":"closed_qa","context":"","instruction":"レイクハウスはデータマートの課題をどのように解決するか","response":"レイクハウスは、企業内に存在するデータウェアハウスとデータマートを単一のプラットフォーム上に設置し、セキュリティとガバナンスを統一することで、データマートの課題を解決しています。同時に、各チームが独自システムを持つことができる柔軟な環境を提供します。また、データマートや「拡張コピー」は、他の全てのデータと同じレイクハウスプラットフォーム上で作成されるため、類似したコピーの複製を防止できます。レイクハウスのデータカタログにより、重複コピーを発見し、タグ付けやデータ辞書の使用などのデータガバナンスルールを使用して、誰もが拡張コピーを発見できるようになります。"}
{"category":"closed_qa","context":"","instruction":"データ共有とは？","response":"データ共有とは、同じデータを複数のユーザーで利用できるようにすることです。増加し続けるデータは、あらゆる企業にとって重要な戦略的資産です。組織内外におけるデータ共有は、新たなビジネスチャンスを生み出すカギとなる技術です。外部データを利用するだけでなく、データを共有することで、パートナーとのコラボレーション、新たなパートナーシップの確立、データのマネタイズによる新たな収益源の確保が可能になります。"}
{"category":"closed_qa","context":"","instruction":"Delta Sharing","response":"Delta Sharing は、セキュアなデータ共有のための世界初のオープンプロトコルです。使用しているコンピューティングプラットフォームを問わず、組織内外でのデータ共有を容易にします。\n・ライブデータを直接共有：Delta Lake にある既存のライブデータを、他のシステムにコピーすることなく容易に共有できます。\n・さまざまなクライアントに対応：データ受信者は、特定のコンピューティングプラットフォームを事前導入しなくても、Pandas、Apache Spark™、Rust、その他のシステムから Delta Sharing に直接アクセスできるため、データ送信における摩擦が軽減されます。\n ・セキュリティとガバナンス： Delta Sharing では、共有データセットへのアクセスの管理、追跡、監査を容易に行うことができます。\n ・スケーラビリティ：S3、ADLS、GCS などのクラウドストレージシステムを活用することで、大規模なデータセットを確実かつ効率的に共有できます。"}
{"category":"closed_qa","context":"","instruction":"メダリオンアーキテクチャとは？","response":"メダリオンアーキテクチャとは、レイクハウスのデータを論理的に整理するために用いられるデータ設計を意味します。データがアーキテクチャの 3 つのレイヤー（ブロンズ → シルバー → ゴールドのテーブル）を流れる際に、データの構造と品質を増分的かつ漸次的に向上させることを目的としています。メダリオンアーキテクチャは、「マルチホップ」アーキテクチャとも呼ばれます。"}
{"category":"closed_qa","context":"","instruction":"レイクハウスとは？","response":"レイクハウスは、データレイクとデータウェアハウスの優れた要素を取り入れた新しいデータプラットフォームアーキテクチャです。モダンレイクハウスは、拡張性と性能に優れたデータプラットフォームです。未加工データと準備済みデータの両方をホストし、ビジネスにおける迅速な消費、高度な知見や意思決定を促進します。データサイロを解消し、企業全体の認証済ユーザーによる、単一プラットフォームでのシームレスかつセキュアなデータアクセスを可能にします。"}
{"category":"closed_qa","context":"","instruction":"レイクハウスアーキテクチャのメリットは何？","response":"レイクハウスアーキテクチャのメリットはたくさんあります。1. シンプルなデータモデル, 2. わかりやすく、導入が容易, 3. 増分 ETL が可能, 4. 未加工データからのテーブル再作成がいつでも可能, 5. ACID トランザクション、タイムトラベル"}
{"category":"closed_qa","context":"","instruction":"ブロンズレイヤー（未加工データ）は何？","response":"ブロンズレイヤーでは、外部ソースシステムからのあらゆるデータを格納します。ブロンズレイヤーのテーブル構造は、ロード日時、プロセス ID などの付加的な行のメタデータに加えて、ソースシステムの「そのまま」のテーブル構造に対応します。このレイヤーは、変更データの迅速な取得、ソースの履歴アーカイブ（コールドストレージ）、データリネージ、監査性の機能を備えており、ソースシステムからデータの再読み込みなしで、必要に応じた再処理を行います。"}
{"category":"closed_qa","context":"","instruction":"シルバーレイヤー（クレンジング・適合済みデータ）は何？","response":"レイクハウスのシルバーレイヤーでは、ブロンズレイヤーのデータをマッチング、マージ、フィルタリング、クレンジング（適度なレベル）し、あらゆる主要なビジネスエンティティ、コンセプト、トランザクションの「エンタープライズビュー」を提供します。（顧客マスターテーブル、店舗マスターテーブル、重複を排除したトランザクションテーブル、相互参照テーブルなど）\n シルバーレイヤーでは、異なるソースからのデータをエンタープライズビューに取り込み、アドホックレポート、高度な分析、機械学習などのセルフサービス分析を可能にします。シルバーレイヤーのデータは、アナリスト、データエンジニア、データサイエンティストによるプロジェクトや分析のソースとなり、ゴールドレイヤーの企業や部門のデータプロジェクトにおいて、ビジネス上の課題の解決を支援します。\n レイクハウスのデータエンジニアリングでは、一般的に、ETL ではなく ELT 処理が行われます。これは、シルバーレイヤーにデータがロードされる際に、最小限、あるいは「適度な」変換およびデータクレンジングルールのみが適用されることを意味します。データレイクでのデータの取り込みや供給における迅速性と俊敏性が優先され、シルバーレイヤーからゴールドレイヤーにデータがロードされる際には、プロジェクト特有の複雑な変換やビジネスルールが多く適用されます。データモデリングに関しては、シルバーレイヤーは第 3 正規形に近いデータモデルを備えています。このレイヤーでは、Data Vault のような書き込み可能なデータモデルを使用できます。"}
{"category":"closed_qa","context":"","instruction":"ゴールドレイヤー（ビジネスレベルのキュレート済みテーブル）は何？","response":"レイクハウスのゴールドレイヤーにあるデータは、通常、消費可能な「プロジェクト専用」データベースに整理されています。ゴールドレイヤーはレポート作成に適しています。結合が少なく、より非正規化され、読み取りが最適化されたデータモデルを使用しています。データ変換とデータ品質ルールの最終レイヤーです。顧客分析、製品品質分析、インベントリ分析、顧客セグメンテーション、商品推薦、マーキング／販売分析などのプロジェクトにおける最終プレゼンテーションレイヤーです。多くのキンボール式スタースキーマベースのデータモデル、またはインモン式データマートは、レイクハウスのこのゴールドレイヤーに適合します。\n このように、データはレイクハウスの異なるレイヤーを移動する際に、キュレートされます。従来の RDBMS テクノロジースタックのデータマートや EDW をレイクハウスに取り込み、企業における初の「汎 EDW」の高度な分析や機械学習を可能にするケースもあります。従来のスタックでは不可能、あるいはコストがかかりすぎるため、実現されていなかったことです。（IoT／製造業データと営業・マーケティングデータを連携させ、欠陥分析やヘルスケアゲノミクス解析を行う、EMR/HL7 臨床データ市場と金融債権データの連携によりヘルスケアデータレイクを構築してタイムリーな患者ケアの分析や改善を行うなど）。"}
{"category":"closed_qa","context":"","instruction":"メダリオンアーキテクチャとデータメッシュの互換性がありますか？","response":"はい、メダリオンアーキテクチャは、データメッシュのコンセプトと互換性があります。ブロンズとシルバーのテーブルは「1 対多」方式で結合できるため、1 つのアップストリームテーブルのデータを使用して、複数のダウンストリームテーブルを生成できます。"}
{"category":"closed_qa","context":"","instruction":"金融サービスのパーソナライズとは？","response":"金融商品やサービスのコモディティ化が進み、メディアや小売業界がパーソナライズされた体験を好むようになったことで、消費者の目は肥えてきています。消費者から求められるものが日々変化していく中で、銀行がこれからも必要とされ続けるためには、パーソナライズされた知見やレコメンド、財務目標の設定、レポート機能といった従来の銀行業務を超えた魅力的な銀行体験を提供する必要があり、これらは全て地理空間や自然言語処理（NLP）などの高度な分析機能によって実現されます。金融サービスのパーソナライズは、オープンファイナンスとも呼ばれ、データ共有の原則に基づき、銀行が顧客のニーズに特化した幅広い可能性を提供できるようにするものです。金融サービスのパーソナライズは、オープンバンキング（次節参照）の標準採用と、世界中で変わり続ける規制によって実現されます。"}
{"category":"closed_qa","context":"","instruction":"データベースやデータストレージシステムにおけるトランザクションとは？","response":"データベースやデータストレージシステムにおけるトランザクションとは、1 つの作業単位として扱われるあらゆる操作のことです。トランザクションは、完全に実行される、もしくは全く実行されないかのいずれかで、ストレージシステムを一貫した状態に保ちます。トランザクションの典型的な例として、銀行の預金口座から現金を引き出す処理が挙げられます。この場合、預金口座から現金を引き出したか、もしくは全く引き出さなかったか、どちらかの処理が発生し、中間の状態はありません。"}
{"category":"closed_qa","context":"","instruction":"ACID とは？","response":"ACID とは、トランザクションを定義する4つの重要な特性、Atomicity（原子性）、Consistency（一貫性）、Isolation（独立性）、Durability（永続性）の頭文字をとった略語です。データベース操作にこれらの ACID 特性がある場合は、ACID トランザクションと呼ぶことができます。また、これらの操作を適用したデータストレージシステムは、トランザクションシステムと呼ばれます。ACID トランザクションにより、テーブルの読み取り、書き込み、変更の各処理で次の特性が保証されます。\n ・原子性（A）：トランザクションの各ステートメント（データの読み取り、書き込み、更新、削除）は、1 つの単位として扱われます。ステートメントは、実行されるか、あるいは全く実行されないかのいずれかです。この特性は、ストリーミングデータソースがストリーミングの途中で障害が発生した場合などに、データの損失や破損が発生するのを防ぎます。\n ・一貫性（C）：トランザクションがテーブルに、事前定義された予測可能な方法でのみ変更を加えることを保証します。トランザクションの一貫性により、データの破損やエラーが起きた場合でもテーブルの整合性を保ち、意図しない実行結果を防ぎます。\n ・独立性（I）：複数のユーザーが同じテーブルで読み書きを同時に実行しても、トランザクションが分離され、同時進行のトランザクションが相互に干渉したり、影響を受けたりしないようにします。実際は、同時に発生していも、各要求は単独で発生しているように扱われます。\n ・永続性（D）：システム障害が発生した場合でも、正常に実行されたトランザクションによるデータの変更が保存されることを保証します。"}
{"category":"closed_qa","context":"","instruction":"ACID トランザクションによるメリットは何ですか？","response":"ACID トランザクションは、データの信頼性と整合性を最大限に高めます。部分的にしか操作を完了できなかった場合でも、データの一貫性を保つことができます。例えば、データベースのテーブルへの書き込み中に、予期せず電源が切れた場合、ACID トランザクションがなければ、データは一部のみ保存され、残りは保存されない可能性があります。その結果、データベースの一貫性は失われ、復旧は困難で時間のかかるものとなります。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeはデータレイクにどういう特性を提供します？","response":"ACID トランザクションは、これまでデータウェアハウスのみが持つ特性でしたが、Delta Lakeは、データレイクにACID トランザクションを提供します。これにより、新たなデータがテーブルに書き込まれても、ユーザーは、データの一貫性のあるリアルタイムなビューを見ることができます。各書き込みは、順序付けられたトランザクションログに記録される独立したトランザクションとなるためです。Delta Lake は、最高レベルの独立性（直列化可能な独立性）を採用し、1 つのテーブルでの読み取り／書き込みの一貫性と信頼性を確保しています。ACID トランザクションの実装により、Delta Lake は、ラムダアーキテクチャの問題である複雑さ、データの誤表示、ラムダパイプラインが壊れた後に必要となるリワークや再処理などを効果的に解決します。また、ユーザーは、データに対して複数の同時トランザクションを実行できます。データソースやストリームでエラーが発生した場合でも、Delta Lake は、トランザクションの実行を中止し、データをクリーンで元の状態に保つことを保証します。ACID トランザクションによる大きなメリットは、ユーザーが Delta Lake のデータを信頼できることです。データアナリストは、ダッシュボードの準備の ETL 実行に Delta Lake を使用することで、データアナリストが見ている KPI が、データの実際の状況を表していることを信用できます。機械学習のエンジニアは、特徴量エンジニアリングに Delta Lake のテーブルを利用することで、変換や集約が意図したとおりに実行できているか、または全く実行できなかったかを確信できます（その場合、通知を受けます）。データに対して持っているメンタルモデルが、実際のデータに反映していると信頼できる価値は計り知れません。"}
{"category":"closed_qa","context":"","instruction":"AdaGradとは何ですか？","response":"AdaGradはAdaptive Gradient Algorithmの略です。Adaptive Gradient Algorithm (Adagrad)は勾配に基づいて最適化するアルゴリズムです。過去の観測データに関する情報を取り入れることで、学習率がベクトルの要素ごとにパラメータに適用されます。低頻度の特徴に関連するパラメータには更新量を増やし（高い学習率など）、高頻度の場合は更新量を減らします（低い学習率）。Adagradの更新量は小さいため、スパースデータ（NLPや画像認識）の処理に適しています。各パラメータには固有の学習率があり、スパースな勾配に伴う問題解決能力が向上します。"}
{"category":"closed_qa","context":"","instruction":"AdaGrad を使用するメリットは何？","response":"AdaGrad を使用するメリットはいくつあります。1. 手動による学習率のチューニングが不要。2.重みのスケーリングが異なる場合、シンプルな確率的勾配降下法と比較して収束が高速で確実。3. マスターステップのサイズの影響をあまり受けない"}
{"category":"closed_qa","context":"","instruction":"Apache Hive とは？","response":"Apache Hive は、Apache Hadoop 分散ファイルシステム (HDFS) から抽出された大規模なデータセットの読み取り、書き込み、および管理を行うために設計されたオープンソースのデータウェアハウスソフトウェアで、より規模の大きい Hadoop エコシステムの側面も持ち合わせています。\n Apache Hiveの豊富なドキュメントと継続的なアップデートにより、Apache Hiveはアクセスしやすい方法でデータ処理に革新を起こし続けます。"}
{"category":"closed_qa","context":"","instruction":"Hive の特徴は何？","response":"Apache Hive は、HDFS や Amazon S3、Azure Blob Storage、 Azure Data Lake Storage、Google Cloud Storage、Alluxio といった Hadoop 互換のファイルシステムに格納された大規模データの分析をサポートします。スキーマオンリードで HiveQL という SQL ライクなクエリ言語を提供し、クエリを Apache Spark、MapReduce、Apache Tez のジョブに透過的に変換します。\n その他の Hive の特徴は以下のとおりです。1. Hive データ関数により、大規模なデータセットの処理やクエリをサポートします。これらの関数が提供する機能には、文字列操作、日付操作、型変換、条件演算子、数学関数などがあります。2. リレーショナルデータベース管理システム（RDBMS）にメタデータを格納します。3. Parquet、プレーンテキスト、RCFile、HBase、ORC（Optimized Row Columnar）といったさまざまなストレージタイプに対応しています。4. Hadoop エコシステムに格納された圧縮データをアルゴリズムで操作します。5. ユーザー定義関数（UDF）が組み込まれており、日付、文字列、その他のデータマイニングツールを操作します。Hive では、組み込み関数が用意されていないユースケースを処理するために、UDF セットの拡張をサポートしています。6. SQL ライクなクエリ言語の HiveQL が、内部的に MapReduce や Tez、Spark のジョブに変換されます。"}
{"category":"closed_qa","context":"","instruction":"Hive メタストアにどのような情報が保存されていますか？","response":"Apache Hive インフラストラクチャの中心的なリポジトリであるメタストアには、Hive のすべてのメタデータが保存されています。メタストアでは、メタデータを Hive テーブル とパーティションにフォーマットして、リレーショナルデータベース間でデータを比較することも可能です。メタデータには、テーブル名、カラム名、データ型、パーティション情報、HDFS 上のデータ位置などが含まれます。"}
{"category":"closed_qa","context":"","instruction":"Apache Hive で使用されるデータ型は何種類がありますか？","response":"Apache Hive のデータ型は、数値、日付/時間、文字列、複合データ、その他の5つのカテゴリで構成されています。\n 1. 数値データ型: その名前が示すとおり、このデータ型は整数データがベースとなっています。データ型の例としては、「TINYINT」や「SMALLINT」、「INT」、「BIGINT」があります。\n 2. 日付/時間データ型: このデータ型は、時刻と日付を入力するもので、「TIMESTAMP」や「DATE」、「INTERVAL」などのデータを入力できます。 \n 3. 文字列データ型: このデータ型も非常にわかりやすく、入力されたテキストデータ、つまり「文字列」を処理するためのデータとして実装することができます。例えば、「STRING」や「VARCHAR」、「CHAR」などのデータを入力できます。\n 4. 複合データ型: より高度なデータ型の一つである複合データ型は、より精巧なデータを記録し、「STRUCT」や「MAP」、「ARRAY」、「UNION」などのデータから構成されています。\n 5. その他のデータ型: 他の4つのカテゴリーのどれにも当てはまらないデータ型をその他のデータ型と呼び、そこには「BOOLEAN」や「BINARY」などのデータを入力できます。"}
{"category":"closed_qa","context":"","instruction":"リレーショナルデータベース管理システム（RDBMS）とは何で、 Apache Hive はそれをどのように利用するのか","response":"リレーショナルデータベース管理システム（RDBMS）は、行または列ベースのテーブルストラクチャにメタデータを格納することで動作するデータベースモデルであり、異なるデータセットを接続して比較することができます。\n RDBMS を利用することで、 Apache Hive はすべてのデータを安全、確実、かつ正確に保存、処理することができます。これは、役割ベースのセキュリティや暗号化通信などの統合機能により、抽出した情報へのアクセス権限が適切な人のみに付与できることにより実現します。"}
{"category":"closed_qa","context":"","instruction":"Apache Hive と従来の RDBMS の違いは何？","response":"Apache Hive と RDBMS には、いくつかの重要な違いがあります。1. RDBMS の機能が何度も読み取り／書き込みを行うのに対し、Hive では一度だけ書き込み、何度も読み取りを行います。2. Hive は、スキーマオンリードのルールに従っており、データのチェックやパース、検証は行わず、ファイルのコピーや移動のみを行います。一方、従来のデータベースでは、スキーマオンライトのルールに従って、テーブルにスキーマが適用されます。3. Hive は Hadoop 上に構築されているため、Hadoop や MapReduce と同じ制限に準拠する必要があります。他の RDBMS は、その限りではありません。"}
{"category":"closed_qa","context":"","instruction":"Apache HiveとApache Sparkの関係を教えてください。","response":"Apache Spark は、様々なデータセットの大量データを処理するために設計された分析フレームワークで、 R から Python まで様々な言語をサポートできる強力なユーザーインターフェースを提供します。\n Hive は、データを行、列、データ型を持つテーブルとして表現し、HiveQL と呼ばれる SQL インターフェースを使ってクエリや分析を行うための抽象化レイヤーを提供します。また、Hive は、Hive LLAP による ACID トランザクションをサポートしています。トランザクションは、複数のユーザーやプロセスが同時にデータにアクセスし、CRUD（生成、読み取り、更新、削除）操作を行う環境において、データ表示の一貫性を保証します。\n Databricks が提供する Delta Lake は、ACID トランザクションを保証するという点では Hive LLAP に似ていますが、オープンソースの Delta Lake には、データアクセス時の性能と信頼性を高める利点があります。Spark SQL は、行、列、データ型を持つテーブルとして表現される構造化データを操作するための Apache Spark のモジュールです。\n Spark SQL は SQL 2003 に準拠しており、データ処理の分散エンジンとして Apache Spark を使用します。Spark SQL インターフェースに加えて、DataFrames API を使用して Java、Scala、Python、R を用いてデータを操作することが可能です。Spark SQL は、HiveQL に似ています。\n どちらも ANSI SQL 構文を使用しており、Hive 関数の大部分は Databricks 上で実行されます。これらの Hive 関数には、日付／時刻の変換やパース、コレクション、文字列操作、数学演算、条件付き関数が含まれます。\n ただし、Hive 特有の関数には、Spark SQL に相当するものに変換が必要な関数や、Databricks 上の Spark SQL には存在しない関数があります。ANSI SQL 構文を使用した全ての HiveQL は、Databricks 上の Spark SQL で動作すると考えてよいでしょう。\n ANSI SQL の集計と分析関数も含まれます。Hive は、ORC ファイル形式に最適化されており、Parquet もサポートしています。Databricks は、Parquet と Delta に最適化されており、ORC もサポートしています。Databricks では、オープンソースの Parquet をファイル形式として使用する Delta の使用を推奨しています。"}
{"category":"closed_qa","context":"","instruction":"PrestoDBとは？","response":"PrestoDB 、通称 Presto は、もともとFacebookで立ち上げられたプロジェクトで、ペタバイト規模のデータを高速に処理・分析できる分散型 SQL クエリエンジンです。Presto のインフラストラクチャは、 MySQL や Teradata から MongoDB や Cassandra まで、リレーショナルデータベースと非リレーショナルデータベースの両方の統合をサポートしています。"}
{"category":"closed_qa","context":"","instruction":"Apache Kudu とは？","response":"Apache Kudu とは、Apache Hadoop 向けに開発された無料のオープンソースの列指向ストレージシステムです。構造化データ用エンジンで、各行への低遅延でランダムなミリ秒スケールのアクセスに加えて、優れたアクセスパターン分析もサポートします。広く普及している Hadoop 分散ファイルシステム（HDFS）と NoSQL データベースの HBase 間をつなぐために作成されたビッグデータエンジンです。"}
{"category":"closed_qa","context":"","instruction":"Apache Kylin とは？","response":"Apache Kylin とは、ビッグデータの対話型分析のための分散型オープンソースのオンライン分析処理（OLAP）エンジンです。Apache Kylin は Hadoop や Spark でSQL インターフェイスと多次元分析（OLAP）を提供するよう設計されています。さらに、ODBC ドライバ、JDBC ドライバ、REST API を介して BI ツールと容易に統合します。2014年に eBay が構築した Apache Kylin は、わずか 1 年後の 2015年に Apache ソフトウェア財団（ASF）のトップレベルプロジェクトに進出し、「Best open Source Big Data Tool （最も優れたオープンソースのビッグデータツール）」を 2015 年から 2 年連続で受賞しています。現在は、ビッグデータ向けの極めて重要な分析アプリケーションとして、世界中にある数千もの企業で使用されています。他の OLAP エンジンがデータ量に苦戦している中、Kylin のクエリ応答はミリ秒単位で、データセット全体を1秒以内のクエリレイテンシでペタバイト規模にスケーリングします。Kylin は、Hive クエリ経由でさまざまな次元の組み合わせと集計測定値を事前計算し、HBase に結果を入力することで、高速化を実現しています。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark とは？","response":"Apache Spark は、ビッグデータのワークロードに使用するオープンソースの分析エンジンです。リアルタイム分析とデータ処理のワークロードに加えて、両方のバッチ処理が可能です。Apache Spark は 2009 年にカリフォルニア大学バークレー校の研究プロジェクトとして開発されました。それまで研究者は、Hadoop システムでのジョブ処理を高速化する方法を模索していました。Apache Spark は Hadoop MapReduce をベースとして、MapReduce モデルを拡張し、対話型クエリやストリーム処理など、より多くのタイプの計算に効率的に使用できるようになっています。プログラミング言語の Java、Scala、Python、R でバインディングをネイティブ機能として実装しています。さらに、機械学習（MLlib）、ストリーム処理（Spark Streaming）、グラフ処理（GraphX）用のアプリケーション構築を支援するライブラリもいくつか備えています。Apache Spark は Spark Core と一連のライブラリから構成され、Spark Core は Apache Spark の中核をなし、タスクの分散伝送、スケジューリング、I/O 機能を提供する役割を果たしています。また、Spark Core のエンジンは基本的なデータタイプである Resilient Distributed Dataset（RDD）の概念を使用し、その RDD は計算複雑性の大部分をユーザーから見えないように設計されています。Spark はデータ処理の方法に優れています。データとパーティションはサーバークラスタ全体で集約され、次に計算が行われて別のデータストアへ移動するか、分析モデルを使用して実行します。ファイルを保存・取得するために必要なファイルや計算リソースの保存先の指定は不要です。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark を使用するメリットは何？","response":"Apache Spark を使用するメリットはたくさんあります。\n 1. スピード。Spark では、複数の並列処理中にメモリにデータをキャッシュするため、動作が非常に高速です。Spark の主な特徴は、処理速度を高めるインメモリエンジンです。インメモリ処理での大規模なデータ処理に関しては、MapReduce と比較して最大 100 倍、ディスクでは 10 倍高速になります。Spark は、ディスク操作への読み取り／書き込み回数を減らすことによってこれを可能にします。\n 2. リアルタイムのストリーム処理。Apache Spark は、他のフレームワークの統合に加えて、リアルタイムのストリーミング処理を実行できます、Spark はミニバッチ単位でデータを取り込み、そのミニバッチのデータに対してRDD変換を行います。3. 複数のワークロードをサポート。Apache Spark は、対話型クエリ、リアルタイム分析、機械学習、グラフ処理など複数のワークロードを実行できます。複数のワークロードは、単一のアプリケーションにシームレスに統合可能です。\n 4. 操作性の向上。複数のプログラミング言語をサポートする機能により、操作が動的になります。Java、Scala、Python、R でアプリケーションを迅速に記述することができ、アプリケーション構築のための多様な言語を提供します。\n 5. 高度なアナリティクス。Spark は SQL クエリ、機械学習、ストリーム処理、グラフ処理をサポートしています。"}
{"category":"closed_qa","context":"","instruction":"Catalyst オプティマイザとは？","response":"Catalyst オプティマイザとは、Spark SQL で主要な役割を果たす最適化機能です。Scala のパターンマッチングや準クォートなどの高度なプログラミング言語の機能を斬新な方法で利用し、拡張可能なクエリオプティマイザを構築します。Catalyst は Scala の関数型プログラミング構造に基づいており、次の 2 つの主要な目的を想定して設計されています。1. Spark SQLへの新しい最適化技術と機能の追加を容易にする, 2. 外部の開発者でもオプティマイザの拡張を実行できるようにする（データソース特有のルール追加、新規データ型のサポートなど）"}
{"category":"closed_qa","context":"","instruction":"畳み込みニューラルネットワーク（CNN または ConvNet）とは？","response":"深層学習において、畳み込みニューラルネットワーク（CNN または ConvNet）はディープニューラルネットワークの１つの手法です。画像内のパターン認識に通常使用されますが、空間データ分析、コンピュータビジョン、自然言語処理、信号処理などさまざまな用途に対する導入事例もあります。畳み込みネットワークのアーキテクチャは人間の脳内のニューロン結合パターンに類似し、視覚野の組織構造に着想を得ました。人工ニューラルネットワーク関連のこのタイプは、ネットワークの最も重要な操作の一つである「畳み込み」から名付けられています。"}
{"category":"closed_qa","context":"","instruction":"畳み込みとは","response":"畳み込みとは、2つの情報源が組み合わさった系統的な手法で、ある関数を別のものに変える操作です。一般的に、畳み込みは画像をぼかしたり鮮明にしたりする効果を得るために画像処理で長らく使用されてきましたが、エッジやエンボスの高度化などその他の処理にも使用されてきました。CNNは隣接する層のニューロン間における局所結合パターンを強化します。"}
{"category":"closed_qa","context":"","instruction":"畳み込み層とは？","response":"畳み込みニューラルネットワークの第1層は常に畳み込み層です。畳み込み層は、畳み込み演算を入力に適用し、次の層に結果を渡します。畳み込みは、その受容フィールド内の全てのピクセルを単一の値に変換します。たとえば、畳み込みを画像に適用した場合、画像サイズが縮小すると共に単一のピクセルに受容フィールド内の全情報をまとめます。畳み込み層の最終出力はベクトルです。解決すべき問題のタイプと注視している特微量の種類に基づき、各種の畳み込みを使用できます。"}
{"category":"closed_qa","context":"","instruction":"Databricks ランタイム(Runtime)とは？","response":"Databricks ランタイムは、データブリックスが管理するマシンのクラスタ上で実行されるソフトウェアアーティファクトのセットです。Spark はもちろん、ビッグデータ分析の操作性やパフォーマンス、セキュリティなどを大幅に向上させるコンポーネントや更新プログラムも数多く追加されています。"}
{"category":"closed_qa","context":"","instruction":"Databricks ランタイムが他のランタイムよりも優れている点は何ですか？","response":"Databricks ランタイムが他のランタイムよりも優れている点は次のとおりです。\n 1. 優れたパフォーマンス：Databricks I/Oモジュール（DBIO）は、垂直統合スタックを活用してクラウドでのSparkのパフォーマンスを大幅に向上させます。\n 2. セキュリティの強化：データブリックスのエンタープライズセキュリティ（DBES）モジュールは、静止時と移動時におけるデータの暗号化、きめ細かなデータアクセス制御、監査機能を追加し、標準コンプライアンス（HIPAA、SOC2など）や大企業に求められる厳しいセキュリティ要件を満たします。\n 3. 運用の複雑さを大幅に低減：計算リソースやローカルストレージの自動スケーリングなどの機能により、Sparkを「自動運転」にして、運用の複雑さと管理コストを大幅に低減します。\n 4. 迅速なリリースと新機能への早期アクセス：オープンソースのアップストリームリリースに比べて、データブリックスの SaaS では、リリースサイクルを短縮化し、オープンソースのリリースにはまだ利用できない最新機能やバグフィックスをユーザーに提供します。"}
{"category":"closed_qa","context":"","instruction":"DataFrame とは？","response":"DataFrame の概念は、多くの言語やフレームワークで共通しています。DataFrame は、柔軟かつ直感的にデータの保存や操作ができるため、最新のデータ分析で最も一般的に使用されるデータ構造の 1 つです。DataFrame にはスキーマと呼ばれる青写真が含まれており、各列の名前とデータタイプが定義されています。DataFrame の概念は、多くの言語やフレームワークで共通しています。DataFrame は、 Python のデータ分析ライブラリである pandas で主に使用されているデータ構造で、R や Scala、その他の言語でも使用されています。"}
{"category":"closed_qa","context":"","instruction":"DNA シーケンスとは？","response":"DNA シーケンスとは、DNA（デオキシリボ核酸）のヌクレオチドの正確な配列を決定するプロセスです。塩基としても知られる4つの化学構成要素（アデニン、グアニン、シトシン、チミン）の順序のDNAシーケンシングは、DNA分子内で発生します。DNA シーケンシングの最初の手法は、1970年代半ばにフレッド・サンガー（Fred Sanger）、ウォルター・ギルバート（Walter Gilbert）、アラン・マクサム（Allan Maxam）によって開発されました。配列決定された最初の DNA 断片は、大腸菌にのみ感染する T4 バクテリオファージというウイルスのものでした。ヒトゲノム計画の完了以来、大規模シーケンシングのための新しい手法が数多く開発されてきました。技術の改善と自動化により、研究室で、年間100兆をはるかに超える塩基をシーケンシングできる基準に達しています。大規模シーケンシングのために、多くの新しい方法が開発されています。これらの新しい方法は、多数の短い DNA 断片、または染色体全体を対象とすることができます。"}
{"category":"closed_qa","context":"","instruction":"ETL とは？","response":"ETL は Extract（抽出）、Transform（変換）、Load（ロード）の頭文字をとった略語です。ETL とは、データエンジニアがさまざまなソースからデータを抽出し、そのデータを使用可能で信頼できるリソースに変換し、エンドユーザーがアクセスできるシステムにロードするプロセスです。これにより、ダウンストリームでビジネス上の問題解決にデータを活用できるようになります。"}
{"category":"closed_qa","context":"","instruction":"ETL パイプラインとは？","response":"ETL パイプライン（またはデータパイプライン）とは、ETL 処理を行うための仕組みのことです。データパイプラインは、データを保存・処理するシステムから、保存・管理方法が異なる別のシステムにデータを移動させるための一連のツールとアクティビティです。パイプラインを使用することで、さまざまなソースから情報を自動的に取得し、単一の高性能なデータストレージに変換して統合できます。"}
{"category":"closed_qa","context":"","instruction":"Delta Lake でどのように信頼性の高い ETL を自動化するのか？","response":"Delta Live Tables（DLT）は、Delta Lake に高品質データをもたらす信頼性の高いデータパイプラインの構築と管理を容易にします。DLT の宣言型パイプラインの開発、データテストの自動化、監視とリカバリの詳細な視覚化は、データエンジニアリングのチームによる ETL の開発と管理をシンプルにします。"}
{"category":"closed_qa","context":"","instruction":"特徴量エンジニアリングとは？","response":"特徴量エンジニアリングは、データを使用してモデルをトレーニングするために使用される機械学習アルゴリズムのパフォーマンスを向上させるために、データを変換および強化するプロセスです。\n 特徴量エンジニアリングには、データのスケーリングまたは正規化、非数値データ (テキストや画像など) のエンコード、時間またはエンティティによるデータの集約、さまざまなソースからのデータの結合、さらには他のモデルからの知識の転送などの手順が含まれます。これらの変換の目的は、機械学習アルゴリズムがデータセットから学習する能力を高め、より正確な予測を行うことです。"}
{"category":"closed_qa","context":"","instruction":"Hadoop とは？","response":"Hadoop は、High Availability Distributed Object Oriented Platform の略です。そして、これこそが Hadoop テクノロジーが開発者に提供するものです。オブジェクト指向タスクの並列分散による高可用性を実現します。\n Apache Hadoop とは、オープンソースの Java ベースのソフトウェアプラットフォームで、ビッグデータアプリケーションのデータ処理とストレージを管理します。プラットフォーム は、コンピューティングクラスタ内のノード間で Hadoop ビッグデータセットと分析ジョブを分散させ、並列実行できる小さなワークロードに分割します。\n また、構造化データと非構造化データの両方を処理し、1 台のサーバーから数千台のマシンまで、確実にスケールアップします。"}
{"category":"closed_qa","context":"","instruction":"Hadoop プログラミングとは？","response":"Hadoop フレームワークでは、コードのほとんどは Java で書かれていますが、C ベースのネイティブコードもあります。Hadoop MapReduce では、Java が最もよく使われますが、Hadoop streaming などのモジュールを通じて、ユーザーは自分の好きなプログラミング言語を使って、マップとリデュースの機能を実装することができます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop データベースとは？","response":"Hadoop は、データストレージやリレーショナルデータベースのためのソリューションではありません。その代わり、オープンソースのフレームワークとして、大量のデータを同時にリアルタイムに処理することを目的としています。\n データは HDFS に格納されますが、これは非構造化データベースとみなされ、リレーショナルデータベースには該当しません。実際、Hadoop では、データは非構造化、半構造化、構造化のいずれの形でも保存できます。これにより、企業は自社のビジネスニーズやその先にあるものに合わせて、より柔軟にビッグデータを処理することができるようになります。"}
{"category":"closed_qa","context":"","instruction":"Hadoop はどのようなデータベースなのですか？","response":"技術的には、Hadoop はそれ自体が SQL や RDBMS のようなデータベースの一種ではありません。その代わり、Hadoop のフレームワークは、幅広い種類のデータベースに対する処理ソリューションをユーザーに提供します。\n Hadoop は、企業が膨大な量のデータを短時間で処理できるようにするためのソフトウェアエコシステムです。これは、大規模なコンピュータの並列処理を容易にすることで実現されています。Apache HBase などの各種データベースは、数百台、数千台のコモディティサーバに搭載されたデータノードクラスタに分散させることができます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop はどのように機能するのか？","response":"Hadoop は、巨大なデータセットをコモディティハードウェアのクラスタに分散して配置することを可能にするフレームワークです。Hadoop の処理は、同時に複数のサーバーで並列に行われます。\n クライアントは Hadoop にデータやプログラムを投入します。簡単に言うと、HDFS（Hadoop のコアコンポーネント）がメタデータと分散ファイルシステムを処理します。次に、Hadoop MapReduce が入出力データの処理と変換を行います。最後に、YARN がクラスタ間でタスクを分割します。\n Hadoop を利用することで、お客様はコモディティリソースをより効率的に利用することができ、高可用性と内蔵の障害点検知機能を期待することができます。また、接続された業務システムに対して問い合わせを行う際にも、迅速なレスポンスが期待できます。\n Hadoop は、ビッグデータの活用を目指す企業にとって、比較的容易なソリューションといえます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop はどのような言語で書かれているのですか？","response":"Hadoop のフレームワーク自体は、ほとんどが Java で構築されています。その他のプログラミング言語としては、C 言語のネイティブコードやコマンドライン用のシェルスクリプトがいくつかあります。しかし、Hadoop のプログラムは、Python や C++ など、他の多くの言語で書くことができます。そのため、プログラマーは自分が最も慣れ親しんだツールを使って柔軟に作業することができます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop の使い方を知りたい。","response":"Hadoop はビッグデータを管理する必要がある組織にとって、簡単なソリューションを生み出します。しかし、だからといって、常にわかりやすい使い方ができるわけではありません。上記のユースケースから分かるように、Hadoopフレームワークをどのように実装するかはかなり柔軟性があります。\n ビジネスアナリスト、データサイエンティスト、開発者がどのように Hadoop を使用するかは、全て組織とその目標によって決まります。\n Hadoop は全ての企業に適しているわけではありませんが、ほとんどの企業は Hadoop との関係を再評価する必要があります。.ビジネスがコアプロセスの一部として大量のデータを扱う場合、Hadoopはニーズに合った、柔軟でスケーラブルかつ手頃な価格のソリューションを提供します。ここから先は、あなたとあなたのチームの想像力と技術力次第です。"}
{"category":"closed_qa","context":"","instruction":"Hadoop のコアモジュールとは何か？","response":"Hadoop のコアモジュールは４つあります。HDFS：Hadoop 分散ファイルシステム。HDFS は Java ベースのシステムで、クラスタ内のノード間で、フォールトトレラント方式で大規模なデータセットを格納できます。\n YARN：YARN は、Yet Another Resource Negotiator（もう一つのリソースネゴシエーター）。YARN は、Hadoop 上で実行されるクラスタリソース管理、タスクの計画、およびジョブのスケジューリングに使用されます。\n MapReduce：MapReduce は、大規模なデータセットの並列処理に使用されるプログラミングモデルおよびビッグデータ処理エンジンです。もともと、MapReduce は Hadoop で利用可能な唯一の実行エンジンでしたが、その後、Hadoop は Apache Tez、Apache Spark を含む他のシステムのサポートも追加しています。\n Hadoop Common：Hadoop Common は、ライブラリやユーティリティにまたがるサービスを提供し、他の Hadoop モジュールをサポートします。"}
{"category":"closed_qa","context":"","instruction":"Hadoop MapReduce は?","response":"MapReduce は、Hadoop の別のデータ処理層です。大規模な構造化データと非構造化データを処理する機能を備えている他、ジョブを独立したタスク (サブジョブ) のセットに分割して、非常に大きなデータファイルを並行して管理できます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop 分散ファイルシステム(HDFS)は何？","response":"Hadoop 分散ファイルシステム（HDFS）は、最も大きい Apache プロジェクトと Hadoop のプライマリストレージシステムの 1 つで、ネームノードとデータノードのアーキテクチャを採用しています。コモディティハードウェアのクラスタ上で実行されている大きなファイルを格納できる分散ファイルシステムです。\n Hadoop 分散ファイルシステムは、全てのデータストレージの始まりと終わりの場所です。このコンポーネントは、さまざまな構造化および非構造化データノードにまたがる大規模なデータセットを管理します。同時に、ログファイルの形でメタデータを維持します。HDFS には、NameNode と DataNode という 2 つの副次的なコンポーネントがあります。"}
{"category":"closed_qa","context":"","instruction":"NameNodeとは？","response":"NameNode はHadoop HDFS のマスターDaemonです。このコンポーネントは、ファイルシステムの名前空間を維持し、当該ファイルへのクライアントアクセスを制御します。マスターノードとも呼ばれ、ブロック数や位置などの Metadata を保存します。主にファイルとディレクトリで構成され、ファイルの命名、クローズ、オープンなどのファイルシステム実行を行います。"}
{"category":"closed_qa","context":"","instruction":"Hadoop YARNとは？","response":"YARN は、Yet Another source Navigator の頭字語をとった略語です。リソース管理に適したオープンソース Apache Hadoop のコアコンポーネントの 1 つであり、ワークロードの管理、監視、およびセキュリティ制御の実装を担当します。また、Hadoop クラスタで実行されているさまざまなアプリケーションにシステムリソースを割り当てると同時に、各クラスタノードで実行するタスクを割り当てます。YARN には、2 つの主要なコンポーネントがあります。1. リソースマネージャー, 2. ノードマネージャー"}
{"category":"closed_qa","context":"","instruction":"Apache Impalaとは？","response":"Apache ImpalaとはApache Hadoop のためのオープンソースでネイティブな分析データベースです。"}
{"category":"closed_qa","context":"","instruction":"Apache Pigとは？","response":"Pig は、Hadoop 内で使用される大規模なデータセットのクエリを実行するために使用される、高レベルスクリプト言語です。Pig の単純な SQL のようなスクリプト言語は Pig Latin と呼ばれ、その主な目的は、必要な演算を実行し、最終的な出力を目的の形式で準備することです。"}
{"category":"closed_qa","context":"","instruction":"Apache Zookeeperとは","response":"Apache Zookeeperは、信頼性の高い分散処理を実現するための集中型サービス。"}
{"category":"closed_qa","context":"","instruction":"Apache Sqoopとは？","response":"Apache Sqoopは、Apache Hadoop とリレーショナルデータベースなどの構造化データストア間でバルクデータを効率的に転送するために設計されたツール。"}
{"category":"closed_qa","context":"","instruction":"Apache Oozieとは？","response":"Apache Hadoop のジョブを管理するためのワークフロースケジューラーシステム。Oozie ワークフローのジョブは、アクションの順番を指定する有向非巡回グラフ（DAG: Directed Acyclical Graph）で実行されます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop のメリットは何？","response":"Hadoopに主に３つのメリットがあります。\n 1. スケーラビリティ：Hadoop は、従来のシステムのようなデータストレージの制限がなく、分散環境で動作するため、スケーラブルです。初期のデータレイクが Hadoop 上に構築された理由は、この特長にありました。詳しくはデータレイクの歴史と進化をご覧ください。\n 2. レジリエンス：Hadoop 分散ファイルシステム (HDFS) は基本的に復元力が備わっています。Hadoop クラスタの任意のノードに格納されたデータは、ハードウェアまたはソフトウェアの障害の可能性に備えて、クラスタの他のノードにもレプリケートされます。この意図的に冗長な設計により、フォールトトレランスが保証され、1 つのノードがダウンしても、クラスタ内で利用可能なデータのバックアップが常に存在します。\n 3. 柔軟性：従来のリレーショナルデータベース管理システムとは異なり、Hadoop を使用する場合は、半構造化形式や非構造化形式を含む任意の形式でデータを格納できます。Hadoop を使用することで、新しいデータソースに簡単にアクセスしてさまざまな種類のデータを活用できます。"}
{"category":"closed_qa","context":"","instruction":"Hadoop アーキテクチャの課題は何？","response":"Hadoop アーキテクチャに多く３つの課題があります。\n 1. 複雑さ：Hadoop は、低レベルの Java ベースのフレームワークであり、エンドユーザーが作業するには複雑で困難です。Hadoop アーキテクチャには、セットアップ、保守、およびアップグレードに関する重要な専門知識とリソースが必要になる場合もあります。\n 2. パフォーマンス：Hadoop は、ディスクに対する頻繁な読み取りと書き込みを使用して計算を実行します。これは Apache Spark などの可能な限りメモリ内にデータを格納して処理することを目的とするフレームワークと比較すると、時間がかかり非効率です。\n 3. 長期的な実現性：Hadoop を取り巻く環境は 2019 年に大きく変わりました。Google は、2004 年にMapReduce に関する論文を発表し、Apache Hadoop の開発を支えてきましたが、Google テクニカルインフラストラクチャ担当 SVP のウルス・ヘルツル（Urs Hölzle）氏がツイートで、MapReduce の使用を全面的に中止したことを投稿しました。また、Hadoop の世界では、極めて注目度の高い合併や買収が行われました。Hadoop の大手プロバイダは、 Hadoop を技術ではなく哲学とみなすようになり、2020 年には Hadoop 中心の製品群からの転換を図っています。そして、2021 年はさらなる変化の年となりました。 4 月に ASF による Hadoop エコシステムの 10 のプロジェクトの終了の発表があり、同年の 6 月に、Cloudera が非公開化に合意しています。この決定が Hadoop ユーザーに与える影響は未だ不明ですが、増加する懸念事項と加速するデジタル化の必要性により、多くの企業が Hadoop の利用を見直しています。"}
{"category":"closed_qa","context":"","instruction":"Hadoop エコシステムとは？","response":"Apache Hadoop エコシステムとは、Apache Hadoop ソフトウェアライブラリのさまざまなコンポーネントを指します。オープンソースプロジェクトだけでなく、補足ツールの全てが含まれます。Hadoop エコシステムの最もよく知られているツールには、HDFS、Hive、Pig、YARN、MapReduce、Spark、HBase Oozie、Sqoop、Zookeeper、などがあります。開発者が頻繁に使用する主要な Hadoop エコシステムコンポーネントは次のとおりです。"}
{"category":"closed_qa","context":"","instruction":"Hadoop クラスタとは？","response":"Apache Hadoop とは、オープンソースの Java ベースのソフトウェアフレームワークで、並列データ処理エンジンです。アルゴリズム（MapReduce アルゴリズムなど）を使用してビッグデータ分析処理タスクを並列実行できる小さなタスクに分割し、Hadoop クラスタ全体に分散させることができます。Hadoop クラスタとは、ビッグデータセットに対してこのような並列計算を実行するためにネットワーク化された、ノードと呼ばれるコンピュータの集合体です。Hadoop クラスタは、他のコンピュータクラスタとは異なり、分散コンピューティング環境で大量の構造化・非構造化データを格納および分析するために特別に設計されています。Hadoop エコシステムが他のコンピュータクラスタと異なる点は、独自の構造とアーキテクチャです。Hadoop クラスタは、高可用性、低コストのコモディティハードウェアを利用する、接続されたマスターノードとスレーブノードのネットワークから構成されます。ボリュームの要求に応じてノードを直線的にスケーリングし、素早く加算または減算する機能により、データセットのサイズの変化が大きいビッグデータ分析に適しています。"}
{"category":"closed_qa","context":"","instruction":"Hive 日付関数とは？","response":"Hiveでは、データの処理や照会を行う際に役立つ多くの組み込み関数を提供しています。これらの関数が提供する機能には、文字列操作、日付操作、型変換、条件演算子、数学関数などがあります。"}
{"category":"closed_qa","context":"","instruction":"Jupyter Notebook とは？","response":"Jupyter Notebook は、オープンソースで提供された Web アプリケーションであり、プログラムや数式、その他のマルチメディアリソースを含むドキュメントを作成・共有する目的で、主にデータサイエンティストに利用されています。"}
{"category":"closed_qa","context":"","instruction":"Databricks でJupyter Notebook の利用をサポートしていますか？","response":"はい、サポートしています。Databricks クラスタは、Jupyter エコシステムのオープンソースツール（表示・出力ツールなど）を利用するために IPython カーネルを使用するよう設定できます。また、.ipynb ファイルのインポート／エクスポートをサポートしており、Databricks 上においても Jupyter Notebook で中断したところから容易に再開できます。Databricks では、 Databricks 機械学習のランタイム内において、オープンソースの Jupyter のライブラリを長年サポートしています。"}
{"category":"closed_qa","context":"","instruction":"Keras モデルとは？","response":"Keras とは、Theano と Tensorflow 上に構築された深層学習のためのハイレベルのライブラリです。Keras は、Python で記述され、深層学習モデルの範囲を作成するためのクリーンで便利な方法を提供します。Keras は、ニューラルネットワークの開発とテストに関して最も使用されている高レベルのニューラルネットワーク API の 1 つです。現在では、ニューラルネットワークのレイヤーの作成や複雑なアーキテクチャの設定が、Keras の高レベル API により簡単に行われています。Keras モデルは、シーケンスまたはスタンドアロングラフで構成されます。完全に構成可能なモジュールがいくつかあり、それらを組み合わせて新しいモデルを作成できます。これらの構成可能なモジュールの中には、ニューラルレイヤー、コスト関数、オプティマイザ、初期化スキーム、ドロップアウト、ロス、活性化関数、および正規化スキームがあります。新たな機能を別々のモジュールとして容易に追加できることは、モジュール化によってもたらされるメリットの 1 です。Keras の高い柔軟性は、革新的な研究に適しています。Keras モデルを開発する方法には、Sequential と Functional があります。"}
{"category":"closed_qa","context":"","instruction":"Sequential API モデルは何？","response":"Sequential API モデルは、極めてシンプルなモデルであり、ほとんどの問題に対してレイヤーごとにモデルを構成できるようにする線形のレイヤーの積み重ねで構成されています。Sequential モデルは非常に簡単に使えますが、トポロジでは制限されています。この制限は、共有レイヤーを使用したモデルの構成や、複数の入力または出力を持つモデルを構成できないことから生じています。"}
{"category":"closed_qa","context":"","instruction":"Functional APIとは？","response":"Functional API は、柔軟性を高める複雑なモデルを作成するのに最適です。Functional API により、フィーチャレイヤーが前後のレイヤー以外の複数のレイヤーに接続するモデルを定義できます。モデルとは、レイヤーのインスタンスを作成し、それらを直接ペアで接続することで定義されます。実際、このモデルを使用することで他のレイヤーにレイヤーを接続することが可能です。このモデを用いることで、シャムネットワーク、残余ネットワーク、マルチ入力/マルチ出力モデル、有向非巡回グラフ（DAG）、共有レイヤーを持つモデルなどの複雑なネットワークを作成することが可能になります。  "}
{"category":"closed_qa","context":"","instruction":"MLOps とは？","response":"MLOps は、Machine Learning Operations（機械学習オペレーション）の略語です。機械学習エンジニアリングの中核となる MLOps は、機械学習モデルを本番環境に移行し、維持・監視のプロセスを効率化することに重点を置いています。MLOps は、多くの場合、データサイエンティスト、DevOps エンジニア、IT 部門で構成されるチーム間のコラボレーションを担います。"}
{"category":"closed_qa","context":"","instruction":"MLOpsはなぜ必要なのですか？","response":"機械学習のライフサイクルは、データ取り込み、データ準備、モデルのトレーニング、チューニング、デプロイメント、監視、説明可能性といった多くの複雑な要素で構成されており、機械学習の製品化は容易ではありません。また、データエンジニア、データサイエンティスト、機械学習エンジニアからなるチーム間の連携や引き継ぎも必要です。これら全てのプロセスを同期、連動させるためには、厳格な運用が必須となります。MLOps の概念は、機械学習のライフサイクルにおける実験フェーズ、反復、継続的な改善を包括しています。"}
{"category":"closed_qa","context":"","instruction":"MLOps のメリットは何？","response":"MLOps の主なメリットは、機械学習のライフサイクルにおける効率性、拡張性、リスク軽減です。\n 1. 効率性：データチームは、MLOps によりモデル開発の迅速化、高品質な機械学習モデルの提供、迅速なデプロイメントと製品化が可能になります。\n 2. 拡張性：MLOps は、広範なスケーラビリティと管理を可能にし、継続的インテグレーション、継続的デリバリー、継続的デプロイメントを目的として、数千のモデルを監督、制御、管理、監視できます。具体的には、MLOps が機械学習パイプラインの再現性を確保し、データチーム間の緊密なコラボレーションを可能にするため、DevOps や IT 部門との衝突を減らし、リリースサイクルを高速化します。\n 3. リスク軽減：機械学習モデルは、規制の精査やドリフトチェックが必要になるケースが多々あります。MLOps により、こうした要求に対するより高い透明性と迅速な対応が可能になり、組織や業界のポリシーへの準拠が確実になります。"}
{"category":"closed_qa","context":"","instruction":"MLOps の構成要素は何？","response":"機械学習プロジェクトにおける MLOps は、プロジェクトに応じて範囲を絞ることも、拡大することも可能です。例えば、データパイプラインからモデル作成までの全てを網羅する場合もあれば、他のプロジェクトでは、モデルのデプロイメントプロセスのみの実装が必要な場合もあります。多くの企業では、次の領域に MLOps の原則を導入しています。1. 探索的データ解析（EDA）, 2. データ準備と特徴量エンジニアリング, 3. モデルのトレーニングとチューニング, 4. モデルのレビューとガバナンス, 5. モデル推論とサービング, 5. モデル監視, 6. モデル再トレーニングの自動化"}
{"category":"closed_qa","context":"","instruction":"MLOps のベストプラクティスを教えてください。","response":"MLOps のベストプラクティスは、MLOps の原則が適用される段階によって区別できます。\n 1. 探索的データ解析（EDA）：再現性、編集性、共有性のあるデータセット、表、可視化方法を作成することで、機械学習のライフサイクルにおいてデータを繰り返し探索、共有、準備できます。\n 2. データ準備と特徴量エンジニアリング：データの変換、集約、重複排除を繰り返すことで、洗練された特徴量を作成できます。特徴量ストアを活用して、データチーム間で特徴量を可視化し、共有することが重要です。\n 3. モデルのトレーニングとチューニング：scikit-learn や Hyperopt などの一般的なオープンソースライブラリを使用して、モデルのトレーニングと性能の改善ができます。より簡潔な方法として、AutoML などの自動機械学習ツールを使ってテストを実行し、レビューやデプロイが可能なコードを作成します。\n 4. モデルのレビューとガバナンス：モデルの系統、モデルのバージョンを追跡し、ライフサイクルを通じてモデルの成果物および移行を管理します。MLflow のようなオープンソースの MLOps プラットフォームを使用して、機械学習モデルの発見、共有、コラボレーションを行うことも可能です。\n 5. モデル推論とサービング：モデルの更新頻度、推論要求時間などの本番環境に特化した管理を、テストと QA で行います。DevOps の原則を取り入れたリポジトリやオーケストレータなどの CI/CD ツールを利用して、本番前のパイプラインを自動化します。\n 6. モデルのデプロイと監視：パーミッションの設定やクラスタの作成を自動化し、登録されたモデルを本番稼動に備えます。REST API モデルのエンドポイントを有効にしてください。\n 7. モデル再トレーニングの自動化：トレーニングデータと推論データの違いによるモデルドリフトが発生した場合、アラートと自動化により是正措置を講じます。"}
{"category":"closed_qa","context":"","instruction":"MLOps と DevOps の違いについて教えてください。","response":"MLOps は、機械学習プロジェクトに特化したエンジニアリングプラクティスのセットです。ソフトウェアエンジニアリングで広く採用されている DevOps の原則が基になっています。一方、DevOps は、アプリケーションの出荷に対して迅速かつ継続的な反復アプローチを提供します。MLOps では、DevOps の原則をベースに機械学習モデルを本番環境に導入します。どちらもソフトウェアの品質改善、パッチやリリースの迅速化、顧客満足度の向上という成果をもたらします。"}
{"category":"closed_qa","context":"","instruction":"MLOps プラットフォーム","response":"MLOps プラットフォームは、データサイエンティストやソフトウェアエンジニアに反復的なデータ探索、実験追跡、特徴量エンジニアリング、モデル管理のためのリアルタイムな共同編集機能、制御されたモデルの移行、デプロイメント、監視を促進するコラボレーション環境を提供します。MLOps は、機械学習のライフサイクルにおける運用と同期の両側面を自動化します。MLflow のフルマネージド環境であり、世界をリードするオープンな MLOps プラットフォームの Databricks はこちらからお試いただけます。"}
{"category":"closed_qa","context":"","instruction":"pandas とは？","response":"プログラミング言語 Python でデータ分析を行うためのオープンソースのライブラリでで、高速で適応性の高いデータ構造を提供します。この使いやすいデータ操作ツールは、ウェス・マッキニー氏が開発したものでNumPy パッケージ上に構築されています。その主要なデータ構造は DataFrame と呼ばれています。"}
{"category":"closed_qa","context":"","instruction":"pandas DataFrame って何？","response":"Pandas DataFrame とは、表形式のデータを表示、操作する方法の一つです。二次元のデータ構造で、データを行と列に整理してテーブルとして表示します。DataFrame は、ゼロから作成することも、NumPy 配列のような他のデータ構造を利用することも可能です。"}
{"category":"closed_qa","context":"","instruction":"pandas DataFrame の行をどうやって削除するか？","response":"DataFrame の行を削除する方法を紹介します。\n df.drop_duplicates( ) を実行すると、行ラベルに指定した条件に応じて重複している行が削除されます。列に対して動作するのと同じ .drop() メソッドを使用することもできますが、その場合は行のインデックスを指定してドロップします。この場合、必ずインデックスをリセットしてください。\n 具体的に欠損値を持つ行を削除するには、DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False) を使用できます。これは、Null 値を持つ行を自動的に取り出します。また、関数のパラメータを調整して、特定のデータセクションにおいて欠損値を削除するかどうかを決定することもできます。\n Null 値を特定の値に置き換えたい場合は、.dropna() で削除するのではなく、 .fillna() コマンドを使用します。"}
{"category":"closed_qa","context":"","instruction":"pandas DataFrame の列をどうやって削除するか？","response":"DataFrame の列の削除は簡単です。ドロップ（ ）方式で、列を取り出すことができます。この場合、削除したい列のラベルを入力する必要がありますので、ドロップ（ ）コマンドを実行する前に、正しい列名をメモしておいてください。また、引数 inplace を True にすると、 DataFrame を再割り当てせずに列を削除できるようになります。"}
{"category":"closed_qa","context":"","instruction":"pandas DataFrame のインデックスをどうやって削除するか？","response":"DataFrame は常に何らかのインデックスを持つため、インデックスを完全に削除することは困難ですが、インデックスラベルの変更やその名前を完全に削除することは可能です。名前の削除は、 del df.index.name コマンドを実行することで実施可能です。\n また、DataFrame のインデックスをリセットすることも可能です。これは、インデックス値が重複している場合に役立ちます。必要なのは、インデックスをリセットし、重複を削除し、重複のない新しい列インデックスを復活させることです。"}
{"category":"closed_qa","context":"","instruction":"pandas DataFrame のインデックスや列の名前をどうやって変更するか？","response":"pandas DataFrame のインデックスや列の名前を変更する方法を紹介します。インデックスや列の名前の変更は、削除するよりも簡単でわかりやすいです。これらのオブジェクトの名前を変更するために必要なことは、 .rename( ) メソッドを使用し、新しい値を与えたいインデックス（または列）を記入することだけです。\n リネームタスクで引数 inplace を False に変更すると、列の名前を変更しても DataFrame が再アサインされなくなります。"}
{"category":"closed_qa","context":"","instruction":"pandas DataFrame のデータをどうやってフォーマットするか","response":"pandas DataFrame のデータをフォーマットする方法を紹介します。文字列のすべてのインスタンスを置換したい場合は、 .replace() コマンドを使い、（変更する値、変更先の値）のフォーマットで隙間を埋めることができます。そして、変更したい文字列や値のすべてのインスタンスを、変更する項目に自動的に置換します。\n もう一つ必要なのは、文字列から不要な部分を取り除くフォーマット機能です。map （）コマンドは、 result 列で使用すると、その列の各要素に選択したラムダ関数を適用します。また、列のテキストを複数の行に分割することもできますが、これは少し複雑なので、簡単なチュートリアルを紹介します。まず、長すぎる行を特定し、どの行を分割すべきかをわかるようにします。行を特定後、その列の中にある文字列をスペースで囲みます。次のステップは、行をまたいで分割される値を受け取り、 Series オブジェクトに入れることです。もし、 Series に NaN 値があったとしても、問題ありません。Series を積み上げるだけで、最終的な Series のコピーには不要な NaN 値が含まれないことが保証されます。積み上げた Series を希望のフォーマットにするためには、 DataFrame と並ぶようにレベルを落とす必要があります。その後、 Series を DataFrame に変換し、元の DataFrame に戻し、元の DataFrame から問題のある列を削除します。最後のステップ（列の削除）により、重複の発生を防ぐことができます。\n DataFrame のデータに適用可能な最後のフォーマットは、 DataFrame の行または列に関数を適用する形です。まず、 .loc[ ] または .iloc[ ] を用いて作業したい行を選択します。しかし、ここでは DataFrame を使用しているので、より具体的には df.loc と df.iloc を使うことになるでしょう。そして、正しい行を選択した後、 apply() を使い、行または列に doubler のような機能を適用することができます。"}
{"category":"closed_qa","context":"","instruction":"空の DataFrame を作る関数は何？","response":"空の DataFrame を作るには、 pandas DataFrame() 関数を使います。"}
{"category":"closed_qa","context":"","instruction":"pandas はデータをインポートする際に日付を認識するのか？","response":"原則的にはそうで、実際にはもう少し複雑です。\n pandas は日付が入力されていることを認識できますが、正しい方向に少し誘導することでうまく機能します。具体的には、 CSV ファイルや類似ファイルなどからデータを取り込む際に、 parse_dates 引数を追加します。CSV ファイルからデータを取り込む場合、以下のようになります。pd.read_csv('yourFile', parse_dates=True)。この方法は、数値書式を使用する日付に最も適しています。しかし、すべての日付がこのような形式を取るわけではありません。\n 変わった日付形式や、 DataFrame が認識しにくい日付形式の場合は、独自のパーサーを作成します。これらは、日付と時刻の入力認識を制御するためのフォーマット文字列を使用するラムダ関数の形式をとる場合があります。\n pandas は、どのような方法でナッジを与えても、完了後に日時を認識できます。つまり、最小限の入力で、 DataFrame に日付ベースの情報を取り込むように指示することができます。"}
{"category":"closed_qa","context":"","instruction":"pandasGUIとは？","response":"pandasGUI は、データ操作や要約統計を支援するために作成された Python ベースのライブラリです。これらは、 GUI （グラフィカル・ユーザー・インターフェース）を使い DataFrame ライブラリのデータに適用され、インターフェース上で意図した操作を行うことになります。pandasGUI は、 UI （ユーザーインターフェース）でコマンドを入力すると、プログラムがそのコマンドを pandas 本体で実行するように設計されています。"}
{"category":"closed_qa","context":"","instruction":"Tablooとは？","response":"Tabloo は自らを「表形式データを視覚化するためのミニマルなダッシュボードアプリ」と表現しています。Python から実行できるので、 pandas とも完全な互換性があります。"}
{"category":"closed_qa","context":"","instruction":"D-Tale？","response":"D-Tale は、 Tabloo と同様に Flask のバックエンドと、React のフロントエンドを使用しており、 D-Tale で使用できる豊富なオプションを最大限に活用することができます。"}
{"category":"closed_qa","context":"","instruction":"Parquet とは？","response":"Apache Parquet は、効率的なデータの保存と検索のために設計された、オープンソースの列指向データファイル形式です。複雑なデータを一括処理するための効率的なデータ圧縮と符号化方式を提供し、パフォーマンスを向上させます。Apache Parquet は、バッチとインタラクティブの両方のワークロードで共通の交換形式となるように設計されており、Hadoop で利用可能な他の列指向ストレージファイル形式である RCFile や ORC に似ています。"}
{"category":"closed_qa","context":"","instruction":"Parquet の特徴は何か？","response":"Parquet の特徴は以下６点あります。\n 1. コスト不要のオープンソースのファイル形式, 2. あらゆる言語に対応, 3. 列ベースのフォーマット：ファイルは行ではなく列ごとに整理されるため、ストレージ容量を節約し、分析クエリを高速化します。4. 分析（OLAP）のユースケースに使用：通常は、従来の OLTP データベースと組み合わせて使用します。5. 高効率なデータ圧縮と解凍, 6. 複雑なデータタイプ、高度にネストされたデータ構造をサポート"}
{"category":"closed_qa","context":"","instruction":"Parquet を使用するメリットは何？","response":"Parquet を使用するメリットは主に以下３点あります。1. ビッグデータの保存に最適：構造化データ、テーブル、画像、動画、ドキュメントなど、あらゆる種類のデータを保存できます。2. クラウドストレージを節約：高効率なカラム単位の圧縮と、異なるデータの種類の列に対応する柔軟な符号化方式を採用しています。3. データのスループットとパフォーマンスの向上：データスキッピングなどの技術を用いて、特定の列の値を取得するクエリによるデータの行全体の読み取りを不要にします。"}
{"category":"closed_qa","context":"","instruction":"Parquet と CSV の違いは何？","response":"CSV は、Excel や Google スプレッドシートなどの多くのツールで使用されており、他にも多数のツールで使用されているシンプルで一般的なフォーマットです。Parquet により、大規模なデータセットのストレージ必要量は少なくとも 3 分の 1 削減され、スキャンとデシリアライゼーションに必要な時間の大幅な改善で、全体のコストが削減されています。"}
{"category":"closed_qa","context":"","instruction":"Parquet と Delta Lakeの関連を教えてください。","response":"オープンソースの Delta Lake プロジェクトは、Parquet 形式に基づいて構築され、さまざまな機能の追加により拡張されています。追加機能には、クラウドオブジェクトストレージの ACID トランザクション、タイムトラベル、スキーマの拡張、シンプルな DML コマンド（CREATE、UPDATE、INSERT、DELETE、MERGE）などがあります。Delta Lake は、順序付けられたトランザクションログを使用してこれらの重要な機能の多くを実装しています。これにより、クラウドのオブジェクトストレージ上におけるデータウェアハウス機能が可能になります"}
{"category":"closed_qa","context":"","instruction":"PySpark とは？","response":"Apache Spark は、プログラミング言語 Scala で記述されています。PySpark とは、Spark を実行するための Python API です。PySpark は、Apache Spark とプログラミング言語 Python での Resilient Distributed Dataset（RDD）とのインターフェイスもサポートしており、これは Py4J ライブラリを活用することで実現しています。Py4J は、PySpark に組み込まれた定評のあるライブラリで、Python の JVM 上のオブジェクトとの動的なインターフェースを可能にします。PySpark には、効率的にプログラミングするためのライブラリが数多く実装されており、互換性のある外部ライブラリもあります。"}
{"category":"closed_qa","context":"","instruction":"PySparkSQLとは？","response":"PySparkSQL は、膨大な量の構造化・半構造化データにSQLライクな分析を行うPySparkライブラリです。PySparkSQL を使用して SQL クエリを実行することができ、Apache Hive との連携や、HiveQL の適用も可能です。さらに、PySparkSQL は、Pyspark Core のラッパーです。PySparkSQL では、リレーショナルデータベース管理システムのテーブルと類似の表形式で構造化データを表現する DataFrame を導入しています。"}
{"category":"closed_qa","context":"","instruction":"MLlibとは？","response":"MLlib は、PySpark のラッパーで、Spark の機械学習（ML）ライブラリです。このライブラリは、データの蓄積および集計に並列処理を行います。MLlib ライブラリが提供する機械学習APIは、簡単に利用することができます。MLlib は、分類、回帰、クラスタリング、協調フィルタリング、次元削減、基礎となる最適化プリミティブのための多くの機械学習アルゴリズムをサポートしています。"}
{"category":"closed_qa","context":"","instruction":"GraphFramesとは？","response":"GraphFrames は、グラフデータ処理ライブラリで、PySpark Coreと PySparkSQL を使用して、グラフ分析を効率的に行うためのAPIセットを提供します。また、高速な分散コンピューティングに最適化されています。 PySpark を使用するメリット。\n・Python の学習や実装は極めて容易\n・シンプルで包括的な API を提供\n・Python の使用で、コードの可読性、保守性、使いやすさが向上\n・Scala やJava では困難なデータ可視化に多くの選択肢が充実"}
{"category":"closed_qa","context":"","instruction":"レジリエントな分散データセット(RDD)とは？","response":"レジリエントな分散データセット(RDD)は、分散コンピューティングを用いたレコードコレクションです。フォールトトレラントで不変な性質を有しています。RDDは、低レベルAPIとの並列操作が可能で、遅延機能によりSparkの操作を迅速化します。"}
{"category":"closed_qa","context":"","instruction":"Spark Elasticsearch とは？","response":"Spark Elasticsearch とは、ドキュメント指向および半構造化データを格納、取得、管理する NoSQL 分散データベースです。GitHub オープンソースである Elasticsearch は、Apache Lucene をベースに構築され、Apache ライセンスの条件下でリリースされた RESTful な検索エンジンでもあります。"}
{"category":"closed_qa","context":"","instruction":"Spark SQL とは？","response":"Spark SQL とは、構造化データ処理のためのSparkモジュールです。DataFrames と呼ばれるプログラミングの抽象化が可能で、分散型 SQL クエリエンジンとしても機能します。これにより、既存のデプロイやデータで未修正の Hadoop Hive クエリを最大 100 倍の速さで実行できるようになりました。また、他の Spark エコシステムとの統合も可能です。(例: SQL クエリ処理と機械学習の統合)"}
{"category":"closed_qa","context":"","instruction":"Sparklyrとは？","response":"Sparklyr とは、R 言語と Apache Spark 間のインターフェースを提供するオープンソースのパッケージです。Spark では、分散データを低レイテンシで扱えるため、Spark の機能を最新のR環境で活用することができるようになりました。Sparklyr は、インタラクティブな環境にある大規模なデータセットと連動するための有効なツールです。これにより、Spark でデータを分析するために、R の使い慣れたツールを使用することが可能となり、R と Spark 両方のメリットを享受できます。"}
{"category":"closed_qa","context":"","instruction":"SparkRとは？","response":"SparkR とは、R 言語を Spark 上で動作させるためのツールです。Spark の他の言語バインディングと同じ原理に基づいています。SparkR を使用するには、環境にインポートしてコードを実行するだけです。Python ではなくR 言語の構文に従っていることを除けば、Python API と非常に類似しています。ほとんどの場合、Python で利用可能なものは、SparkR でも利用できます。"}
{"category":"closed_qa","context":"","instruction":"Spark アプリケーションとは？","response":"Spark アプリケーションとは、ドライバプロセスと一連のエグゼキュータプロセスで構成されるアプリケーションプログラムです。ドライバプロセスは、main() 関数を実行し、クラスタのノード上で動作します。また、3 つの役割があり、Spark アプリケーションに関する情報管理、ユーザーのプログラムや入力への応答、およびエグゼキュータ（瞬間的に定義）全体におけるタスクの分析、分散、スケジューリングを行います。ドライバプロセスは必要不可欠です。Sparkアプリケーションの中心であり、アプリケーションのライフタイム全体にわたって全ての関連情報を保持します。エグゼキュータは、ドライバが割り当てたタスクを実行します。各エグゼキュータの役割としては、ドライバから割り当てられたコードの実行、およびそのエグゼキュータでの計算の進捗をドライバノードに報告するという2つがあります。"}
{"category":"closed_qa","context":"","instruction":"Sparkストリーミングとは？","response":"Apache Sparkストリーミングとは、スケーラブルで耐障害性に優れた特性を持つストリーミング処理システムです。バッチ処理とストリーミング処理のワークロードをネイティブにサポートしています。Spark ストリーミングは、コアのSpark APIを拡張したもので、データエンジニアやデータサイエンティストは、KafkaやFlume、Amazon Kinesisなどの複数のソースからリアルタイムデータを処理することが可能です。処理されたデータは、ファイルシステムやデータベース、ライブダッシュボードに出力することができます。その鍵となる抽象化は、小さなバッチに分割されたデータのストリームを表現する「離散ストリーム（DStream）」です。DStreamは、Sparkのコアなデータ抽象化機能であるRDD を基盤に構築されています。これにより、SparkストリーミングはMLlibやSpark SQLのような他のSparkコンポーネントとシームレスに統合することが可能となります。Sparkストリーミングは、ストリーミングのためだけに設計された処理エンジンを備えているシステムや、Sparkストリーミングと類似したバッチとストリーミングAPIを持ちながら、内部的には異なるエンジンにコンパイルされるシステムとは異なります。Sparkの単一実行エンジンと、バッチとストリーミングのための統合プログラミングモデルは、従来のストリーミングシステムにはないメリットをもたらします。"}
{"category":"closed_qa","context":"","instruction":"Spark ストリーミングの4つの特徴は何？","response":"Spark ストリーミングの4つの特徴：1. 障害時における迅速なリカバリ, 2. 優れた負荷分散とリソース使用率, 3. ストリーミングデータと静的データセット、対話型クエリの統合, 4. 高度な処理ライブラリ（SQL、機械学習、グラフ処理）とのネイティブ統合。\n このような異なるデータ処理機能の統合は、Sparkストリーミングを急速に普及させる大きな理由になりました。この統合により、開発者は単一のフレームワークを使用して、容易にあらゆる処理ニーズに対応できます。"}
{"category":"closed_qa","context":"","instruction":"Sparkパフォーマンスチューニングとは？","response":"Sparkパフォーマンスチューニングとは、システムが使用するメモリやコア、インスタンスなどを記録するための設定を調整する処理のことです。この処理により、Sparkは優れた性能を発揮し、リソースのボトルネックの防止も可能になります。"}
{"category":"closed_qa","context":"","instruction":"データのシリアライズとは？","response":"メモリ使用量を削減するために、Spark RDDをシリアル化して格納する必要があります。また、データのシリアライズは、ネットワークのパフォーマンスにも影響します。Sparkの性能を向上させるには、次のような処理が挙げられます。\n 長時間実行されているジョブを終了 \n 適切な実行エンジンでジョブを実行 \n 全てのリソースを効率的に使用 \n システムがパフォーマンスに要する時間を改善。\n Sparkは、次の2つのシリアライズライブラリをサポートしています。1. Javaのシリアライズ, 2. Kryoのシリアライズ"}
{"category":"closed_qa","context":"","instruction":"メモリ使用量の調整に注意点が何？","response":"メモリ使用量の調整には、次の3つの留意点があります。1. データセットは全てメモリに収める必要があるため、オブジェクトによって使用されるメモリ量の考慮が不可欠になること。 2. オブジェクトの回転率の上昇により、ガベージコレクションのオーバーヘッドが必要になること。3. オブジェクトにアクセスするコスト"}
{"category":"closed_qa","context":"","instruction":"データ構造のチューニングとは？","response":"メモリの消費量を削減するためには、オーバーヘッドになりうるJava機能を避けるという選択肢があります。具体的な方法は次のとおりです。\n 1. RAMサイズが32GB未満の場合、JVMフラグを–xx:+ UseCompressedOopsに設定し、ポインタを8バイトから4バイトにする \n 2. 複数の小さなオブジェクトとポインタを使用し、入れ子構造を避ける。 \n 3. キーに文字列を使用する代わりに、数値IDまたは列挙型オブジェクトを使用する"}
{"category":"closed_qa","context":"","instruction":"ガベージコレクションのチューニングとは？","response":"プログラムが以前確保した不要なRDDがメモリ領域を占有するのを避けるため、Javaは古いオブジェクトを削除し、新しいオブジェクトにスペースを割り当てます。一方、オブジェクトの少ないデータ構造を使用することで、コストは大きく削減されます。例として、LinkedListの代わりにIntの配列を使用することが挙げられます。または、シリアル化されたオブジェクトを使用することで、オブジェクトはRDDパーティションごとに1つのみとなります。"}
{"category":"closed_qa","context":"","instruction":"メモリ管理とは？","response":"優れた性能を発揮するには、メモリの効果的な使用が必要不可欠です。Sparkでは、主に保存と実行のためにメモリを使用します。保存メモリは、後に再利用するデータをキャッシュするために使用されます。一方、実行メモリは、シャッフル、ソート、結合、集約などの計算に使用されます。Apache Sparkが解決しなくてなならないメモリの競合の課題は次の3つです。1. 実行メモリと保存メモリのアービトレート。 2. 同時に実行しているタスクのメモリのアービトレート。3. 同一タスク内で実行しているオペレータのメモリのアービトレート。\n 事前にメモリの静的確保を避けるのではなく、メモリからデータを強制的に解放し、発生したメモリの競合に対応することが可能です。"}
{"category":"closed_qa","context":"","instruction":"Sparse Tensorとは？","response":"Sptensor は、Sparse Tensor を表すクラスです。Sparse Tensor とは、エントリの大部分がゼロであるデータセットです。例としては、大規模な対角行列（多くがゼロ要素）が挙げられます。Tensor オブジェクトの値全体を保存するのではなく、非ゼロ値とそれに対応する座標を保存します。Sparse Tensor のストレージ形式では、非ゼロ値のみを保存することが可能なため、保存要件を減らし、ゼロ値を含む不要なサイレント計算を排除することができます。"}
{"category":"closed_qa","context":"","instruction":"TensorFlowとは？","response":"TensorFlow は、数値計算、大規模な機械学習、深層学習、その他の統計および予測分析のワークロードに利用されるオープンソースライブラリです。このような技術は、開発者が機械学習モデルをより迅速かつ容易に実装することを可能にします。データの取得、規模に応じた予測の提供、将来の学習結果の洗練といったプロセスを支援するためです。"}
{"category":"closed_qa","context":"","instruction":"TensorFlow Liteとは？","response":"TensorFlow Lite は、組み込み機器やモバイル機器での利用に最適化されています。オープンソース、プロダクトレディ、かつクロスプラットフォームの深層学習フレームワークで、事前学習された TensorFlow モデルを特別な形式に変換し、速度やストレージを最適化します。"}
{"category":"closed_qa","context":"","instruction":"TensorFlow と Python の違いは何？","response":"TensorFlow はオープンソースの機械学習フレームワークであり、Python は人気のプログラミング言語の 1 つで、TensorFlow にも使用されています。TensorFlow の推奨言語は Python ですが、C++ や JavaScript も使用可能です。\n Python は、大小さまざまなプロジェクトで、プログラマーの簡潔で論理的なコードを支援することを目的に開発されました。Web サイトやソフトウェアの構築、タスクの自動化、データ分析などによく利用されます。そのため、初心者でも比較的簡単に TensorFlow を習得できます。\n TensorFlow がサポートする Python のバージョンは何か、というのは有用な質問です。TensorFlow のリリースによっては、特定のバージョンの Python にしか対応していないものがあり、2.0 では Python 3.7 から 3.10 までが対象です。TensorFlow をインストールする前に、必ず要件を確認してください。"}
{"category":"closed_qa","context":"","instruction":"TensorFlow と PyTorch はどう違うのでしょうか？","response":"PyTorch と TensorFlow の決定的な違いは、コードの実行方法です。PyTorch は、Python とより密に統合されています。\n TensorFlow は堅牢な可視化機能、プロダクション向けのデプロイメントオプション、モバイルプラットフォームのサポートなどを備えています。PyTorch はそれほど確立されたものはありませんが、そのシンプルさと使いやすさに加え、動的な計算グラフや効率的なメモリ使用で人気を博しています。\n TensorFlow と Pytorch のどちらが優れているかという質問については、何を実現したいかによります。AI 関連の製品を作ることが目的なら TensorFlow、研究志向の開発者なら PyTorch が向いていると言われています。PyTorch はプロジェクトを短期間で立ち上げるのに適していますが、TensorFlow はより大規模なプロジェクトや、より複雑なワークフローに対応できる堅牢な機能を備えています。"}
{"category":"closed_qa","context":"","instruction":"TensorFlow と Pytorch のどちらが優れているか？","response":"TensorFlow と Pytorch のどちらが優れているかという質問については、何を実現したいかによります。AI 関連の製品を作ることが目的なら TensorFlow、研究志向の開発者なら PyTorch が向いていると言われています。PyTorch はプロジェクトを短期間で立ち上げるのに適していますが、TensorFlow はより大規模なプロジェクトや、より複雑なワークフローに対応できる堅牢な機能を備えています。"}
{"category":"closed_qa","context":"","instruction":"DataRobotとは？","response":"DataRobot は、クラウドベースの機械学習フレームワークです。機械学習モデルを導入し、高度な AI アプリケーションを作成することで、企業がデータサイエンス能力を拡張できるように設計されています。\n このフレームワークでは、R、Python、Spark、H2O、VW、XGBoost などのオープンソースのモデリング技術を利用し、モデルを最適化することが可能です。DataRobot は、予測分析を自動化することで、データサイエンティストやアナリストがより正確な予測モデルを作成できるよう支援します。\n 各モデルを構築するための最適な機能、アルゴリズム、パラメータを提供するライブラリは増え続けています。さらに、自動アンサンブルにより、ユーザーは特徴抽出とデータ準備のために複数のアルゴリズムと事前構築されたプロトタイプを簡単に検索して組み合わせることができます。トライ＆エラーの推測は必要ありません。"}
{"category":"closed_qa","context":"","instruction":"KerasとTensorFlowはどのような関係ですか？","response":"Keras は、ユーザーフレンドリーでモジュール化され、簡単に拡張できるように設計された高水準のオープンソースのニューラルネットワークライブラリです。Python で書かれており、複数のバックエンドのニューラルネットワーク計算エンジンをサポートしています。ただし、主要な（そしてデフォルトの）バックエンドは TensorFlow で、その主要なサポーターは Google です。"}
{"category":"closed_qa","context":"","instruction":"MXNetとは？","response":"Apache MXNet は、オープンソースの深層学習ソフトウェアフレームワークで、深層ニューラルネットワークの定義、学習、幅広いデバイスへのデプロイに使用されています。AWS における深層学習フレームワークの最高峰として、Amazon に採用された実績があります。"}
{"category":"closed_qa","context":"","instruction":"KerasはGPU と分散学習を強力にサポートしていますか？","response":"はい、Keras は複数の GPU と分散学習を強力にサポートします。"}
{"category":"closed_qa","context":"","instruction":"CNTKとは？","response":"CNTK は、Microsoft Cognitive Toolkit とも呼ばれ、グラフ構造を使ってデータの流れを一連の計算手順として記述する、統一された深層学習ツールキットです。TensorFlow と同じですが、学習や導入はそれほど簡単ではありません。"}
{"category":"closed_qa","context":"","instruction":"TensorFlow を使うべきか？","response":"TensorFlow には、多くのメリットがあります。オープンソースの機械学習フレームワークは、優れたアーキテクチャをサポートしており、さまざまなプラットフォームで計算フレームワークを容易にデプロイできます。Google の評判の良さもあり、いくつかの大手企業が TensorFlow を採用して人工知能のタスクを実行しています。\n 一方、TensorFlow の実装の詳細によっては、完全に決定論的なモデル学習結果を得ることが困難な場合もあります。しかし、開発チームは、ワークフローにおける決定論に影響を与える制御を増やすことを検討しています。\n TensorFlow の導入はシンプルです。特に、TensorFlow by Databricks は、Databricks Runtime for Machine Learning を介してすぐに統合できるため、簡単に始めることができます。クラスタを数秒で立ち上げることができ、低レベルおよび高レベルの API を幅広く利用できます。"}
{"category":"closed_qa","context":"","instruction":"Tungsten プロジェクトとは？","response":"Tungsten は、Apache Spark の実行エンジンを変更する包括プロジェクトのコードネームです。Spark アプリケーション向けのメモリと CPU の効率を大幅に向上させることに重点を置き、性能を最新のハードウェアの限界に近づけます。"}
{"category":"closed_qa","context":"","instruction":"アノマリー検知とは？","response":"アノマリー検知とは、定常状態とは統計的に異なる不審なイベントや観測値を特定する手法です。異常検知とも呼ばれます。このような「異常」な挙動は、多くの場合に、クレジットカードの不正使用、マシンの故障、サイバー攻撃といった問題の存在を意味します。例えば、膨大な数のトランザクションの監視が必要な金融業界では、アノマリー検知がエラーの発生場所の特定や原因の分析を支援し、問題への迅速な対応を可能にします。また、検知した異常値をもとにしたアラートの発行にも活用され、担当者の行動を促します。そこから得られる情報は、カオスエンジニアリングにも利用されます。最近では、詐欺行為の検知やマネーロンダリング対策（AML）に機械学習や AI が活用されるケースが増えています。"}
{"category":"closed_qa","context":"","instruction":"オルタナティブ（代替）データとは？","response":"オルタナティブデータ（代替データとも呼ばれる）とは、従来のソースではなく、他のユーザーによって使用されていない代替データソースから収集されたデータ情報です。オルタナティブデータを分析に活用することで、業界標準のデータソースでは得ることができない洞察を取得することが可能です。ただし、正確には何をオルタナティブデータとみなすかは業界によって異なり、自社や競合他社で既に使用されている従来のデータソースに依存されています。"}
{"category":"closed_qa","context":"","instruction":"オーケストレーションとは？","response":"オーケストレーションとは、複数のコンピュータシステム、アプリケーション、サービスを調整および管理し、大規模なワークフローやプロセスを実行するために複数タスクをつなぎ合わせることです。これらのプロセスは、自動化された複数タスクで構成され、複数のシステムをまたぐこともあります。\n オーケストレーションは、頻繁に繰り返されるプロセスの実行を効率化および最適化し、データチームが複雑なタスクやワークフローを容易に管理できるようにします。プロセスはいつでも繰り返しが可能で、タスクは自動化できるため、オーケストレーションは時間の節約、効率性の向上、重複の排除を実現します。例えば、Databricks のジョブオーケストレーションは、データや機械学習をシンプルにします。"}
{"category":"closed_qa","context":"","instruction":"プロセスオーケストレーションとプロセス自動化は何か違いか？","response":"自動化とオーケストレーションは、極めて補完的な関係にあります。しかし、それぞれの意味は異なります。自動化とは、人の手を介することなく実行されるタスクをプログラミングすることです。オーケストレーションとは、複数のタスク（一部は自動化されている場合もある）を 1 つの完全なエンドツーエンドのプロセスまたはジョブに設定することです。オーケストレーションソフトウェアは、プロセス全体のイベントまたはアクティビティに反応し、1 つの自動化タスクからの出力に基づいて意思決定を行い、次のタスクを決定し調整する必要があります。"}
{"category":"closed_qa","context":"","instruction":"オープンバンキングとは？","response":"オープンバンキングとは、消費者の事前同意のもとに、消費者の金融データへのアクセスをセキュアに共有する方法です²。規制や技術革新、競合の勢いに後押しされ、オープンバンキングは、銀行以外の第三者や消費者などが顧客データをさらに活用できるよう、顧客データの民主化を呼びかけています。この技術革新は、銀行業界を他業界との高い連携性を持つプラットフォーム提供者へと進化させると同時に、銀行にエコシステムを拡大し、新規市場への参入機会を与えています。"}
{"category":"closed_qa","context":"","instruction":"サービスとしての Apache Spark（Apache Spark as Spark-as-a-Service）とは？","response":"データブリックスでは、Apache Sparkの最適化されたバージョンを、Spark-as-a-Service（サービスとしてのSpark）として複数のクラウドでホストしています。サービスとしての Spark には、データアクセスおよびデータ分析を迅速に行うアプリケーションが組み込まれており、ストリーミングデータの処理、グラフ計算の実行、Hadoop で SQL の提供、機械学習機能など、ビッグデータ上で動作する Spark の多くの機能を活用しています。"}
{"category":"closed_qa","context":"","instruction":"スタースキーマとは何？","response":"スタースキーマとは、データベース内のデータを整理することで理解・分析しやすくなった多次元データモデルで、データウェアハウスやデータベース、データマート、その他のツールに適用できます。スタースキーマの設計は、大規模なデータセットへのクエリを実行するために最適化されています。"}
{"category":"closed_qa","context":"","instruction":"スタースキーマと 3NF の違いを知りたいです。","response":"3NF（第 3 正規形）とは、正規化によってデータの冗長性を減らす手法であり、完全に正規化されたとみなされるデータベースの標準規格です。データの正規化により、スタースキーマと比べて多くのテーブルを持つのが一般的です。一方で、大規模なテーブル間の結合が増えるため、クエリが複雑になる傾向があります。"}
{"category":"closed_qa","context":"","instruction":"スノーフレークスキーマとは？","response":"スノーフレークスキーマは、スタースキーマを拡張した多次元データモデルで、ディメンションテーブルがサブディメンションテーブルに細分化されたものです。スノーフレークスキーマは、データウェアハウスやデータマート、リレーショナルデータベースの多次元分析を使用した BI（ビジネスインテリジェンス）やレポーティングによく使用されています。"}
{"category":"closed_qa","context":"","instruction":"スノーフレークスキーマとスタースキーマの比較を教えてください。","response":"スタースキーマと同様に、スノーフレークスキーマの中央にもファクトテーブルがあり、外部キーを介して複数のディメンションテーブルに接続されています。しかし、スタースキーマと異なり、スノーフレークスキーマはより正規化が進んでいます。\n スノーフレークスキーマは、高い正規化基準に厳密に準拠しており、ストレージ効率性は高いのですが、非正規化データモデルと比べるとクエリ時のパフォーマンスは劣ります。一方で、スタースキーマのような非正規化データモデルでは、データの冗長性（データの重複）が高く、データの重複を許容する代わりにクエリのパフォーマンスを高速化することができます。"}
{"category":"closed_qa","context":"","instruction":"データウェアハウスとは何ですか？","response":"データウェアハウス（DWH）は、複数のソースから得られた最新データや履歴データをビジネスに適した形で蓄積し、知見の取得やレポート作成を容易にするデータ管理システムです。主に、ビジネスインテリジェンス（BI）、レポート作成、データ分析に使用されます。\n データウェアハウスでは、POS システム、インベントリ管理システム、マーケティングや販売データベースなどの業務システムに蓄積されたデータを、迅速かつ容易に分析可能です。データは、オペレーショナルデータストアを中継する場合があり、データウェアハウスでレポート作成のために使用する前にデータクレンジングを行い、データ品質を確保する必要があります。"}
{"category":"closed_qa","context":"","instruction":"ETL と ELTの違いは何？","response":"データの変換には、ETL（抽出、変換、ロード）と ELT（抽出、ロード、変換）の 2つの方法があります。データエンジニアは通常、ETL（抽出・変換・ロード）を使用し、さまざまなデータソースから抽出したデータをデータウェアハウスに移動させて、データクレンジングやデータの構造化を行います。一方、ELT では、まず元の形式のデータをデータウェアハウスにロードし、処理する際にデータクレンジングと構造化を行います。\n ETL は通常、全社的なデータクレンジングと準拠規則を適用するために、企業のデータエンジニアリングチームによって一元的に行われます。一方、ELT は、プロジェクトやビジネスチームに特化した変換が後の段階で行われ、セルフサービス分析を可能にします。"}
{"category":"closed_qa","context":"","instruction":"トランザクション処理（OLTP）と分析処理（OLAP）の比較を知りたいです。","response":"オンライントランザクション処理（OLTP）システムは、データベース内のトランザクションデータの取得および管理を行います。トランザクションには、複数のフィールドやカラムで構成される独立したデータベースレコードが含まれます。OLTP データベースは、オンラインバンキング、ERP システム、インベントリ管理などのアプリケーションで頻繁に使用され、瞬時に処理される行レベルのデータを迅速に更新します。\n オンライン分析処理（OLAP）システムは、データマイニング、分析、ビジネスインテリジェンスのプロジェクトに使用され、OLTP データベースやその他のソースから集約された膨大な履歴データに複雑なクエリを適用します。データウェアハウスは OLAP システムです。OLAP データベースとデータウェアハウスにより、アナリストや意思決定者は、カスタムレポートツールを使用して、データドリブンな情報の収集と意思決定ができるようになります。OLAP データベースでクエリに失敗しても、顧客のトランザクション処理が中断したり遅延したりすることはありませんが、ビジネスインテリジェンスに関する分析結果の取得が遅れたり、精度に影響を及ぼしたりする可能性があります。"}
{"category":"closed_qa","context":"","instruction":"データガバナンスとは？","response":"データガバナンスとは、データがビジネス戦略に沿った価値をもたらすよう、組織内のデータを統制することを意味します。単なるツールやプロセスにとどまらず、人、プロセス、技術、データを包括するフレームワークを用いてデータを統制し、ビジネスの目標達成を支援するものです。"}
{"category":"closed_qa","context":"","instruction":"データ管理とデータガバナンスの違いは何ですか？","response":"データ管理は、信頼性のあるデータを提供するために、データガバナンスのポリシーや原則、基準に準拠することを重要視した実務です。データ管理は通常、プロジェクト中心の短期的なものです。一方、データガバナンスは、長期的なメリットの実現を目指すプログラムとして扱われます。ガバナンスの実施には、一元化されたガバナンスツールが重要な役割を果たします。"}
{"category":"closed_qa","context":"","instruction":"データレイクハウスとは？","response":"データレイクハウスとは、データレイクの柔軟性、経済性、スケーラビリティとデータウェアハウスのデータ管理や ACID トランザクションの機能を取り入れたオープンで新たなデータ管理アーキテクチャで、あらゆるデータにおけるビジネスインテリジェンス（BI）と機械学習（ML）を可能にします。"}
{"category":"closed_qa","context":"","instruction":"データ分析プラットフォームとは？","response":"データ分析プラットフォームとは、膨大で複雑な動的データの分析に必要なサービスとテクノロジーのエコシステムです。企業が所有する各種ソースからのデータの取得、結合、連動、検索、視覚化を可能にします。包括的なデータ分析プラットフォームには、予測分析、データ視覚化、ロケーションインテリジェンス、自然言語、コンテンツ分析など、さまざまな機能を搭載した複数のツールが組み込まれています。その主な目的は、あらゆる種類のデータを実用的な洞察に変換し、真のビジネス成果につなげることです。"}
{"category":"closed_qa","context":"","instruction":"ニューラルネットワークとは？","response":"ニューラルネットワークとは、層状構造が人間の脳内にあるニューロンのネットワーク構造に類似した数理モデルです。ニューロンと呼ばれる相互に結合する処理要素を特徴としており、出力機能を生成します。ニューラルネットワークは、入力層と出力層で構成されており、その多くには隠れ層があります。この隠れ層は、入力を出力層で使用できるものに変換するユニットで構成されています。"}
{"category":"closed_qa","context":"","instruction":"ハッシュバケットとは？","response":"コンピューティングにおけるハッシュテーブル [ハッシュマップ] とは、キー [一意の文字列または整数] に基づいてオブジェクトに事実上直接アクセスできるデータ構造です。ハッシュテーブルは、バケットやスロットの配列にインデックス計算を行うために、ハッシュ関数を使用し、そこから目的の値をみつけます。使用されるキーの主な特徴は次のとおりです。1. 社会保障番号、電話番号、口座番号などのキーを使用します。2. キーは一意である必要があります。3. 各キーは、値に関連付け（マッピング）されます。"}
{"category":"closed_qa","context":"","instruction":"ビッグデータ分析とは？","response":"ビッグデータ分析とは、 eコマース、モバイルデバイス、ソーシャルメディア、IoT などのさまざまなソースから生成された大規模で多様なデータセット、ビッグデータを検証することで、一般的に複雑なプロセスです。"}
{"category":"closed_qa","context":"","instruction":"ベイジアンニューラルネットワークとは何ですか？","response":"ベイジアンニューラルネットワーク（BNN）とは、過学習の制御を目的として、事後確率推定により標準ネットワークを拡張することを指します。広い視点からみると、ベイジアン手法は統計的方法論を使用して、モデルパラメータ（ニューラルネットワークの重みとバイアス）を含む、あらゆるものがそれに付随する確率分布を持つようにすることです。"}
{"category":"closed_qa","context":"","instruction":"Delta パイプラインデータセットはどんなもの？","response":"Delta パイプラインの主要な抽象化手法は、データセットとして知られています。データセットとは、基本的にベーステーブルのクエリやビューのことで、API を使用して表示されます。"}
{"category":"closed_qa","context":"","instruction":"マネージドSparkとは何？","response":"マネージド Spark は、バッチ処理、クエリ、ストリーミング、機械学習などのオープンソースのデータツールを利用できるマネージドサービスです。ユーザーは、このような自動化を使用することで、オンデマンドでクラスタの迅速な作成や管理を容易し、タスクが完了したときにクラスタをオフにすることができます。ワークロード、パフォーマンス要件、または既存のリソースに基づいてクラスタのサイズを設定することも可能です。さらに、ほんの数秒で動的にスケールアップおよびスケールダウンできる、完全に管理された Spark クラスタへのアクセス権が付与されます。これは、ジョブの処理中でも実行できます。さらに、ユーザーは不要になったらクラスタをオフにすることで、コストを節約できます。マネージド Spark プロバイダーは、プロビジョニングを行い、全てのジョブのクラスタを保持する代わりに、一時クラスタを作成します。通常、マスターノードとワーカーノードを持つマシンのクラスタを使用します。マネージド Spark のサービスを採用することで、組織は貴重なリソースを運用ではなく分析に活用することができ、データから価値を引き出す作業に集中できるようになります。"}
{"category":"closed_qa","context":"","instruction":"モデルリスク管理はどういうこと？","response":"モデルリスク管理とは、モデルの誤りまたは誤用に基づく意思決定によって生じる潜在的な悪影響がもたらすリスクを管理することです。モデルリスク管理は、モデルリスク、すなわちモデルの誤りや誤用の可能性を特定、測定、軽減する技術や手法を取り入れることを目的にしています。金融サービスにおけるモデルリスクとは、精度が低いモデルを使用して意思決定を行うことで生じる損失リスクを意味します。多くの場合は金融証券の評価に使用され、消費者信用スコアの付与、クレジットカードの不正取引のリアルタイムな確率予測、マネーロンダリングといった範囲にも広がっています。金融機関では信用や市場への依存度が高く、モデルリスクに対する行動モデルは、リスク管理や業務効率化において重要な要素となっています。このような金融機関は、主にリスクを負うことで利益を得ています。そのため、リスクの評価や顧客行動の把握、コンプライアンス対応の自己資本の評価、投資の意思決定、データ分析の管理にモデルを最大限に活用しています。効果的なモデルリスク管理のフレームワークの導入は、業務や意思決定において定量モデルに大きく依存している組織には不可欠です。"}
{"category":"closed_qa","context":"","instruction":"ラムダアーキテクチャとは何？","response":"ラムダアーキテクチャとは、膨大なデータ「ビッグデータ」を処理するアプローチです。ハイブリッドアプローチを使用してバッチ処理やストリーム処理メソッドへのアクセスを提供し、任意の関数を計算する問題を解決するために使用されます。ラムダアーキテクチャは3つのレイヤーから構成されています。"}
{"category":"closed_qa","context":"","instruction":"リテール向けレイクハウスについて教えてください。","response":"リテール向けレイクハウスは、Databricks 初の業界特化型レイクハウスです。ソリューションアクセラレータ、データ共有のケイパビリティ、パートナーエコシステムを通じて、小売業者の迅速な業務遂行を支援します。"}
{"category":"closed_qa","context":"","instruction":"予測分析とは何？","response":"予測分析とは、新しいデータと過去のデータを活用してパターンを見つけ出し、将来の結果や傾向を予測する高度な分析手法です。"}
{"category":"closed_qa","context":"","instruction":"予測型メンテナンスとは何？","response":"予測型メンテナンスとは、一言でいうと、予め定められたスケジュールだけでなく、設備の実際の状態や状況に基づき、いつ頃、具体的にどのようなメンテナンスを行うべきかを判断し、設備の稼働時間と生産性を最大化するためのものです。故障を予測や予防し、適切な定期メンテナンスを実施することで、コストのかかる機器のダウンタイムを削減できます。\n IoT とセンサーデータが機器からストリーミングされることで、予測型メンテナンスは、製造業者が効率的に機械が停止するタイミングを予測することを可能にします。データは、ばらつきを検知し警告信号を把握することで、故障の可能性を示すあらゆるパターンを特定します。製造業者は、分析と機械学習を使用し機械が停止する確率を正確に予測できます。これにより、早期修正対策の計画（スペアパーツの発注や修理のスケジューリングなど）と最も効果的な方法での導入が可能になり、予想外のダウンタイムとコストがかかってしまうスタッフやリソースを回避できます。"}
{"category":"closed_qa","context":"","instruction":"人工ニューラルネットワーク（ANN）は何？","response":"人工ニューラルネットワーク（ANN）は、階層で構成される重み付き有向グラフにするとわかりやすく、これらの階層は人間の脳の生体ニューロンを模した多数のノードを特徴とし、相互に接続され、活性化関数を含みます。"}
{"category":"closed_qa","context":"","instruction":"構造化ストリーミングとは何？","response":"構造化ストリーミングとは、ストリーミングデータを処理するための高レベル API です。Spark 2.2 で実運用が可能になりました。構造化ストリーミングでは、Spark の構造化 API を使用してバッチモードで実行するのと同じ操作が、ストリーミング形式で実行可能です。これにより、レイテンシの短縮、インクリメンタル処理が可能になります。構造化ストリーミングの最大のメリットは、事実上コードを変更することなく、ストリーミングシステムから迅速に価値を引き出すことができることです。また、バッチジョブをプロトタイプとして記述し、それをストリーミングジョブに変換できるため、推論も容易になります。これはデータを段階的に処理することで可能になります。"}
{"category":"closed_qa","context":"","instruction":"機械学習パイプラインはどういう機能ですか？","response":"機械学習パイプラインとは、'spark.ml' パッケージ下にあるMLlibのための高レベルAPIです。パイプラインには、一連のステージがあり、基本的なパイプラインのステージタイプは、トランスフォーマーとエスティメーターの2つです。トランスフォーマーは、データセットを入力として受け取り、出力として拡張データセットを生成します。例えば、トークナイザは、テキストのデータセットをトークン化した単語のデータセットに変換するトランスフォーマーです。エスティメーターは、入力データセットを変換するトランスフォーマーであるモデルを生成するために、最初に入力データセットに適合させる必要があります。例えば、ロジスティック回帰は、ラベルと特徴量でデータセットをトレーニングし、ロジスティック回帰モデルを生成するエスティメーターです。 "}
{"category":"closed_qa","context":"","instruction":"機械学習モデルとは何？","response":"機械学習モデルとは、未知のデータセットからパターンを発見したり、判断を導き出すプログラムのことです。例えば、自然言語処理では、機械学習モデルにより、これまで聞き取れなかった文章や単語の組み合わせの背後にある意図を解析し、正しく認識できます。また、画像認識では、機械学習モデルを学習させることで、車や犬などのオブジェクトを認識できます。機械学習モデルは、大規模なデータセットを用いて「トレーニング」することで、上述のようなタスクの実行が可能になります。トレーニングでは、機械学習アルゴリズムはデータセットから特定のパターンやアウトプットを発見するように最適化されます。このプロセスによるアウトプットは、多くの場合、特定のルールとデータ構造を持つコンピュータプログラムであり、機械学習モデルと呼ばれます。"}
{"category":"closed_qa","context":"","instruction":"機械学習アルゴリズムとは？","response":"機械学習アルゴリズムとは、データセットからパターンを見つけるための数学的手法です。機械学習アルゴリズムは、統計学、微積分学、線形代数学の知識が用いられていることが多く、代表的な例には、線形回帰、決定木、ランダムフォレスト、XGBoost があります。"}
{"category":"closed_qa","context":"","instruction":"トレーニングデータとは？","response":"トレーニングデータと呼ばれるデータセット上で機械学習アルゴリズムを実行し、特定のパターンやアウトプットを発見するためにアルゴリズムを最適化するプロセスを、モデルのトレーニングと呼びます。また、その結果として得られるルールとデータ構造を持つ関数を、トレーニング済み機械学習モデルと呼びます。"}
{"category":"closed_qa","context":"","instruction":"機械学習の手法は主に何種類がありますか？","response":"機械学習の手法は主に、教師あり学習、教師なし学習、強化学習に分類されます。"}
{"category":"closed_qa","context":"","instruction":"教師あり学習ではどういうことを行うか？","response":"教師あり学習では、アルゴリズムは提供されたインプットデータセットに対して特定のアウトプットのセットを満たすように正解を与えられ、最適化されます。例えば、画像認識には、教師あり学習の分類と呼ばれる手法が広く用いられています。また、人口増加や健康状態などの人口統計の予測には、教師あり学習の回帰と呼ばれる手法が利用されています。"}
{"category":"closed_qa","context":"","instruction":"教師なし学習ではどういうことを行うか？","response":"教師なし学習では、アルゴリズムは入力データセットを提供されますが、特定のアウトプットに対する正解は与えられず、共通の特性によってデータセット内のオブジェクトをグループ化するようにトレーニングされます。例えば、オンラインショップの推薦エンジンには、教師なし学習のクラスタリングと呼ばれる手法が用いられています。"}
{"category":"closed_qa","context":"","instruction":"強化学習ではどういうことを行うか？","response":"強化学習では、アルゴリズムは多くの試行錯誤を繰り返しながら、自らトレーニングを行います。強化学習は、アルゴリズムがトレーニングデータに依存するのではなく、環境と継続的に相互作用することで実現されます。強化学習の代表的な例として、自律走行が挙げられます。"}
{"category":"closed_qa","context":"","instruction":"機械学習ライブラリ（MLlib）とは？","response":"Apache Spark の機械学習ライブラリ（MLlib）とは、シンプルでスケーラビリティが高く、他のツールと容易に統合できるように設計された、機械学習を実装するためのツールです。Sparkのスケーラビリティ、言語の互換性、高速性により、データサイエンティストは、分散データを取り巻く複雑さ（インフラストラクチャ、構成など）の解決ではなく、データの問題とモデルに集中できます。Spark 上に構築されたMLlibは、分類、回帰、クラスタリング、協調フィルタリング、次元削減、基になる最適化プリミティブなど、一般的な学習アルゴリズムとユーティリティで構成されるスケーラブルな機械学習ライブラリです。"}
{"category":"closed_qa","context":"","instruction":"深層学習とは何？","response":"深層学習とは、人間の脳の構造と機能にインスパイアされたアルゴリズムを用いて膨大なデータを扱う機械学習のサブセットです。そのため、深層学習モデルはディープニューラルネットワークと呼ばれます。深層学習は、データ表現の学習に基づく機械学習手法の1つで、従来のタスク固有のアルゴリズムとは異なります。"}
{"category":"closed_qa","context":"","instruction":"深層学習層は何種類がありますか？","response":"基本的に三種類があります。1. 入力層：情報を受け取り、下層のノードに伝達します。ここで、ネットワークはローカルコントラストのパターンを重要なものとして、固定化します。2. 隠れ層：計算が表示される層で、ローカルコントラストのパターンを使用して類似のものに固定化します。3. 出力層：計算結果が表示されます。この層では、特徴量がテンプレートに適用されます。"}
{"category":"closed_qa","context":"","instruction":"総合人工知能（ UAI ）は何？","response":"総合人工知能（ UAI ）は、開発者カンファレンス「F8 」で Facebook によって発表されました。UAI は Facebook 主導で開発された、PyTorch と Caffe の 2 つの深層学習フレームワークを統合したもので、PyTorch は大規模なコンピューティングリソースへのアクセスを想定したリサーチに焦点を当て、Caffeは、Android や Raspberry Pi デバイスのモデル展開に焦点を当てています。"}
{"category":"closed_qa","context":"","instruction":"統合データウェアハウスとは？","response":"統合データウェアハウス（エンタープライズデータウェアハウスとも呼ばれる）は、業務に関わるあらゆるデータを保持し、エンタープライズ全体でのアクセスが可能な統合データベースです。今日、多くの企業においてデータはサイロ化されています。データの品質、統合、ガバナンスの保守や、メタやマスターデータ、B2B データ交換、データベース、アーキテクチャの管理など、同じ組織内の異なるそれぞれの部門で、多様なデータをさまざまなツールで管理しています。大企業におけるデータウェアハウス（ DW）の採用は、さまざまな異種オペレーショナルソースから抽出した統合および一元化されたデータを格納するためのベストプラクティスとなっています。DW を採用することで、運用システムのトランザクション処理と競合することなく、複雑なクエリを実行することができます。"}
{"category":"closed_qa","context":"","instruction":"耐障害性分散データセット（RDD）とは？","response":"耐障害性分散データセット（RDD）とは、クラスタ内の複数のノードに配置されたデータ要素の不変の集合体であり、変換その他の操作のための基礎的な API と並行して使用することが可能です。"}
{"category":"closed_qa","context":"","instruction":"複合イベント処理（CEP）とは？","response":"複合イベント処理（CEP）とは、イベント処理、ストリーム処理、あるいはイベントストリーム処理とも呼ばれ、データベースにデータを格納する前か、場合によっては格納せずに、データを照会する技術を使用した処理です。複合イベント処理は、多くの異なる情報を集約するのに有用で、イベント間の因果関係をリアルタイムで特定、分析する整理ツールです。CEPでは連続的に収集されるイベントをパターンと照合し、イベント内容についての洞察の提供、効果的なアクションの積極的な実行を可能にします。"}
{"category":"closed_qa","context":"","instruction":"Databricks コミュニティ エディションとは何ですか？","response":"Databricks Community Edition は、 クラウドベースのビッグ データ プラットフォームの無料バージョンです。そのユーザーは、クラスター マネージャーとノートブック環境だけでなく、マイクロ クラスターにもアクセスできます。すべてのユーザーは、ノートブックを共有し、Databricks を使用して無料でホストできます。これにより、Apache Spark™ コミュニティ全体に利益をもたらす新しいエキサイティングなコンテンツを誰もが作成できるようになることを願っています。\n\nDatabricks Community Edition には、受賞歴のあるトレーニング リソースの豊富なポートフォリオも付属しており、開発者、データ サイエンティスト、データ エンジニア、およびその他の IT プロフェッショナルが Apache Spark を学習するのに理想的です。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Edition はどこでホストされていますか?","response":"Databricks Community Edition は、Amazon Web Services でホストされています。ただし、Databricks Community Edition を使用する場合、AWS のコストは発生しません。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Editionはどれくらいしますか？","response":"Databricks Community Edition は無料です。プラットフォームの料金を支払う必要はなく、AWS のコストも発生しません。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Editionを使う場合、既に IPython ノートブックを持っていますが、Databricks ノートブックと互換性がありますか?","response":"Databricks Community Edition ノートブックは、IPython ノートブックと互換性があります。既存の IPython ノートブックを Databricks Community Edition ノートブック環境に簡単にインポートできます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Editionを使う場合、ノートブックで他に何ができますか?","response":"すべてのユーザーは、Databricks を使用してノートブックを無料で共有およびホストできます。これにより、Apache Spark コミュニティ全体に利益をもたらす新しいエキサイティングなコンテンツを誰もが作成できるようになることを願っています。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Editionでより多くの容量や機能が必要な場合はどうすればよいですか?","response":"Databricks Community Edition ユーザーは、サブスクリプションを完全な Databricks プラットフォームにアップグレードすることで、より多くの容量を取得し、運用グレードの機能を得ることができます 。アップグレードするには、  14 日間の無料試用版にサインアップする か、 お問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Edition と完全な Databricks Platform の違いは何ですか?","response":"Databricks Community Edition を使用すると、ユーザーは 15 GB のクラスター、クラスター マネージャー、および単純なアプリケーションのプロトタイプを作成するためのノートブック環境、および BI 分析のための JDBC / ODBC 統合にアクセスできます。Databricks Community Edition へのアクセスは時間制限がなく、ユーザーはクラスターの使用に対して AWS のコストを負担しません。\n\n完全な Databricks プラットフォームは 、簡単にスケールアップまたはスケールダウンできる無制限の数のクラスター、ジョブ ランチャー、コラボレーション、高度なセキュリティ コントロール、専門家によるサポートなど、運用レベルの機能を提供します。ユーザーがデータを大規模に処理したり、チーム設定で Apache Spark アプリケーションを構築したりするのに役立ちます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Editionを使っています。バグを報告したり、フィードバックを提供したりするにはどうすればよいですか?\n\n","response":"Databricks Community Edition に固有のバグやフィードバックについては、  feedback@databricks.comにメールでお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"Databricks Community Editionを使っています。データを Databricks の AWS アカウントに転送する必要がありますか?","response":"Databricks の AWS アカウントに保持する必要はありません。ほとんどの場合、S3 またはその他のデータ ソースの現在の場所からデータにアクセスできます。"}
{"category":"closed_qa","context":"","instruction":"パブリックとプライベートのインストラクター主導の配信とは何ですか?","response":"Databricks のインストラクター主導のトレーニングは、有料で誰でも利用できます。\n\n公開研修はスケジュールがあらかじめ決まっており、多くの団体の方が参加されます。クラスへの登録に関心のある出席者が数人しかいない場合に最適です。\n\nプライベート トレーニングは、よりパーソナライズされたトレーニング体験を希望し、コースのスケジュールと提供を管理したいグループ (組織内など) に利用できます。パブリック クラスまたはプライベート クラス用に新しいカスタム トレーニング コンテンツを作成することはありませんが、プライベート配信の場合は、既存のコースウェアをカスタムで組み合わせて作成できます。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のオンサイト/対面トレーニングを提供していますか?","response":"プライベート トレーニング クラスについては、COVID-19 のポリシーに従い、オンサイトでの移動を受け付けています。すべての公開トレーニング コースは、仮想的に提供されます。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のトレーニングは記録されますか？トレーニングを記録できますか？","response":"残念ながら、内部目的であろうとお客様のためにであろうと、インストラクターによるトレーニング クラスの記録は許可されていません。\n\n録音を禁止することで、コースを受講するインストラクターと受講生のプライバシーを保護することができます。また、トレーニング セッションを記録すると、学生が率直な質問をする可能性が低くなり、コースの教材に積極的に関与する気がなくなるため、効果的な教育体験が得られない可能性があることもわかりました。\n\n記録されたトレーニング資料にアクセスしたい場合は、Databricks Academy にログインすると、トレーニング資料の多くが事前に記録されており、自分のペースで進められる無料のコンテンツとして利用できます。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のトレーニングを受けたいです。ラーニング サブスクリプションまたはサクセス クレジットを持っている場合、トレーニングに登録するにはどうすればよいですか?","response":"あなたの会社がサクセス クレジットを購入したか、ラーニング サブスクリプションを持っている場合は、パブリック トレーニング リクエスト フォームに記入してください。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のトレーニングを受けたいです。トレーニングを複数のセッションに分割することはできますか (例: あるセッションの月曜日、別のセッションの火曜日から木曜日)?","response":"クラスの混乱を最小限に抑えるため、生徒が複数のクラスに参加することは許可されていません。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のトレーニングを受ける場合は、コース教材をエクスポートして、後で自分の時間にアクセスできますか?","response":"はい、コースの資料をエクスポートして、後で参照するために独自の Databricks ワークスペースにインポートすることができます。独自の Databricks ワークスペースがない場合は、  無料の Databricks Community Edition アカウントにサインアップできます (注: Community Edition ですべての機能が有効になっているわけではありません)。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のトレーニングを受講後に修了証はもらえますか？","response":"はい、修了証が発行されます。ただし、バッジを受け取るには、監督付きの認定評価を受けて合格する必要があります。"}
{"category":"closed_qa","context":"","instruction":"Databricks は他の言語でインストラクター主導のトレーニングを提供していますか?","response":"はい！他の言語で提供される今後の公開クラスを確認できます (英語以外の場合は言語を示します)。また、これをプライベートでリクエストすることもできます。対応するリソースがあるかどうかを確認します.\n\n注: 私たちのカリキュラムは、英語と日本語で利用できます。他のすべての言語については、インストラクターはその要求された言語で教えますが (空き状況に応じて)、コースウェア (ノートとスライド) は英語のままです。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のトレーニング(ILT)を受けたいです。プライベート トレーニング クラスをスケジュールするにはどうすればよいですか?","response":"プライベート トレーニング リクエストを送信してください 。Databricks は、通常、5 営業日以内にスケジュール リクエストに対応します。リクエストされた日付をサポートできる場合、Databricks はメールで確認し、リクエストされたトレーニングの実施日を一時的に「ロック」します。インストラクターの利用可能性を確認したら、最終確認で速やかにフォローアップします (「コミット セッション」)。それ以外の場合は、お客様と協力して、コミット セッションの代替日について合意します。なんらかの理由でインストラクターがコミット セッションに参加できなくなった場合、当社は速やかに電子メールで顧客に通知し、支払った関連料金、またはオーダー フォームで指定された有効期限内に予定されている他の将来のプライベート クラス セッションで使用するために引き換えたサクセス クレジットを返金します。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導のプライベートクラスのキャンセルと返金のポリシーを教えてください。","response":"インストラクター主導のプライベートクラスのキャンセルと返金のポリシーは以下です。\n予定された開始日の 14 日以上前にキャンセルされた場合: お客様は、再スケジュールされたクラスが注文フォームに示されている有効期限内にある限り、違約金なしで (1 回) スケジュールを変更することができます。そのようなクラスのそれ以上の再スケジュールには、30 日前の通知が必要であり、有効期限内の日付で行われるようにスケジュールする必要があります。\n開始予定日の 14 日以内にキャンセルされた場合: お客様は、支払った料金を放棄し、サクセス クレジットを引き換えます。ただし、お客様が有効期限内にキャンセルしたプライベート クラスの配信を再スケジュールする場合、お客様は、再スケジュールされたクラスに対して支払った料金または引き換えた成功クレジットの 50% を適用することができます。"}
{"category":"closed_qa","context":"","instruction":"インストラクター主導の公開クラスのキャンセルと返金のポリシーを教えてください。","response":"インストラクター主導の公開クラスのキャンセルと返金のポリシーはこちらです。\nDatabricks は、クラスの開始日の 14 日前に公開クラスを実行またはキャンセルすることを確認します。クラスの 14 日前まで、学習者は登録をキャンセルして全額返金するか、別の日にスケジュールを変更する権利を留保します。クラス開始まで14日未満の場合、キャンセルは認められませんが、代わりに代理を送ることができます。こちらからリクエストを送信してください 。 すべてのリクエストは、ケースバイケースで審査されます。\n参加者がクラスをキャンセルまたは欠席しない場合の返金または振替はありません。学習管理システムでは、セッションから登録を解除しても、コースに登録されたままになります。返金を受けるには、コースへの登録も解除する必要があります。\nDatabricks は、公開クラスをいつでもキャンセルまたは変更する権利を留保します。これには、参加者の不足、教室またはインストラクターの空き状況が含まれますが、これらに限定されません。クラスがキャンセルされた場合、学習者はメールで通知されます。学習者は、返金またはクラスの振替のいずれかを選択できます。通知から 2 営業日以内に応答がない場合、学習者は自動的に次のクラスに転送されます。"}
{"category":"closed_qa","context":"","instruction":"ファイルが追加または上書きされた場合、Auto Loader はファイルを再度処理しますか?","response":"cloudFiles.allowOverwritesが有効になっていない限り、ファイルは一度だけ処理されます。ファイルが追加または上書きされた場合、Databricks は、ファイルのどのバージョンが処理されるかを保証しません。Databricks では、Auto Loader を使用して不変ファイルのみを取り込むことをお勧めします。これが要件を満たしていない場合は、Databricks の担当者にお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"データ ファイルが継続的に到着せず、定期的に (たとえば 1 日 1 回) 到着する場合でも、Auto Loaderを使用する必要がありますか? 何かメリットはありますか?\n","response":"この場合は 構造化ストリーミング ジョブにTrigger.AvailableNow(Databricks Runtime 10.2 以降で使用できる)を設定し、予想されるファイルの到着時間の後に実行するようにスケジュールできます。Auto Loader は、更新頻度が低い場合でも頻繁な場合でもうまく機能します。最終的な更新が非常に大きい場合でも、Auto Loader は入力サイズに合わせて適切にスケーリングします。Auto Loader の効率的なファイル検出技術とスキーマ進化機能により、Auto Loader は増分データの取り込みに推奨される方法になっています。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderのストリームを再開するときにチェックポイントの場所を変更するとどうなりますか?","response":"チェックポイントの場所は、ストリームの重要な識別情報を保持します。チェックポイントの場所を変更するということは、事実上、以前のストリームを破棄して新しいストリームを開始したことを意味します。"}
{"category":"closed_qa","context":"","instruction":"Auto Loader に事前にイベント通知サービスを作成する必要がありますか?","response":"いいえ。ファイル通知モードを選択し、必要なアクセス許可を提供すると、Auto Loader はファイル通知サービスを作成できます。"}
{"category":"closed_qa","context":"","instruction":"Auto Loader によって作成されたイベント通知リソースをクリーンアップするにはどうすればよいですか?","response":"クラウド リソース マネージャーを使用して、リソースを一覧表示および破棄できます。クラウド プロバイダーの UI または API を使用して、これらのリソースを手動で削除することもできます。"}
{"category":"closed_qa","context":"","instruction":"Auto Loader で同じバケット/コンテナの異なる入力ディレクトリから複数のストリーミング クエリを実行できますか?","response":"はい、親子ディレクトリでない限り実行できます。たとえば、prod-logsと/usage/prod-logsは機能しません。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderについて、バケットまたはコンテナーに既存のファイル通知がある場合、この機能を使用できますか?\n","response":"はい、入力ディレクトリが既存の通知プレフィックスと競合しない限りは実行できます。"}
{"category":"closed_qa","context":"","instruction":"Auto Loader はどのようにスキーマを推測しますか?","response":"DataFrame が最初に定義されると、Auto Loader はソース ディレクトリを一覧表示し、最新の (ファイル変更時間による) 50 GB のデータまたは 1000 ファイルを選択し、それらを使用してデータ スキーマを推測します。\n\nAuto Loader は、ソース ディレクトリ構造を調べてパーティション列を推測し、その/key=value/構造を含むファイル パスを探します。ソース ディレクトリの構造に一貫性がない場合、Auto Loader は、パーティション列が空であると推測します。cloudFiles.partitionColumnsディレクトリ構造から列を明示的に解析するために使用します。"}
{"category":"closed_qa","context":"","instruction":"ソース フォルダが空の場合、Auto Loader はどのように動作しますか?","response":"ソース ディレクトリが空の場合、推論を実行するためのデータがないため、Auto Loader にスキーマを提供する必要があります。"}
{"category":"closed_qa","context":"","instruction":"Autoloader がスキーマを推論するのはいつですか? マイクロバッチごとに自動的に進化しますか?","response":"スキーマは、DataFrame がコードで最初に定義されたときに推測されます。各マイクロバッチ中に、スキーマの変更がオンザフライで評価されます。したがって、パフォーマンス ヒットについて心配する必要はありません。ストリームが再開すると、スキーマの場所から展開されたスキーマが取得され、推論によるオーバーヘッドなしで実行が開始されます。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderのスキーマ推論を使用すると、データの取り込みのパフォーマンスにどのような影響がありますか?","response":"非常に大きなソース ディレクトリの場合、最初のスキーマ推定時にスキーマ推定に数分かかることが予想されます。それ以外の場合は、ストリームの実行中に重大なパフォーマンス ヒットが発生することはありません。Databricks ノートブックでコードを実行すると、Auto Loader がデータ スキーマのサンプリングと推測のためにディレクトリを一覧表示するタイミングを指定するステータスの更新を確認できます。"}
{"category":"closed_qa","context":"","instruction":"ファイルとリポジトリのサイズ制限を教えてください。","response":"Databricks は、リポジトリのサイズに制限を適用しません。しかし：\n\n作業ブランチは 200 MB に制限されています。\n\n個々のファイルは 200 MB に制限されています。\n\n10 MB を超えるファイルは、Databricks UI で表示できません。\n\nDatabricks は、レポで次のことをお勧めします。\n\nすべてのファイルの合計数が 10,000 を超えないこと。\n\nノートブックの総数は 5,000 を超えません。\n\nリポジトリがこれらの制限を超えると、エラー メッセージが表示される場合があります。リポジトリのクローンを作成するときにタイムアウト エラーが発生することもありますが、操作はバックグラウンドで完了する可能性があります。"}
{"category":"closed_qa","context":"","instruction":"Databricks リポジトリのコンテンツはどこに保存されますか?","response":"リポジトリのコンテンツは、コントロール プレーンのディスクに一時的に複製されます。Databricks ノートブック ファイルは、メイン ワークスペースのノートブックと同様に、コントロール プレーン データベースに格納されます。ノートブック以外のファイルは、最大 30 日間ディスクに保存できます。"}
{"category":"closed_qa","context":"","instruction":"Repos は、オンプレミスまたはセルフホストの Git サーバーをサポートしていますか?","response":"サーバーがインターネットにアクセスできる場合、Databricks Repos は Bitbucket Server との統合をサポートします。\n\nBitbucket Server、GitHub Enterprise Server、またはインターネットにアクセスできない GitLab セルフマネージド サブスクリプション インスタンスと統合するには、Databricks の担当者にお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"Repos は.gitignoreファイルをサポートしていますか?","response":"はい。リポジトリにファイルを追加し、それを Git で追跡したくない場合は、.gitignoreファイルを作成するか、リモート リポジトリから複製したファイルを使用して、拡張子を含むファイル名を追加します。\n\n.gitignoreはGit によってまだ追跡されていないファイルに対してのみ機能します。Git によって既に追跡されているファイルを.gitignoreファイルに追加しても、そのファイルは引き続き Git によって追跡されます。"}
{"category":"closed_qa","context":"","instruction":"ユーザー フォルダではないトップレベルフォルダを作成できますか?","response":"はい、管理者であればトップレベルフォルダを単一の深さまで作成できます。Repos は、追加のフォルダー レベルをサポートしていません。"}
{"category":"closed_qa","context":"","instruction":"Repos は Git サブモジュールをサポートしていますか?","response":"いいえ。Git サブモジュールを含むリポジトリを複製できますが、サブモジュールは複製されません。"}
{"category":"closed_qa","context":"","instruction":"ワークスペースで Repos を無効にするにはどうすればよいですか?","response":"ワークスペースで Repos for Git を無効にするには、次の手順に従います。\n\n管理設定ページに移動します。\n\n[ワークスペース設定]タブをクリックします。\n\n[詳細]セクションで、[リポジトリ]トグルをクリックします。\n\n[確認]をクリックします。\n\nブラウザを更新します。"}
{"category":"closed_qa","context":"","instruction":"別のブランチをプルまたはチェックアウトすると、ノートブック ダッシュボードが消えるのはなぜですか?","response":"Databricks ノートブック ソース ファイルにはノートブック ダッシュボード情報が格納されないため、これは現在の制限です。"}
{"category":"closed_qa","context":"","instruction":"IPYNB ノートブック ファイルを取り込むことはできますか?","response":"はい。Jupyter ノートブック (.ipynb ファイル) のサポートは Repos で利用できます。.ipynb ノートブックを使用してリポジトリを複製し、Databricks UI で作業してから、.ipynb ノートブックとしてコミットおよびプッシュできます。ノートブック ダッシュボードなどのメタデータは保持されます。管理者は、出力をコミットできるかどうかを制御できます。以下もできます。\n新しい .ipynb ノートブックを作成します。\n\nノートブックを .ipynb ファイル形式に変換します。\n\nコード差分(セル内のコードの変更) または生の差分(JSON 内のコード変更、メタデータとしてのノートブック出力を含む)として差分を表示します。"}
{"category":"closed_qa","context":"","instruction":"Repos はブランチのマージをサポートしていますか?","response":"いいえ。Databricks では、プル リクエストを作成し、Git プロバイダーを介してマージすることをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Databricks リポジトリからブランチを削除できますか?","response":"いいえ。ブランチを削除するには、Git プロバイダーで作業する必要があります。"}
{"category":"closed_qa","context":"","instruction":"ライブラリがクラスターにインストールされ、同じ名前のライブラリがリポジトリ内のフォルダーに含まれている場合、どのライブラリがインポートされますか?","response":"リポジトリ内のライブラリがインポートされます。"}
{"category":"closed_qa","context":"","instruction":"外部のオーケストレーション ツールに依存せずに、ジョブを実行する前に Git からリポジトリの最新バージョンを取得できますか?","response":"いいえ。通常、これを Git サーバーの事前コミットとして統合して、ブランチ (main/prod) へのすべてのプッシュが Production リポジトリを更新するようにすることができます。"}
{"category":"closed_qa","context":"","instruction":"レポをエクスポートできますか?","response":"はい、ノートブック、フォルダー、またはリポジトリ全体をエクスポートできます。ノートブック以外のファイルをエクスポートすることはできません。リポジトリ全体をエクスポートすると、ノートブック以外のファイルは含まれません。エクスポートするには、Workspace CLIまたはWorkspace API 2.0を使用します。"}
{"category":"closed_qa","context":"","instruction":"Databricks リポジトリのコンテンツは暗号化されていますか?","response":"Databricks リポジトリのコンテンツは、プラットフォームで管理されたキーを使用して Databricks によって暗号化されます。カスタマー マネージド キーを使用した暗号化はサポートされていません。"}
{"category":"closed_qa","context":"","instruction":"GitHub トークンは Databricks にどのように、どこに保存されますか? 誰が Databricks からアクセスできますか?","response":"認証トークンは Databricks コントロール プレーンに格納され、Databricks の従業員は、監査された一時的な資格情報を介してのみアクセスできます。\n\nDatabricks は、これらのトークンの作成と削除をログに記録しますが、使用状況はログに記録しません。Databricks には、Databricks アプリケーションによるトークンの使用状況を監査するために使用できる Git 操作を追跡するログがあります。\n\nGitHub エンタープライズは、トークンの使用状況を監査します。他の Git サービスにも Git サーバー監査がある場合があります。"}
{"category":"closed_qa","context":"","instruction":"Repos はコミットの GPG 署名をサポートしていますか?","response":"いいえ。"}
{"category":"closed_qa","context":"","instruction":"Repos は SSH をサポートしていますか?","response":"いいえ、HTTPS のみです。"}
{"category":"closed_qa","context":"","instruction":"Gitから入ってくる変更は、ノートブックの状態をクリアしますか？","response":"はい、ノートブックのソース コードを変更する Git 操作により、セル出力、コメント、リビジョン履歴、ウィジェットなどのノートブックの状態が失われます。たとえば、Git プルはノートブックのソース コードを変更できます。この場合、Databricks Repos は既存のノートブックを上書きして変更をインポートする必要があります。Git のコミットとプッシュ、または新しいブランチの作成は、ノートブックのソース コードには影響しないため、これらの操作ではノートブックの状態が保持されます。"}
{"category":"closed_qa","context":"","instruction":"リポジトリで MLflow 実験を作成できますか?","response":"No. ワークスペースでのみ MLflow 実験を作成できます。3.72 プラットフォーム リリースより前にリポジトリで作成された実験は サポートされなくなりましたが、保証なしで引き続き動作する可能性があります。Databricks では、 MLflow エクスポート ツールを使用して、リポジトリ内の既存の実験をワークスペースの実験にエクスポートすることをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Git 操作の進行中にノートブックでジョブの実行を開始するとどうなりますか?","response":"Git 操作の進行中の任意の時点で、リポジトリ内の一部のノートブックが更新され、他のノートブックは更新されていない可能性があります。これにより、予期しない動作が発生する可能性があります。\n\nたとえば、ノートブック A が%runコマンドを使用してノートブック Z を呼び出すとします。Git 操作中に実行されているジョブがノートブック A の最新バージョンを開始するが、ノートブック Z がまだ更新されていない場合、ノートブック A のコマンドはノートブック%runZ の古いバージョンを開始する可能性があります。予測可能であり、ジョブが失敗するか、異なるコミットからノートブック A とノートブック Z を実行する可能性があります。"}
{"category":"closed_qa","context":"","instruction":"リポジトリで Databricks ノートブック以外の ファイルを実行するにはどうすればよいですか? 例えば.pyファイル？","response":"次のいずれかを使用できます。\n\nクラスタにライブラリとしてバンドルしてデプロイします。\n\nGit リポジトリを直接 pip インストールします。これには、 secrets managerの資格情報が必要です。\n\n%runノートブックのインライン コードで使用します。\n\nカスタム コンテナー イメージを使用します。Databricks Container Services を使用してコンテナーをカスタマイズする を参照してください。"}
{"category":"closed_qa","context":"","instruction":"パーティションのcoalescingが既に有効になっているにもかかわらず、AQE がシャッフル パーティション数を変更しなかったのはなぜですか?","response":"AQE は初期パーティション数を変更しません。シャッフル パーティション数に適度に高い値を設定し、クエリの各段階での出力データ サイズに基づいて、AQE が小さなパーティションを結合できるようにすることをお勧めします。\n\nジョブにスピルが見られる場合は、次のことを試すことができます。\n\nシャッフル パーティション数を増やす:spark.sql.shuffle.partitions\n\nspark.databricks.adaptive.autoOptimizeShuffle.enabledをtrueに設定して自動最適化シャッフルを有効にする。"}
{"category":"closed_qa","context":"","instruction":"AQE が小さな結合テーブルをブロードキャストしなかったのはなぜですか?","response":"ブロードキャストされると予想されるリレーションのサイズがこのしきい値を下回っているにもかかわらず、ブロードキャストされない場合:\n\n結合タイプを確認してください。ブロードキャストは、特定の結合タイプではサポートされていません。たとえば、 LEFT OUTER JOINはブロードキャストできません。\n\nリレーションに多くの空のパーティションが含まれている可能性もあります。この場合、タスクの大部分はsort merge joinで迅速に終了するか、skew join handlingで最適化できる可能性があります。AQE は、空でないパーティションのパーセンテージがspark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin よりも低い場合、そのようなsort merge joinsをbroadcast hash joinsに変更することを回避します。"}
{"category":"closed_qa","context":"","instruction":"AQE が有効になっている場合はbroadcast join戦略のヒントを引き続き使用する必要がありますか?","response":"はい。通常、静的に計画されたブロードキャスト結合(broadcast join)は、AQE によって動的に計画されたブロードキャスト結合よりもパフォーマンスが高くなります。AQE は、結合の両側でシャッフルを実行するまでブロードキャスト結合に切り替わらない可能性があるためです (それまでに、実際のリレーション サイズが取得されます)。そのため、クエリをよく知っている場合は、ブロードキャスト ヒントを使用することをお勧めします。AQE は、静的最適化と同じ方法でクエリ ヒントを尊重しますが、ヒントの影響を受けない動的最適化を適用することもできます。"}
{"category":"closed_qa","context":"","instruction":"スキュー結合ヒントと AQE スキュー結合最適化の違いは何ですか? どちらを使用すべきでしょうか?","response":"AQE スキュー結合は完全に自動化されており、通常は対応するヒントよりもパフォーマンスが優れているため、スキュー結合ヒントを使用するのではなく、AQE スキュー結合処理に依存することをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"AQE が結合順序を自動的に調整しないのはなぜですか?","response":"動的結合の並べ替えは、Databricks Runtime 7.3 LTS の時点で AQE の一部ではありません。"}
{"category":"closed_qa","context":"","instruction":"AQE がデータの偏りを検出しなかったのはなぜですか?","response":"AQE がパーティションをスキュー パーティションとして検出するには、次の 2 つのサイズ条件を満たさなければなりません。\n\nパーティション サイズがspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes (デフォルトの 256MB)より大きい\n\nパーティション サイズが、すべてのパーティションの中央サイズにspark.sql.adaptive.skewJoin.skewedPartitionFactor(既定値は 5)を掛けた値よりも大きい\n\nさらに、スキュー処理のサポートは、特定の結合タイプに対して制限されています。たとえば、 LEFT OUTER JOINでは、左側のスキューのみを最適化できます。"}
{"category":"closed_qa","context":"","instruction":"Azure Synapse コネクタの使用中にエラーが発生しました。このエラーが Azure Synapse によるものか Databricks によるものかはどうすればわかりますか?","response":"エラーのデバッグに役立つように、Azure Synapse コネクタに固有のコードによってスローされた例外は、SqlDWException特性を拡張する例外にラップされます。例外は、次の区別もあります。\nSqlDWConnectorExceptionはAzure Synapse コネクタによってスローされたエラーです。\nSqlDWSideExceptionは接続された Azure Synapse インスタンスによってスローされたエラーです。"}
{"category":"closed_qa","context":"","instruction":"Shared Access Signature (SAS) を使用して、tempDirで指定された BLOB ストレージ コンテナーにアクセスできますか?","response":"Azure Synapse では、SAS を使用した Blob Storage へのアクセスはサポートされていません。したがって、Azure Synapse コネクタは、tempDirによって指定された BLOB ストレージ コンテナーにアクセスするためのSASをサポートしていません。"}
{"category":"closed_qa","context":"","instruction":"dbTableオプションを指定して Azure Synapse コネクタを使用して Spark テーブルを作成し、この Spark テーブルにデータを書き込んでから、この Spark テーブルを削除しました。Azure Synapse 側で作成したテーブルは削除されますか?","response":"いいえ。Azure Synapse は外部データ ソースと見なされます。dbTableによって名前が設定された Azure Synapse テーブルは、Spark テーブルが削除されても削除されません。"}
{"category":"closed_qa","context":"","instruction":"DataFrame を Azure Synapse に書き込むときに、単に.saveAsTable(tableName) の代わりに.option(\"dbTable\", tableName).save()を書く必要があるのはなぜですか?","response":"これは、「.option(\"dbTable\", tableName) 」がデータベース(つまり、Azure Synapse) テーブルを指し、「.saveAsTable(tableName)」がSpark テーブルを指すという区別を明確にしたいためです。実際には、この 2 つを組み合わせることもできます「df.write. ... .option(\"dbTable\", tableNameDW).saveAsTable(tableNameSpark)」。これにより、呼び出された Azure Synapse 内に「tableNameDW」テーブルが作成され、 Azure Synapse テーブルによってサポートされる呼び出された Spark 内に「tableNameSpark」外部テーブルが作成されます。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQL ワークロードはどのように課金されますか?","response":"Databricks SQL ワークロードは、SQL コンピューティング SKUに従って課金されます。"}
{"category":"closed_qa","context":"","instruction":"SQL エンドポイントはどこで実行されますか?","response":"Databricks クラスターと同様に、クラシック SQL エンドポイントはAWS アカウントで作成および管理されます。クラシック SQL エンドポイントは、アカウント内の SQL 最適化クラスターを自動的に管理し、エンドユーザーの需要に合わせてスケーリングします。\n\n一方、サーバーレス SQL エンドポイント(パブリック プレビュー) は、Databricks クラウド アカウントのコンピューティング リソースを使用します。サーバーレス SQL ウェアハウスは、SQL エンドポイントの構成と使用を簡素化し、起動時間を短縮します。サーバーレス オプションは、ワークスペースで有効になっている場合にのみ使用できます。詳細については、「サーバーレス コンピューティング」を参照してください。"}
{"category":"closed_qa","context":"","instruction":"Data Science & Engineering ワークスペースの SQL ノートブックから SQL エンドポイントを使用できますか?","response":"いいえ。Databricks SQLクエリ、BI ツール、その他のJDBC および ODBC クライアントから SQL エンドポイントを使用できます。"}
{"category":"closed_qa","context":"","instruction":"クラウド プロバイダーの資格情報を使用してデータへのアクセスが許可されています。Databricks SQL でこのデータにアクセスできないのはなぜですか?","response":"Databricks SQL では、データへのすべてのアクセスはデータ アクセス制御の対象となり、管理者またはデータ所有者は最初に適切な権限を付与する必要があります。"}
{"category":"closed_qa","context":"","instruction":"Databricks Terraform プロバイダーは、Visual Studio Code の Databricks 拡張機能とどのように関連していますか?","response":"Databricks は、予測可能な方法で CI/CD パイプラインを管理するために、Databricks Terraform プロバイダーを引き続き推奨しています。今後、IDE を使用して展開を管理する方法について、Databricks の担当者にお知らせください。Databricks は、お客様の意見を将来の計画に組み込みます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Labs による dbx は、Visual Studio Code の Databricks 拡張機能とどのように関連していますか?","response":"Databricks Labs による dbxの主な機能は次のとおりです。\nプロジェクトのscaffolding。\ndbx executeコマンドによる限定的なローカル開発。\nDatabricks ジョブの CI/CD。\n\nVisual Studio Code の Databricks 拡張機能を使用すると、Databricks クラスターでローカル開発とリモートで Python コード ファイルを実行し、Databricks ジョブで Python コード ファイルとノートブックをリモートで実行できます。dbxプロジェクトのscaffoldingと Databricks ジョブの CI/CD に引き続き使用できます。"}
{"category":"closed_qa","context":"","instruction":"Databricks CLI で作成した既存の Databricks 構成プロファイルが既にある場合はどうなりますか?","response":"Visual Studio Code の Databricks 拡張機能を構成するときに、既存の構成プロファイルを選択できます。拡張機能とコード プロジェクトを開いた状態で、次の操作を行います。\n\n[構成]ペインで、歯車 ( [ワークスペースの構成] ) アイコンをクリックします。\nたとえば、ワークスペース インスタンスの URLを入力しますhttps://dbc-a1b2345c-d6e7.cloud.databricks.com。\n\nコマンド パレットで、既存の構成プロファイルを選択します。"}
{"category":"closed_qa","context":"","instruction":"Databricks ワークスペースで Visual Studio Code 用の Databricks 拡張機能を使用するには、どのアクセス許可が必要ですか?","response":"コードを実行するための Databricks クラスターの実行アクセス許可と、Databricks Repos にリポジトリを作成するためのアクセス許可が必要です。"}
{"category":"closed_qa","context":"","instruction":"Databricks ワークスペースで Visual Studio Code 用の Databricks 拡張機能を使用するには、どの設定を有効にする必要がありますか?","response":"ワークスペースでは、Files in Repos設定がオンになっている必要があります。手順については、Repos 内のファイルのサポートを構成する を参照してください。この設定を自分で有効にできない場合は、Databricks ワークスペース管理者に問い合わせてください。"}
{"category":"closed_qa","context":"","instruction":"プロキシで Visual Studio Code の Databricks 拡張機能を使用できますか?","response":"はい。プロキシ経由で同期するときのエラーで推奨される解決策を参照してください。https://docs.databricks.com/dev-tools/vscode-ext.html#troubleshooting-proxy"}
{"category":"closed_qa","context":"","instruction":"リモート Git プロバイダーに格納されている既存のリポジトリで、Visual Studio Code の Databricks 拡張機能を使用できますか?","response":"いいえ。Visual Studio Code の Databricks 拡張機能は、作成したリポジトリでのみ機能します。"}
{"category":"closed_qa","context":"","instruction":"1 つのクエリで同じパラメータを複数回再利用できますか?","response":"はい。{{}}で同じ識別子を使用します。"}
{"category":"closed_qa","context":"","instruction":"1 つのクエリで複数のパラメーターを使用できますか?","response":"はい。各パラメータに一意の名前を使用してください。"}
{"category":"closed_qa","context":"","instruction":"Spark DataFrame 列が Snowflake で同じ順序で表示されないのはなぜですか?","response":"Spark 用の Snowflake コネクタは、書き込まれるテーブルの列の順序を尊重しません。DataFrame 列と Snowflake 列の間のマッピングを明示的に指定する必要があります。このマッピングを指定するには、columnmap パラメータを使用します。"}
{"category":"closed_qa","context":"","instruction":"Snowflake にINTEGERとして書き込まれたデータがDECIMAL として読み戻されるのはなぜですか?","response":"Snowflake はすべてのINTEGER型はNUMBER として表します。これにより、Snowflake にデータを書き込んだり、Snowflake からデータを読み取ったりするときに、データ型が変更される可能性があります。たとえば、 Snowflake ではDECIMALとINTEGER が意味的に同等であるため、Snowflake への書き込み時にINTEGERデータをDECIMAL に変換する場合があります( Snowflake Numeric Data Typesを参照)。"}
{"category":"closed_qa","context":"","instruction":"Snowflake テーブル スキーマのフィールドが常に大文字なのはなぜですか?","response":"Snowflake はデフォルトで大文字のフィールドを使用します。つまり、テーブル スキーマは大文字に変換されます。"}
{"category":"closed_qa","context":"","instruction":"ソリューションアクセラレータを使用するには、Databricks ユーザーである必要がありますか？","response":"ソリューションアクセラレータの使用には、Databricks 無料トライアルへのご登録、または既存の Databricks アカウントが必要です。"}
{"category":"closed_qa","context":"","instruction":"ソリューションアクセラレータのメリットは？","response":"ソリューションアクセラレータを使用することで、発見、設計、開発、テストにかかる時間を短縮できます。Notebook や実証済みのパターン、ベストプラクティスといったリソースを活用し、データと AI のユースケースをすぐに展開できるように設計されています。発案から PoC までを最短 2 週間で完了できます。"}
{"category":"closed_qa","context":"","instruction":"Databricks ソリューションアクセラレータは何？","response":"Databricks ソリューションアクセラレータは、さまざまな業界・業種に共通の主要なユースケースに対応する、フル機能の Notebook やベストプラクティスを含む目的に特化したガイドです。ソリューションアクセラレータを使用することで、発見、設計、開発、テストにかかる時間を短縮できます。発案から PoC までを 2 週間以内に完了できるよう設計されています。\n \n\nソリューションアクセラレータは、Databricks の無料トライアルへのご登録、または既存のアカウントで使用できます。"}
{"category":"closed_qa","context":"","instruction":"無料トライアルには何が含まれていますか？","response":"14日間無料トライアル に含まれるもの：\n\nソリューションの共同構築を可能にするプラットフォーム\nApache SparkTM、SQL、Python、Scala、Delta Lake、MLflow、TensorFlow、Keras、Scikit-learn など、多言語のサポートを備えた対話型 Notebook\n※ 無料トライアル期間中にアカウント内で使用されたリソース （コンピュートインスタンスなど） については、クラウドプロバイダの料金が適用されます。"}
{"category":"closed_qa","context":"","instruction":"無料トライアル期間の終了後はどうなりますか？","response":"無料お試し期間が終了すると、自動的に同プランの有料サブスクリプションに移行します。ただし、サブスクリプションはいつでも解約できます。"}
{"category":"closed_qa","context":"","instruction":"料金は使用量・ストレージ容量のどちらで計算されますか？","response":"Databricks の料金は、コンピューティング使用量に基づいています。ストレージ、ネットワークなどの関連コストは、お使いになるサービスやクラウドサービスプロバイダによって異なります。"}
{"category":"closed_qa","context":"","instruction":"DBU とは何を意味しますか？","response":"Databricks ユニット（DBU）とは、Databricks レイクハウスプラットフォームの処理能力を規格化した単位であり、測定と課金の目的で使用されます。ワークロードが消費する DBU の数は、使用するコンピューティングリソースや処理するデータの量などの処理指標によって決まります。"}
{"category":"closed_qa","context":"","instruction":"地域によって料金が異なりますか？","response":"Databricks の料金やスポット割引は、お使いの地域やクラウドサービスプロバイダによって異なる場合があります。詳しくは、各製品の Databricks の料金設定をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricks は、他のクラウドサービスやオープンソースのソリューションと比較して、高価格ですか？","response":"Databricks の製品は、お客様のワークロードの TCO（Total Cost of Ownership）を考慮した価格設定になっています。Databricks によるコスト削減を見積もる際には、 ジョブの完了率、期間、ジョブをサポートするために必要な手作業やリソースなど、代替ソリューションの主要な側面を考慮することが重要です。正確な節約額を見積もるには、PoC の展開の一環として、結果を並べて比較することをお勧めします。 Databricks の導入による高速化の達成事例と お客様によるベンチマーク結果もご参照ください。詳しくはお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"テクニカルサポートは提供されますか？","response":"テクニカルサポートを提供しています。また、技術ドキュメントの参照をお薦めしています。 詳しくはお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"Photon とは何ですか？","response":"Photon は、Databricks レイクハウスプラットフォームの次世代エンジンであり、非常に高速なクエリ性能を、より低いコストで提供します。"}
{"category":"closed_qa","context":"","instruction":"Jobs クラスタと Jobs Light クラスタのオプションがありますが、どちらを選択すべきでしょうか？","response":"Jobs Light クラスタは、Databricks のオープンソース Apache Spark に相当します。Jobs Light クラスタは、Databricks のテクノロジーによる高性能、高信頼性、自動スケーリングを必要としないケース、すなわち、シンプルで重要性の低いワークロードを対象としています。一方、Jobs クラスタでは、Databricks のメリットが全て提供され、生産性向上、総所有コストの削減を実現します。"}
{"category":"closed_qa","context":"","instruction":"Jobs Compute と All-Purpose Compute のワークロードの違いは何ですか？","response":"Jobs Compute のワークロードは、Jobs クラスタで実行するワークロードです。Jobs クラスタは、同一のジョブによって開始・終了します。分離を目的として、1 つの Jobs クラスタでは 1 つのジョブのみを実行します。All-Purpose Compute のワークロードは、All-Purpose クラスタで実行されます。All-Purpose クラスタは、Jobs クラスタに分類されないクラスタです。Databricks Notebook 内でのコマンドの実行、BI ワークロードのための JDBC/ODBC を介した接続、Databricks での MLflow の実験など、さまざまな用途に使用できます。複数のユーザーで All-Purpose クラスタを共有し、インタラクティブな分析を行うことができます。"}
{"category":"closed_qa","context":"","instruction":"ワークスペース内の各パイプラインに異なる DLT Product Edition を選択することはできますか？","response":"はい。パイプラインの設定で、DLT パイプラインごとに異なるエディションを指定することができます。"}
{"category":"closed_qa","context":"","instruction":"パイプライン用の DLT Product Edition はどのように選択すればよいですか？","response":"パイプラインの作成時に、DLT Product Edition を選択するオプションがあります。Delta Live Tables のページから「Create Cluster」をクリックすると、その後のウィンドウに DLT Product Edition を選択するオプションが表示されます。"}
{"category":"closed_qa","context":"","instruction":"DLT の品質に関する期待値を設定しようとしたとき、パイプラインが Advanced Edition を使用していない場合はどうなりますか？","response":"パイプラインに選択された DLT Product Edition に含まれていない機能を含むパイプラインを実行しようとすると、エラーの理由を含むエラーメッセージが表示されます。その後、パイプラインの Product Edition を変更して、必要な機能を含むエディションを選択し、パイプラインの再実行を試みることができます。"}
{"category":"closed_qa","context":"","instruction":"DLT に関連するその他の料金はありますか？","response":"はい。DLT クラスタは、ジョブ計算速度で DBU を消費する Delta Live Table を維持するために、システムが生成するジョブを定期的に自動的に実行します。"}
{"category":"closed_qa","context":"","instruction":"SQL ServerlessとSQL Classic / Proの違いは何ですか？","response":"SQL Classic および Pro の場合、Databricks はお客様のクラウドプロバイダ環境にクラスタリソースをデプロイし、お客様は対応するコンピューティングインフラ料金を支払う責任を負います。サーバーレスコンピュートの場合、Databricks は Databricks のクラウドプロバイダのアカウントにクラスタリソースをデプロイするためお客様はコンピュートインフラ料金を別途お支払いいただく必要はございません。詳しくはこちらをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"サーバーレスで発生する可能性のあるネットワークエグレスチャージは何ですか？","response":"ソースデータが Databricks サーバーレス環境と異なるクラウド領域にある場合、クラウド事業者からネットワークエグレスチャージを請求される場合があります。Databricks は現在、サーバーレス環境からお客様の目的地までの移動にかかる費用を無料としていますが、将来的には市場競争力のある料金でその移動にかかる費用を請求する可能性があります。"}
{"category":"closed_qa","context":"","instruction":"SQL Pro とサーバーレス SQLのプロモーション割引は、交渉による割引に上乗せされますか？","response":"はい、プロモーション割引は、お客様が交渉された契約割引に上乗せされます。"}
{"category":"closed_qa","context":"","instruction":"SQL Pro とサーバーレス SQL のキャンペーン割引はどのように実施されているのですか？","response":"AWS では、キャンペーン期間中に SQL Pro と Serverless SQL のウェアハウスで排出される DBU レートを引き下げることで、キャンペーン割引を実施します。"}
{"category":"closed_qa","context":"","instruction":"同時リクエストごとに提供されるメモリはどのくらいですか？","response":"1同時リクエストあたり 4 GB"}
{"category":"closed_qa","context":"","instruction":"なぜリージョン別価格があるのですか？","response":"リージョン別価格は、Databricks のサーバーレス製品を支えるインフラのリージョン別コストに基づくものです。"}
{"category":"closed_qa","context":"","instruction":"DBU と同時実行性は、モデルサービングにどのように作用するのですか？","response":"クエリの総数／総時間から、1 時間あたりに必要な同時リクエスト数を算出します。\n1 DBU／時間で 1 時間あたり 1 同時リクエスト（最低 4 DBU ／時間）"}
{"category":"closed_qa","context":"","instruction":"モデル実行時間とは？","response":"モデル実行時間とは、モデルサービングで機械学習モデルを実行するのに必要な時間のことです。実行時間を見積もるには、エンドポイントページのレイテンシチャートを確認するか、負荷テストを実施できます。負荷テストに関するガイダンスについては、Databricks チームまでお気軽にお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"「scale to zero」が選択されておらず、トラフィックがない場合の最低料金はいくらですか？","response":"「scale to zero」が選択されていない場合、最低料金は、選択された同時接続数範囲によって指定されるプロビジョニングされた同時接続数の最小値によって異なります。"}
{"category":"closed_qa","context":"","instruction":"アドオン製品料金の基準となる製品支出はどのように計算されますか？","response":"プロダクトスペンドは、割引、使用クレジット、アドオンアップリフト、サポート料適用前のリストでの AWS プロダクトスペンドを基に算出されます。"}
{"category":"closed_qa","context":"","instruction":"Sparkクラスタの汎用クラスタで最大30日間保持できる最大クラスタ数はいくつ？？","response":"200です。"}
{"category":"closed_qa","context":"","instruction":"%runコマンドで他のnotebookの特定のセル、いくつかのセルなど指定することはできますか？","response":"いいえ、特定のセルの指定はできません。"}
{"category":"closed_qa","context":"","instruction":"%runコマンドで指定できるノートブックは異なるワークスペースのものでも可能ですか？","response":"いいえ、ノートブックはworkspace内でのみ共有可能です。"}
{"category":"closed_qa","context":"","instruction":" Azure DatabricksやAWS Databricks等でも操作方法、UI等は同じでしょうか。","response":"はい、AWSとAzureのdatabricksはUIに差異はありません。"}
{"category":"closed_qa","context":"","instruction":"デタッチしてから再アタッチする理由はなんですか？","response":"セッションの貼り直しと同様になりますので、変数などセッション固有の情報がリセットされます。"}
{"category":"closed_qa","context":"","instruction":"デルタレイクで扱えるストレージシステムはなにが含まれますか？","response":"Delta LakeではAzure Datalake Storage gen2, Blog, AWS S3, GCP GCSをストレージとして使用します。"}
{"category":"closed_qa","context":"","instruction":"Delta形式は、rawデータをためておく領域というよりは、データエンジニアが取り出して処理を行うworking領域で効果が価値が発揮されるものと感じますが、正しい理解でしょうか？","response":"はい、そのような認識も可能です。rawデータだと、クエリのパフォーマンスが出ない、ACIDトランザクションがない、テーブルレベルのアクセス権限などがない、などがあり、この部分をDelta Lake形式化することで解消されます。"}
{"category":"closed_qa","context":"","instruction":"デルタ形式でないテーブルも作れるということですか？","response":"はい、作れます。例えば,CSVファイルやJSONファイルをテーブルとして参照することが一般的な「データレイク」の使用ケースとして多いです。"}
{"category":"closed_qa","context":"","instruction":"Snowflakeもタイムトラベル機能があったと記憶していますが、デルタレイクは使っていませんでしたよね？デルタレイクを使用しているDatabricksの方が良い点などありますか？","response":"色々比較は可能なのですが、一つは、Delta Lakeはオープンな規格、SnowFlakeはプロプライエタリな規格という違いが大きいかと思います。"}
{"category":"closed_qa","context":"","instruction":"テーブルはどこに保存されていますか？","response":"Azure上であれば、ADLS, AWS上であればS3上に保存さます。"}
{"category":"closed_qa","context":"","instruction":"delta tableはRDSなのでしょうか？（PK制約をつけられるかどうか）","response":"デルタテーブルはレイクハウス(データレイクとデータウェアハウスを混成した機能)であり、RDSのように使用していただくことも可能です。(PK指定の機能も年内に対応する予定です。）"}
{"category":"closed_qa","context":"","instruction":"使用するストレージの設定はどうやって行うのですか？","response":"Databricksの環境(ワークスペース)を作成する際に、一つのオブジェクトストレージが作成されます。また、ユーザーが所有するオブジェクトストレージを使用することも可能です。設定方法はクラウド環境によって異なります。"}
{"category":"closed_qa","context":"","instruction":"Databricks上でCREATE TABLEしたテーブルは全てDelta Lakeファイルとして作成されるのでしょうか？","response":"デフォルトはDelta形式ですが、CSV/JSONなど任意のフォーマットを指定できます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのとSnowflakeもタイムトラベル機能を持っていますが、両者のその機能のどちらが良いなどがあるかを考えるとき、どのような比較項目がありますでしょうか？","response":"タイムトラベル機能のみに絞れば、両者でできることはほぼ同一なので、大差はないかと思います。それ以外の機能比較でよく見られるのは、クエリの実行速度、実行コストなどがあるかと思います。"}
{"category":"closed_qa","context":"","instruction":"Databricks上ではDelta Lake形式以外のテーブルの保存形式は存在しますか？","response":"任意のフォーマットのファイルをDatabricksのストレージ上に保存可能ですが、Deltaの最適化機能はDeltaLake形式で保存したデータが対象になります。"}
{"category":"closed_qa","context":"","instruction":"「Databricksの環境(ワークスペース)を作成する際に、一つのオブジェクトストレージが作成されます。」このストレージはDatabricks 社のデータセンタにデプロイされるという理解で正しいですか？","response":"ワークスペースを作成する際に最初にするオブジェクトストレージもお客様環境のアカウント内のストレージを使用します。"}
{"category":"closed_qa","context":"","instruction":"parquetファイルとはどんなものなのでしょうか？","response":"テーブルデータをカラムナー形式で効率的に格納するファイル形式になります。(オープンな規格です。)"}
{"category":"closed_qa","context":"","instruction":"タイムトラベルを実現するために目安としてデータの何倍と考えたほうが良いのでしょうか。","response":"更新の頻度により目安は変わりますが、デフォルトで30日分の変更を保持します。テーブル属性で変更は可能です。"}
{"category":"closed_qa","context":"","instruction":"Compacting Small Files and Indexingはどんな時に使うのか？","response":"I/O効率を向上させ、パフォーマンスを改善させる必要がある場合です。"}
{"category":"closed_qa","context":"","instruction":"AWS/GCP/Azureの各プロバイダーは、低料金のスポットインスタンスを提供しています。databricksから、それらのスポットインスタンスを利用可能でしょうか？","response":"はい、クラスタ構成のオプションで指定可能です。"}
{"category":"closed_qa","context":"","instruction":"現在は小さいテーブルなのでよいと思いますが、100Gとかテラ単位のデータを扱うときもsnappyがいくつも出来上がるのでしょか。","response":"その場合はOptimizeコマンドで物理ファイルを再構成できます。"}
{"category":"closed_qa","context":"","instruction":"optimizeが必要かどうか判断するための断片化の指標などはあるのでしょうか。","response":"特にないのですが、日時バッチで回す場合は、最後のバッチのあとにOPTIMIZEを実施するのたプラクティスになっています。また、最近では、Deltaに書き出す(更新も含む)際に、自動的にCompactionを実施する機能も実装されております(Auto Compaction)。"}
{"category":"closed_qa","context":"","instruction":"optimize する頻度はどれぐらいが適切ですか？","response":"テーブルの10%-20%程度の更新があった場合が適切かと思いますが、Optimize済みの領域はSkipすること可能ですので、更新後に実行いただくという運用も可能です。"}
{"category":"closed_qa","context":"","instruction":"タイムトラベル機能は、別のサイトではスナップショットを使うような記載があったにですが、不要ということであっておりますでしょうか？","response":"Databricksではタイムトラベルとスナップショットは同義です。別々の機能ではございません。"}
{"category":"closed_qa","context":"","instruction":"optimize を自動で行うように設定することは可能ですか？","response":"はい、可能です。「Auto-Optimize」コマンドを使用ください。 https://learn.microsoft.com/ja-jp/azure/databricks/optimizations/auto-optimize"}
{"category":"closed_qa","context":"","instruction":"タイムトラベル機能でバージョン指定の方法はわかったのですが、タイムスタンプはどのようにしていすればよいでしょうか？","response":"timestamp as of  'timestamp'で指定可能です。"}
{"category":"closed_qa","context":"","instruction":"Locationによって、ストレージ層が違う等のインフラ側の事由があった場合に使うイメージになりますか？","response":"はい、上記の理由でアンマネージドテーブル(Locationを指定する)を使うケースはよくあります。それ以外でも、ユーザー側でデータロケーションを確実に制御したい場合などでも同様に用いられる場合があります。"}
{"category":"closed_qa","context":"","instruction":"テーブルサイズ（容量表示）はByte単位になっていますか。","response":"はい、Byte単位です。"}
{"category":"closed_qa","context":"","instruction":"外部テーブルと、マネージドテーブルは、性能や機能で、差があるものですか？","response":"性能・機能の差はないです。"}
{"category":"closed_qa","context":"","instruction":"デフォルトスキーマとカスタムスキーマの違いはロケーションの違いのみですか？","response":"物理データ配置先の違いのみです。"}
{"category":"closed_qa","context":"","instruction":"Location指定したスキーマに作成したテーブルは外部テーブルになるのでしょうか？ 反対に、デフォルトスキーマにLocation指定の外部テーブルも作成できるのでしょうか？","response":"Location指定したスキーマ（DB)に対し、マネージド・外部テーブル両方作成できます。"}
{"category":"closed_qa","context":"","instruction":"external table と default table の違いは何ですか？","response":"External Tableは、Location指定して作成したテーブル(DROP TABLE時でもファイルは残る)。Default Table (こちらマネージドテーブルのことだと思います)はLocation指定なしで作成したテーブル(DROP TABLEでファイルも同時に削除される)。"}
{"category":"closed_qa","context":"","instruction":"外部データベースとの接続方法は何が対応されていますか？プライベート接続などは含まれますか？","response":"ODBC/JDBCコネクターを利用できるデータベースであれば対応しております。こちらもご参照ください。https://docs.databricks.com/dev-tools/python-sql-connector.html"}
{"category":"closed_qa","context":"","instruction":"externalテーブルの場合、ファイルを更新してもキャッシュをクリアしないと直近のデータを参照できないということでしたが、これはjdbcで外部のSQL DBに接続した場合も同じでしょうか？","response":"JDBC接続の場合、基本的にキャッシュはされないため、上記の事象は発生しません。"}
{"category":"closed_qa","context":"","instruction":"SHALLOW CLONEで元データを更新した場合はどうなるんですか？","response":"Shallow Cloneしたテーブルに変更を加えた場合は、_delta_logディレクトリと同じ階層にデータファイルが格納されます。"}
{"category":"closed_qa","context":"","instruction":"シャロークローンを作成した場合、トランザクションが発生したタイミングで実データが含まれるパーケットファイルが作成されるのでしょうか？","response":"はい、ご認識の通りになります。 Shallow Cloneしたテーブルに変更を加えた場合は、_delta_logディレクトリと同じ階層にデータファイルが格納されます。"}
{"category":"closed_qa","context":"","instruction":"DEEP CLONE もコピー元に影響与えないですよね？　クローンの値を変えたらコピー元テーブルはそのままですか？","response":"はい、別テーブルになりますので元データへの影響はないです。"}
{"category":"closed_qa","context":"","instruction":"テーブル元のデータなのでAテーブルからSHALLOW CLONEしてBテーブルを作った時に、Aテーブルが変更された場合どうなるのか知りたいです。","response":"Aテーブルに変更が発生した際にはBテーブルも影響を受けます。具体的にはAテーブルにVACUUMコマンドを使用した際には、Bテーブルが利用不可能になるなどの可能性があります。こちらのリンクもご参照いただければと思います。https://www.databricks.com/blog/2020/09/15/easily-clone-your-delta-lake-for-testing-sharing-and-ml-reproducibility.html"}
{"category":"closed_qa","context":"","instruction":"大量のデータをロードしたい場合にある程度の数区切ってロードするためのTIPSなどはありますでしょうか。","response":"COPY INTOなどを使っていただくと既に読み込まれているデータをスキップしてロードできるなど、データによって様々な最適化のテクニックがございます。"}
{"category":"closed_qa","context":"","instruction":"CREATE OR REPLACE TABLEではテーブルのバージョンが繰り上がる認識ですがあってますか？","response":"ご認識の通りです。DESCRIBE HISTORYを実行いただくと、Versionが+1されているかと思います。"}
{"category":"closed_qa","context":"","instruction":"DataBricksとAWSの連携で、CloudFormationテンプレートで環境が構築されますが、その際にEC2インスタンスがスポットインスタンスになっております。AWS側で勝手に止められたりしないのでしょうか？","response":"スポットインスタンスの場合は停止される可能性があります。クラスタを作成する際に設定が可能です。"}
{"category":"closed_qa","context":"","instruction":"コロンとピリオドの違いをご説明いただけますでしょうか？","response":"JSONを扱う場合コロンはJSON構造のデータを配列として展開して取扱やすくします。ピリオドは階層関係（テーブル内のカラム）を表します。"}
{"category":"closed_qa","context":"","instruction":"explode とは何をしてくれる関数なのですか？","response":"explode関数を使用することで、配列のカラムの各要素をそれぞれ行に展開することが可能です。"}
{"category":"closed_qa","context":"","instruction":"10億レコードあるJSONテーブルに対してこのような配列操作を実施する際の性能視点のTIPSはございますでしょうか。JSONの要素の一部をINDEXにして性能改善したいです。","response":"実施するクエリによりますが、JSON要素の一部をカラムにしてテーブルを作成いただいてOPTIMIZE＆ZORDERなど実施いただくとPerformanceが向上される可能性があります。"}
{"category":"closed_qa","context":"","instruction":"JSONに限らずですが、Parquetで列単位でファイルを分けるような考慮は可能でしょうか。10億件レベルの大規模テーブルにおいて特定の列にディスクアクセスを限定する手法があればお伺いしたいです。","response":"Delta Lakeであれば、ファイルレベルの最適な分割はOPTIMIZEコマンドで自動的に実施されます。ユーザー側で明示的に分割したい場合はPartition Byを使います。"}
{"category":"closed_qa","context":"","instruction":"DataBricksでWorkSpaceを作成する際に、自動でCloudFormation のスタックで環境が作成される このとき、EC2インスタンスも作成されるがスポットインスタンスでオンデマンドではないため、 AWS都合の強制停止されませんか？","response":"コンピューティングで使われるAWS側のVMはWorkspace作成時には作られず、クラスタ作成時に作られます。そちらでは「高度なオプション」の箇所でOn-demand／Spotの構成を選択することができ、Spotインスタンスを避けてすべての仮想マシンをOn-demandにすることもできます。"}
{"category":"closed_qa","context":"","instruction":"UDFでSQLからpythonの関数を呼び出すこともできますか？","response":"はい、可能です。こちらにサンプルコードがあります。 https://learn.microsoft.com/ja-jp/azure/databricks/spark/latest/spark-sql/udf-python"}
{"category":"closed_qa","context":"","instruction":"Delta lakeのオープンソースとしても使える機能と、Databricks固有の機能との一覧のようなリンクはございますでしょうか。","response":"delta lakeは今後全ての機能がOSS化されることが発表されております。現状の差異はdelta lakeのドキュメントをご参照ください。 https://delta.io/"}
{"category":"closed_qa","context":"","instruction":"UDFの定義の中で別のUDFはよびだせますか","response":"はい。可能です。"}
{"category":"closed_qa","context":"","instruction":"table_nameでデルタレイクを指定して、checkpoint_directoryにてRocksDBを指定するのでしょうか","response":"はい、その通りです。checkpoint_dirで指定したディレクトリの中にRocksDBファイルが格納されます。"}
{"category":"closed_qa","context":"","instruction":"AutoLoader の停止するタイミングはどうなりますか","response":"ユーザーが明示的に指定しない限り、動き続けます。ブラウザを閉じても止まりません。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderは通常ジョブクラスターで実行するものですか？","response":"はい、そうなるケースが多いです。 All Purposeでコードを開発し、Jobsで本番化する流れはAutoLoaderでも同じになります。"}
{"category":"closed_qa","context":"","instruction":"今回は新しいjsonファイルが追加されると処理されていたが、すでにあるjsonファイルの行が増えたら処理をするようにもできますか？","response":"Autoloaderの機能だと、ファイル内の行追加は検知できないです。バッチ処理のような形で（フラグなど付与して）ファイルごとの差分を判断して、対象のテーブルを更新するような形になるかと思います。"}
{"category":"closed_qa","context":"","instruction":"外部テーブルと内部テーブルというのがありましたが、クエリを作る際にどう指定すればよいでしょうか","response":"テーブル作成時にLocationを指定する場合はアンマネージドテーブル(別名: 外部テーブル)、Locationを指定しない場合はマネージドテーブルになります。"}
{"category":"closed_qa","context":"","instruction":"Spark Streamingを実行している間は、データを処理していなくてもDBUを消費し続ける（=コストが発生する）ということでしょうか？ 商用環境での実装を考えた時、例えば日次でデータを受信するようなケースにおいてStreaming%2BAutoLoaderで処理するのが良いのか、もっと高頻度で受信するようなケースに限った方がよいのか、処理方式の選び方があれば教えていただきたいです。 （Databricksにて日次処理のようなものも実装できるのでしょうか？）","response":"データを処理していなくてもコンピュートリソースは動作しており、DBUは発生します。また要件によって、日次のデータ更新にAutoloaderを使っていただいて問題ございません。加えて、データブリックスでも日次のスケジュールジョブをNotebook単位などで実装することが可能です。"}
{"category":"closed_qa","context":"","instruction":"Databricksの画面でdbfs内のファイルを参照する方法として、dbutils.fs.ls()を実行する以外の方法（例えば、ブラウザ上で確認するなど）はあるのでしょうか？","response":"権限によりますが左のタブから「データ」を選んでいただくとDBFSのディレクトリ構成などを確認できます。"}
{"category":"closed_qa","context":"","instruction":"ゴールドテーブルをキャッシュに固定するというような概念はございますでしょうか？","response":"はい、デルタレイクにはキャッシュ機能がありますので、頻繁にアクセスされるデータはクラウドストレージ上の実態からクラスタのローカルストレージにキャッシュされます。またSparkにもクラスタのメモリにキャッシュする機能がございます。"}
{"category":"closed_qa","context":"","instruction":"静的データはブロンズテーブルにはいっているのですか？","response":"ここではブロンズテーブルではなく、テンプビューに入っています。データの実体はクラウドプロバイダのオブジェクトストレージに入ってます。"}
{"category":"closed_qa","context":"","instruction":"createOrReplaceTempView() ではなく、createOrReplaceTable() とするとdelta lake table になるのでしょうか？","response":"saveAsTable()を使っていただくとDelta Tableになります。"}
{"category":"closed_qa","context":"","instruction":"非構造化データからデルタレイク化する時の変換処理が気になります。Databricksのポリシーに従って変換処理していて、シルバー、ゴールドレイヤーでのデータを信頼して良いかは、「ポリシーに従っているという意味では」という理解でよろしいでしょうか？","response":"はい、変換処理はユーザサイドでのコーディングに依存しますが、Databrickでは非構造化データを処置するさまざまな機能を提供していますので他のシステムでコーディングよりもシンプル且つ高速になります。"}
{"category":"closed_qa","context":"","instruction":"temp view と table の使い分けはどうしたらいいですか？","response":"tempビューはそのジョブを実行しているセッションのみ存在します。Tableはクラウドストレージにデータを永続定期に保存します。"}
{"category":"closed_qa","context":"","instruction":"table を temp view に変換してから処理を行っているのはなぜですか？","response":"連続的な処理であるストリーミングの差分データ抽出をSQLで処理しやすいようにtemp_viewで名前をつけています。"}
{"category":"closed_qa","context":"","instruction":"ゴールドテーブルを元にダッシュボードを作成した場合、ブロンズテーブルだけStreamで処理して、シルバー、ゴールドテーブルはダッシュボードを更新した時だけ随時処理をして最新のものにするでもいいのでしょうか","response":"はい、要件次第です。ブロンズはリアリタイムで更新し、シルバー、ゴールドは新規データだけ差分で取り込み定期的なバッチ的に更新するでも問題ありません。"}
{"category":"closed_qa","context":"","instruction":"オートローダーを使うとずっと課金されるということでしょうか。","response":"ご認識の通りです。クラスターが立ち上がった状態になるので、その時間の料金が発生します。ジョブ用の安価なクラスターを使って実行いただくことで、安価な実装が可能です。"}
{"category":"closed_qa","context":"","instruction":"マルチホップで類似するテーブルがどんどんできた場合、重複排除のようなディスク利用効率を向上させる機能はございますでしょうか。","response":"自動的に重複を排除する機能はございませんが、カタログ機能でデータリネージを取得できますので類似テーブル追跡で利用可能です。"}
{"category":"closed_qa","context":"","instruction":"データリネージは、自動的に作成されるのでしょうか。その条件がパイプラインが開発されていることでしょうか。","response":"データリネージは UnityCatalogを有効にしている環境でクラスタにリネージを取得する設定をしている場合には自動生成されます。https://docs.databricks.com/data-governance/unity-catalog/data-lineage.html"}
{"category":"closed_qa","context":"","instruction":"availableNowでバッチ的な使い方をした場合、差分だけを処理すると理解していますが、meanなどデータ全量が必要な集計は、結局データ全量を見ることになるのでしょうか？","response":"はい、全量データが必要な処理を差分を取り込み後のテーブルデータで実施ください。"}
{"category":"closed_qa","context":"","instruction":"DLTのデータリネージとUnityCatalogのデータリネージの違いは何でしょうか？","response":"それぞれのUIから確認できますが、基本的にテーブル間などデータの関連を表すという意味では同様のものです。"}
{"category":"closed_qa","context":"","instruction":"DELTA LAKEに使われるデータの管理とガバナンスに関するbest practiceについて、ご参考になれる資料はございますでしょうか？","response":"Unity Catalogという強力な機能があり、添付Linkをご参照ください。https://www.databricks.com/product/unity-catalog"}
{"category":"closed_qa","context":"","instruction":"AWS の場合、データプレーンのクラスタの実体は EC2 インスタンスになるのでしょうか？","response":"はい、あっております。"}
{"category":"closed_qa","context":"","instruction":"一つのセクションを誤って消してしまったのですが復旧できますか？","response":"はい、復旧可能です。 ノートブック右の時計のマークから過去のコードを復旧することができます。 こちらの記事をご参照ください。 https://qiita.com/taka_yayoi/items/dfb53f63aed2fbd344fc"}
{"category":"closed_qa","context":"","instruction":"データプレーン(クラスタやDBFSなど?) をAWSで構築する際の資料などありますでしょうか？","response":"基本的に、データプレーンの設定はVPCと関連AWSリソースをセットアップすることになり、以下のドキュメントに詳細がありますので、参照ください。  Databricksにおける顧客管理VPC(実践編) https://qiita.com/taka_yayoi/items/9e04f78ff025f77e52a6"}
{"category":"closed_qa","context":"","instruction":"ノートブックはPDFへエクスポートすることはできないのでしょうか。","response":"ノートブックは、HTML、.pyなどでエクスポートできますが、PDFはサポートしていません"}
{"category":"closed_qa","context":"","instruction":"Gitではなく、AWSのCodeCommitと連携することはできますか？","response":"可能です。Databricks ReposとAWS CodeCommitを連携する - Qiita https://qiita.com/taka_yayoi/items/ac6333ca9472e7749ec0"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeの制約の一覧の資料はありますでしょうか？","response":"こちらになります。  Constraints on Databricks | Databricks on AWS https://docs.databricks.com/tables/constraints.html"}
{"category":"closed_qa","context":"","instruction":"トランザクションはセル単位ではなくステートメント単位ということですかね？","response":"はい、その通りです。"}
{"category":"closed_qa","context":"","instruction":"SQLモードでdisplay関数は使えますか？","response":"SQLの結果はdisplay関数を適用したものと同等になります。"}
{"category":"closed_qa","context":"","instruction":"明示的にトランザクションの開始と終了を指定することもできますか？","response":"現時点では未サポートです。ステートメントレベルのみです"}
{"category":"closed_qa","context":"","instruction":"暗黙的なコミットが実行されるということですが、複数DMLを纏めたトランザクションも可能なのでしょうか。","response":"現時点ではマルチステートメントトランザクションには対応しておりません。"}
{"category":"closed_qa","context":"","instruction":"同時に更新処理走った場合は、ロックされますか？","response":"ロックはしません。楽観的コンカレンシーコントロールというコンセプトを採用しており、要は後出しが負けるというものです。  Delta Lake を深堀りートランザクションログの解析 | Databricks Blog https://www.databricks.com/jp/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html"}
{"category":"closed_qa","context":"","instruction":"ビューとテーブルの違いはなんですか？","response":"テーブルはデータが永続化されます。ビューはクエリーが保存されているのみで呼び出されるたびクエリーが実行され結果が返却されます。"}
{"category":"closed_qa","context":"","instruction":"Databricksとして、どうしてSparkのクラスタを用いる仕組みになっているのでしょうか。","response":"テラバイトを超える「大規模データ」を柔軟にハンドリングできるようにするためです。pandasのみでよろしければ、シングルノードクラスターを作成いただき小規模なワークロードに対応することも可能です。"}
{"category":"closed_qa","context":"","instruction":"Spark Streamingなどでストリーミングで追加している場合は、履歴はどのようにトランザクションログが増えていくのでしょうか？？","response":"マイクロバッチでストリーミングデータがWriteされるタイミングでトランザクションログが記録されます。"}
{"category":"closed_qa","context":"","instruction":"VACUUMを実行してもDESCRIBE HISTORYは過去のバージョンまで表示されていますが、これは正しいのですか？","response":"はい、正しい挙動です。 Vacuumは不必要なデータを削除する機能ですので過去のヒストリを参照することができます。"}
{"category":"closed_qa","context":"","instruction":"vacuum を実施しても、Historyは残るという理解であっていますでしょうか。","response":"はい、あっております。 Vacuumは不必要なデータを削除するだけなので、過去のヒストリはそのまま参照可能です。"}
{"category":"closed_qa","context":"","instruction":"postgresのauto vacuumのような機能もあるのでしょうか？","response":"Auto Vacuumはサポートしていませんが、ジョブ管理機能でスケジュール実行する設定は可能です。"}
{"category":"closed_qa","context":"","instruction":"マネージドテーブルと外部テーブルの使い分けとして、一例を教えていただくことできますでしょうか？","response":"一例としてはマネージドテーブルはテンポラリデーブル,永続的なデータは外部テーブルで作成すると、誤ってTableを削除した場合でもデータセットは残っていますので復旧が容易です。"}
{"category":"closed_qa","context":"","instruction":"外部テーブルはcleanupしなくてよいのでしょうか？","response":"外部テーブルでストレージ上のデータも削除したい場合には明示的にストレージコマンドで削除する必要があります。"}
{"category":"closed_qa","context":"","instruction":"過去のHISTORYが参照できても、VACUUMでデータが削除されていればRESTOREはできないということでしょうか？","response":"はい、あっております。 RESTOREするための条件は、ヒストリーログが残っていることとデータが残っていることの二つになります。 つまりVaccumすることでデータが消えるため、RESTOREすることはできません。 *Vacuum対象データにヒストリーログは含まれないので、Vacuumを実行しても過去のヒストリーは参照可能です。"}
{"category":"closed_qa","context":"","instruction":"ビューとテンポラリビューの使い分けは何でしょうか？ビューを使うメリットがわからないです。","response":"ビューはセッションが切れても残りますが、テンポラリビューはセッションが切れると消去されます。下記リンクもご参照ください。https://qiita.com/taka_yayoi/items/e90be9beeb1a744cee0d"}
{"category":"closed_qa","context":"","instruction":"外部テーブルとマネージドテーブルの定義は何でしょうか？ 外部テーブルは、CSVなどの外部データから作成されたテーブルで、マネージドテーブルは1から自分で作成したテーブルということなのでしょうか？","response":"マネージドテーブルはメタデータとデータの両方をDatabricksが管理します。外部テーブルはDatabricksがメタデータのみを管理いたします。テーブルをDROPした場合にマネージドテーブルはメタデータ・データの両方が削除されますが、外部テーブルはメタデータのみ削除されます。こちらもご参照ください。https://qiita.com/taka_yayoi/items/e90be9beeb1a744cee0d"}
{"category":"closed_qa","context":"","instruction":"テンポラリービューはどのタイミングで削除されますか？","response":"セッションが切れたタイミングで削除されます。"}
{"category":"closed_qa","context":"","instruction":"グローバルテンポラリービューはどのような状況で使われるのでしょうか？","response":"同じクラスターを使用している他ユーザにもViewを共有したい時に使用します。下記のリンクもご参照ください。https://qiita.com/taka_yayoi/items/e90be9beeb1a744cee0d#%E4%B8%80%E6%99%82%E3%83%93%E3%83%A5%E3%83%BC%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%A7%E3%81%99%E3%81%8B"}
{"category":"closed_qa","context":"","instruction":"global temp view はデータプレーンのクラスタ上に揮発性のデータを保存しているという理解で合っているでしょうか？","response":"ビュー自体ではデータを保持するわけではありません。Global temp viewはビューの定義が別のセッションからも参照できるようになっているものです。"}
{"category":"closed_qa","context":"","instruction":"external tableはdropしてもファイルが残るということでしたが、再度ファイルからテーブルに戻すことも可能なのでしょうか？","response":"はい、再度テーブル化することが可能です。 ただメタデータはDrop時に削除されますので、ヒストリーなどを参照することはできません。"}
{"category":"closed_qa","context":"","instruction":"jsonファイルのselectでフォーマットが異なるjsonファイルがある場合はエラーになるのですか？","response":"いいえ、エラーにはなりません。フォーマットが異なる場合は、存在しないカラムにはNUlLLがセットされて返却されます。"}
{"category":"closed_qa","context":"","instruction":"USING CSVで作成したテーブルに対してUPDATEなどを実行したら、元のCSVファイルも更新されるのでしょうか？","response":"CSVフォーマットのテーブルに対してはUPDATE　できません。DELTAフォーマットで作成ください。"}
{"category":"closed_qa","context":"","instruction":"csv ファイルを直接 delta 形式で取り込むことはできるのでしょうか。","response":"いいえ、一度CSVでデータで読み込み、Deltaフォーマットに変換してデータを書き戻していただく必要があります。"}
{"category":"closed_qa","context":"","instruction":"partitioned byとzorderは併用できますか？","response":"はい、併用可能です。"}
{"category":"closed_qa","context":"","instruction":"スキーマという言葉が各所に出てきていますが、CREATE SCHEMAとCREATE TABLE時のスキーマ定義とは別ものの認識でよいですか","response":"紛らわしくて申し訳ありません、別物です。前者はテーブル等を入れる箱としてのスキーマ、後者はカラムやデータ型等の構造を意味するスキーマ、として使っています。"}
{"category":"closed_qa","context":"","instruction":"14日間制限のトライアルの他に、Databricks Community Editionを利用できる情報があります。 Community Editionは現在も登録し利用可能ですか、制限などありましたら教えていただけませんか","response":"作成できるクラスターのサイズや利用できない機能などの制限があります。  Databricks Community EditionはDatabricksと何が違うのか？ - Qiita https://qiita.com/taka_yayoi/items/800fd2714f460a9a1c40"}
{"category":"closed_qa","context":"","instruction":"高階関数とネストしたSQLでは、パフォーマンスの観点でどちらが優れていますか？？","response":"高階関数もビルトイン関数としてサポートしているので、ネイティブSQLと大差ありません。"}
{"category":"closed_qa","context":"","instruction":"作成したUDFを高階関数で利用することもできるのでしょうか","response":"高階関数のtransform/existsなどは引数にfunctionを受け取るのでいけるはずです。今度確認して記事書いておきます。  Databricksにおける高階関数のイントロダクション - Qiita https://qiita.com/taka_yayoi/items/f28a9af7a7e06e7722ed#transformarrayt-functiont-u-arrayu"}
{"category":"closed_qa","context":"","instruction":"ソースデータのディレクトリは、Databricksが構築されているクラウドがAWSの場合は、S3に限定されますか？それとも、AWS以外のGCPのCloudStorageなども指定できるのでしょうか。","response":"クレデンシャル設定すれば、後者の通り、どのクラウドのストレージも参照可能です。例えば、AWS上のDatabricksからGCPのGCSを参照することも可能です。"}
{"category":"closed_qa","context":"","instruction":"streamしているとtargetテーブルにログがたまっていくかと思いますが、やはり定期的にVACCUMしないとまずいのでしょうか？","response":"はい、その通りです。VACUUM以外のオペレーションはファイル削除は一切行いません。"}
{"category":"closed_qa","context":"","instruction":".format('cloudFiles')とは具体的にどういうフォーマットタイプですか？","response":"こちらはAutoLoaderを使うという宣言でして、実際の読み込みはcsv, jsonなど指定する必要があります。またwriteすると Delta formatになります。"}
{"category":"closed_qa","context":"","instruction":"AutoLoaderを本番環境で使う場合、クラスターは永続的に起動させることになるのでしょうか？","response":"ケースバイケースです。永続的に実行することもできますし、バッチで起動して差分のみを処理することも可能です。  DatabricksのAuto Loader - Qiita https://qiita.com/taka_yayoi/items/df143647dcf5942b51c6#%E3%83%97%E3%83%AD%E3%83%80%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%A7%E3%81%AEauto-loader%E3%81%AE%E5%AE%9F%E8%A1%8C"}
{"category":"closed_qa","context":"","instruction":"streamしたデータをpartition byしたい場合は、autoload_to_tableの中身で指定する形になるのでしょうか？","response":"はい、書き込み(writeStream)する部分でpartitionBy()を指定する形になります。"}
{"category":"closed_qa","context":"","instruction":"増分データ処理を実施するテーブルについて制限はありますか？MANAGEテーブルのみ対応可能など。Unity_catalogを利用して外部スキーマに作成したテーブル等も対象になりますか？","response":"特に制限はありません。Unity Catalogの外部テーブルでも大丈夫です。"}
{"category":"closed_qa","context":"","instruction":"AutoLoaderはどのような条件で追加であると判定しているのでしょうか？","response":"特定ストレージパスでファイル一覧をチェックした後、再度チェックした際に新規に作成されているファイルを検知して処理を行います。"}
{"category":"closed_qa","context":"","instruction":"AutoLoaderですでに一度取り込まれたファイルに追加や修正があった場合、その追加・修正がある行だけ取り込まれるのでしょうか？","response":"AutoLoaderでは同一ファイルに対する追加・更新については残念ながら取り込まれません。新規ファイル名として追加していく必要があります。"}
{"category":"closed_qa","context":"","instruction":"永続化したいのであればビューやテーブルにした方がよいと思うのですが、テンポラリビューとしている理由は何ですか？","response":"テンポラリビューはアドホック分析などでクイックにSQLを用いてデータ探索に用いるのが主要なユースケースです。永続化するのであればテーブル・ビュー(こちらはクエリーが永続化されますが)が適しているのはおっしゃる通りです。"}
{"category":"closed_qa","context":"","instruction":"bronzeでは、Auto Loaderを使用するということは、ジョブでそのNoteBookは常に実行されたされた状態を推奨されるのでしょうか。","response":"データの到着頻度が高いのであれば連続稼働が推奨となりますが、そうでない場合にはバッチ的にAutoLoaderを起動し、差分のみを処理する構成にすることも可能です。"}
{"category":"closed_qa","context":"","instruction":"マイクロバッチとは何ですか？","response":"完全に連続的にデータを処理するのではなく、流れ込んでくるデータをマイクロバッチという小規模のデータの塊に区切ってそれに対してバッチ処理を行うという方式です。"}
{"category":"closed_qa","context":"","instruction":"onceとavailableNowの違いは、並列でバッチを実行するか否かということですか？","response":"Onceは対象データすべてを一度に処理しますが、AvailableNowは指定されたファイル数やバイト数に分割して処理します。"}
{"category":"closed_qa","context":"","instruction":"マイクロバッチについて、概要を教えていただけますでしょうか。","response":"完全に連続的にデータを処理するのではなく、流れ込んでくるデータをマイクロバッチという小規模のデータの塊に区切ってそれに対してバッチ処理を行うという方式です。"}
{"category":"closed_qa","context":"","instruction":"trigger について理解しきれておらず、trigger: once, availableNow のユースケースの違いを教えていただけますでしょうか","response":"Onceは対象データすべてを一度に処理しますが、AvailableNowは指定されたファイル数やバイト数に分割して処理します。後者は複数バッチでデータ処理を行うことができます。  プロダクションにおけるSparkストリーミング: ベストプラクティスコレクションPart 1 - Qiita https://qiita.com/taka_yayoi/items/6b6904dd78991efbaa81"}
{"category":"closed_qa","context":"","instruction":"kafkaやMQTTのデータを直接キャプチャすることはできるのでしょうか？","response":"Kafkaは標準のSparkコネクタで接続できます。MQTTは3rd party(有償・無償含め)のコネクタがいくつかありますので、それらを使用すると接続できます。"}
{"category":"closed_qa","context":"","instruction":"マイクロバッチってどういう意味でしょうか？","response":"完全に連続的にデータを処理するのではなく、流れ込んでくるデータをマイクロバッチという小規模のデータの塊に区切ってそれに対してバッチ処理を行うという方式です。"}
{"category":"closed_qa","context":"","instruction":"ストリーミングで読み込む際のワンタイムマイクロバッチとトリガー型マイクロバッチはどのような使い方をするのが理想出来でしょうか。","response":"こちらのページに詳しく書いてあります。https://learn.microsoft.com/ja-jp/azure/databricks/structured-streaming/triggers トリガー型のavailableNowの方が後発で機能も新しいので、これから利用される場合は、こちらがおすすめです。"}
{"category":"closed_qa","context":"","instruction":"メダリオンアーキテクチャの ブロンズ、シルバー、ゴールドは ラルフ・キンボール博士が提唱した ステージング、ファウンデーション、データマート と同じだと思えばいいですか？","response":"はい、同じような考え方になります。"}
{"category":"closed_qa","context":"","instruction":"sparkを実装する際、SQLベース・pysparkベースどちらで実装を進めた方が良いのか指針はございますでしょうか。プロジェクト次第で任意に決めればよい感じでしょうか。","response":"特に指針はないですが、テーブル操作であればSQLの方がシンプルなのと、Databricks SQLのSQL Editorでも利用できるためお勧めですが、pysparkの方がUDFやMLライブラリを使えるため拡張用としてpysparkも必要になるケースがあります。"}
{"category":"closed_qa","context":"","instruction":"ゴールドテーブルはデータマートの位置づけであってますか","response":"はい、一般的にはその位置付けになります"}
{"category":"closed_qa","context":"","instruction":"What is the purpose of having Bronze, Silver and Gold table, we can't compute everything in one table directly?","response":"No. This is reference architecture. So you can create only one table or more tables as well."}
{"category":"closed_qa","context":"","instruction":"DLTのCONSTRAINTでVIOLATIONした行をDROPではなく、どこかのテーブルに追加するとかは出来ないのでしょうか？","response":"default機能だとexpect、expect or drop、expect or fail　の３つの処理だけなのですが、UDFとして機能を拡張することが可能です。https://qiita.com/taka_yayoi/items/123347aa8323b4a90730"}
{"category":"closed_qa","context":"","instruction":"つまりDeltaLiveTableとは、 AutoLoaderやSparkによるストリーミング処理をSQLでも書けるようにした仕組みということですか？","response":"そのような特徴もありますし、それ以外にもDLT の機能があります。データの品質チェックをしてくれたり、実行するインフラを自動的に調整してくれたり、リネージュできるなどです。https://qiita.com/taka_yayoi/items/7fe8ed2c2f95fd53cc3d"}
{"category":"closed_qa","context":"","instruction":"DLTのノートブックを更新してパイプラインに反映する場合、更新前のノートブックの状態で実行中のパイプライン処理が中途半端になったりしないのでしょうか。","response":"DLTでは処理ごとにテーブルに反映させていくので、ノートブック修正後はそのテーブルに対して実行されるようになります。もし中途半端になるようなパイプライン処理の場合は、最初から実行する必要があります。もしくは変更箇所のみ更新していくCDFという機能がございます。https://qiita.com/taka_yayoi/items/5319b9b010ce4703e98a"}
{"category":"closed_qa","context":"","instruction":"月に一度ぐらいでデータが追加されるような低頻度の増分テーブルを作成するには、DLTで実装しスケジュールを組むのが良いでしょうか？また、ファイルが処理済みかどうかの管理はどこで行っていますか？","response":"月一程度の頻度でもパイプライン運用管理の容易性からDLTで実装することをお勧めします。ファイルがしょリズムかどうかは、内部で動作しているSparkのチェックポイントファイルで管理されています。"}
{"category":"closed_qa","context":"","instruction":"パイプライン構成時、ターゲットスキーマが1つしか選べないようですが、任意のテーブルのみ別のスキーマに変更することは可能でしょうか？","response":"いいえ。現在は単一スキーマのみの対応となります。"}
{"category":"closed_qa","context":"","instruction":"pysparkでSQLを叩く時と、純粋にSQLを叩くのでは、処理速度は大幅に変わりますか？","response":"いいで、裏では同じ Spark APIに変換されるため、処理速度に違いはありません。"}
{"category":"closed_qa","context":"","instruction":"ワークフロー同士の実行順序や先行ワークフローの正常終了時に次のワークフローを実行するなど、の制御は可能ですか？Azure Data Factoryでは標準機能になくて、作りこんで実装してます。","response":"複数のタスク同士で依存関係(先行・後続)を定義して実行させることが可能です。また、より発展させて(複数のタスクを包含する)ジョブ同士に依存関係を定義することも今後のリリースで可能になる予定です。"}
{"category":"closed_qa","context":"","instruction":"photonアクセラレーターを今回チェックつけましたが、理由を教えていただけますでしょうか。","response":"Photonは通常よりも高速な処理を可能にするエンジンです。その分1時間あたりの課金量が増えますが、今回は実行を速くするためONにしています。"}
{"category":"closed_qa","context":"","instruction":"API経由で実行するというのがいまいちわかっていないのですが、ワークフローは事前にUIから作成しなければいけないということでしょうか？","response":"いえ、ワークフロー（ジョブ）の作成もAPI経由で可能です。APIドキュメントをご参考までにお知らせいたします。 https://docs.databricks.com/dev-tools/api/latest/jobs.html"}
{"category":"closed_qa","context":"","instruction":"スケジューリングすると、ジョブ実行開始時にクラスターは自動起動し、ジョブが完了すると自動的にクラスターは停止するという認識でよろしいでしょうか？","response":"はい、その通りです。"}
{"category":"closed_qa","context":"","instruction":"パイプライン名が一意である必要があるとのことですが、ワークスペース内で一意でしょうか。または、グローバルで一意でしょうか。","response":"ワークスペースで一意であれば問題ありません。"}
{"category":"closed_qa","context":"","instruction":"Redshift、Snowflakeと比較しての優位性はどうですか","response":"ベンチマークテストにおいて、パフォーマンスとコスト効率が挙げて頂いた2製品よりも優れた結果も出ております。またアーキテクチャとして、ETLで作ったデータをコピー/転送するのではなく、Databricks上のETLで作成したテーブルをクラウドストレージに置いたまま参照できるのも利点だと思います。"}
{"category":"closed_qa","context":"","instruction":"SQLウェアハウスとは、SQL実行に特化したクラスターの一種であると理解して問題ございませんでしょうか？","response":"はい、ご理解の通りで合っております。"}
{"category":"closed_qa","context":"","instruction":"DatabricksSQLを利用する場合は、SQLウェアハウスが必須になりますか？","response":"はい、必須になります。"}
{"category":"closed_qa","context":"","instruction":"ダッシュボードのクエリ結果をどこに保存しているのでしょうか。SQLウェアハウスのインスタンス上でしょうか。その場合、インスタンスを削除するとダッシュボードやクエリ結果などもなくなってしまうのでしょうか。","response":"ダッシボードとクエリはインスタンスではなくワークスペースのコントロールプレーンに保存されます。インスタンスを削除して別のSQLウェアハウスでも実行できます。"}
{"category":"closed_qa","context":"","instruction":"ダッシュボード上から、テーブルデータを直接編集するような機能はありますか？","response":"ダッシュボードの表示内容のテーブルの編集ができません。クエリを編集して可視化内容を変更する流れになります。"}
{"category":"closed_qa","context":"","instruction":"Databricksで、機械学習用のラベル付けを手作業で行いたい場合、どのような方法が一般的でしょうか","response":"ローカルでラベルづけしてその結果をオブジェクトストレージにあげたり、Labelboxなどのアノテーションプラットフォームと組み合わせてラベリングすることが多いです。 https://docs.labelbox.com/reference/databricks-connector"}
{"category":"closed_qa","context":"","instruction":"ユーザー権限によって項目のマスキングを柔軟に指定できますか？たとえば、電話番号の先頭3桁だけ表jして残りは「*」で文字数だけ表示するとか","response":"はい、ダイナミックマスキングが可能です。 https://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/create-views#create-a-dynamic-view"}
{"category":"closed_qa","context":"","instruction":"UnityCatalogを使うことで複数ワークスペースの権限管理を一元管理できるとのことですが、 ワークスペースを複数利用するシナリオ例にはどんなものがあるのでしょうか。 (どんなときにワークスペースを分けるべきか)？","response":"実務においては複数のワークスペースを持つことは珍しくなく、利用部門(IT部門とデータサイエンス部門、等)や、プロダクション環境とステージング環境、等でワークスペースを分けることが多々ございます。"}
{"category":"closed_qa","context":"","instruction":"Parquetのファイルに対してACLをカタログからかけることは可能でしょうか。（テーブルACLのANY FILEで可能でしょうか。また、TABLEにも同時にACL制御をかけた場合、優先度や排他的なのかなどはどういう仕様でしょうか。）","response":"おっしゃる通り、ANY FILEでクラウドストレージ上のParquetファイルそのものへのアクセス権を付与することも可能ですが、通常は行いません。 これを与えた場合、テーブルへのアクセスが禁止されているにも関わらず、実体のParquetファイルにはアクセスできるということが起きてしまうためです。  (参考) https://docs.databricks.com/data-governance/table-acls/object-privileges.html"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogによるデータリネージについて質問ですが、create tabel as selectを使ってない場合も、delta logからデータフロー図やデータカタログを作成する事は可能ですか？","response":"リネージはSparkの実行計画を用いて生成されるので、現状ではdelta logからの作成は未サポートです。"}
{"category":"closed_qa","context":"","instruction":"Databricksの中でリレーショナルデータモデルを組むことはできますか。　また、データモデルをPower BIに取り込むことはできますか？","response":"はい、Databricksで、リレーショナルデータモデルも実現できます。またPowerBIをDatabricks SQLに接続することで、Databricks上のテーブルやビューをPowerBIで可視化を行うことができます。"}
{"category":"closed_qa","context":"","instruction":"Last Mile ETLとはどのような意味ですか？ゴールド層からBIや機械学習へのデータ取り出しは他の製品なら何らかの壁があるけどdatabricksなら簡単とかでしょうか？","response":"IT部門等がETLで作成したデータマートに対して、データアナリスト等の方々が自身の分析タスクのためにSQLクエリでデータを加工することを指してLast Mile ETLと表現しています。ここではその手段としてDatabricks SQLを使用しています。"}
{"category":"closed_qa","context":"","instruction":"SQL言語を勉強する良い教材はありませんか？","response":"SQL初学者向けの書籍として、以下のものが役立ったようです。ご参考になればと思います。\n\nhttps://www.amazon.co.jp/SQL-%E7%AC%AC2%E7%89%88-%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89[…]999d64e71900ae1c128fa665a057&language=ja_JP&ref_=as_li_ss_tl\n\nhttps://www.amazon.co.jp/%E9%81%94%E4%BA%BA%E3%81%AB%E5%AD%A6%E3%81%B6SQL%E5%BE%B9%E5[…]56cfb72-f726-4c7c-8cdc-1ad68324ac68&pd_rd_i=4798157821&psc=1\n\nhttps://www.amazon.co.jp/SQL%E3%82%A2%E3%83%B3%E3%83%81%E3%83%91%E3%82%BF%E3%83%BC%E3[…]286886116e3678c8caf784641376&language=ja_JP&ref_=as_li_ss_tl\n\nhttps://www.amazon.co.jp/SQL%E5%AE%9F%E8%B7%B5%E5%85%A5%E9%96%80%E2%94%80%E2%94%80%E9[…]3cad63b-a7e0-4c34-ba5e-ca2c1158df3e&pd_rd_i=4774173010&psc=1"}
{"category":"closed_qa","context":"","instruction":"Notebook の SQL と、SQL モードの SQL は同じものですか？ 違いがありますか？ （文法、制限、など）","response":"NotebookのSQLはData EngineerがETLバイプラインを作成、SQLモードのSQLはData AnalystがAdhoc分析を行うことを想定してデザインされています。\nSQLモードで利用できる機能については、https://docs.databricks.com/sql/language-manual/index.html をご参照ください。\nNotebook上でのSQLを利用してETLを構成する場合のガイドは https://docs.databricks.com/delta/index.html をご参照ください。"}
{"category":"closed_qa","context":"","instruction":"Notebookでのメソッドの取り扱い方について教えてください。開発ではライブラリや処理を分けて、必要な時にimportしますが、Notebookでもライブラリ用のNotebookとメイン処理するNotebookを分けて、必要なタイミングでライブラリNotebbokを呼び出す、というやり方がいいのでしょうか？","response":"はい、おっしゃるやり方でも結構だと思いますし、作成したライブラリをwheel等でパッケージ化してこちらでご案内しているようにワークスペースあるいはクラスターにライブラリとして組み込むパターンもございます。https://docs.databricks.com/libraries/index.html "}
{"category":"closed_qa","context":"","instruction":"Notebook上でSQLの中でPythonの変数を使用したり、SQLの結果をPythonの変数に設定可能ですか？（念のための確認。Pythonコード中でsparkを使用すればSQL実行可能なのは承知済み）","response":"はい、可能です。ただし、単一のセル内だと現状できません。\nTableやDFへのアクセスを前提として、言語の使い分けを想定しております。"}
{"category":"closed_qa","context":"","instruction":"データベース名／テーブル名／カラム名 などで、日本語の名前は使えますか、使う方法は？ 使う際の注意点があれば教えてください。","response":"テーブル名カラム名で日本語はご利用できますが、Viewの作成を行うことができません。現在、Private Previewとしてお試し申し込みいただいているUnity Catalog環境下だとViewの作成も実施可能です。"}
{"category":"closed_qa","context":"","instruction":"DBSQLからNotebookを呼び出せますか？","response":"現状、DatabricksSQLからNotebookを呼び出すことはできません。"}
{"category":"closed_qa","context":"","instruction":"DBSQLからPythonの関数を呼び出せますか？","response":"DatabricksSQL上でPythonコードを実行することだけが目的であれば、https://docs.databricks.com/dev-tools/python-sql-connector.html に紹介しているpython sql connectorをご利用ください。"}
{"category":"closed_qa","context":"","instruction":"ストアド（PROCEDURE/FUNCTION）を使用可能でしょうか？","response":"こちらが参考になりそう。\nhttps://datathirst.net/blog/2018/10/12/executing-sql-server-stored-procedures-on-databricks-pyspark\nhttps://stackoverflow.com/questions/60354376/how-to-execute-a-stored-procedure-in-azure-databricks-pyspark\n一方でUser Defined Functions(UDF)をご利用いただくことを推奨\nhttps://databricks.com/blog/2021/10/20/introducing-sql-user-defined-functions.html\nhttps://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html?_ga=2.48754721.609412854.1656255900-429840774.1616028656#create-function-sql"}
{"category":"closed_qa","context":"","instruction":"使用可能な場合：作成したストアドの他Notebookから呼び出し方法について教えてください。","response":"UDFの場合は、Create時にTemporaryを指定しなければメタストアに登録されるので、他からの利用可能と想定しています。"}
{"category":"closed_qa","context":"","instruction":"FUNCTIONが使用できる場合：SQL文への組み込み方法について教えてください。","response":"CREATE FUNCTIONでのご利用方法は以下のURLをご参照ください。\nhttps://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html?_ga=2.48754721.609412854.1656255900-429840774.1616028656#create-and-use-a-sql-scalar-function"}
{"category":"closed_qa","context":"","instruction":"SQL実行結果を画面に表示する際の上限設定について（現状10,000行）発行する際に変更することは可能でしょうか？","response":"増やしたい場合：\nDatabricks Supportへの依頼をお願いいたします。\n減らしたい場合：Limit句により可能です。\nhttps://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-limit.html"}
{"category":"closed_qa","context":"","instruction":"SQL Editor はバージョン管理みたいなことはできませんか？","response":"残念ながら現状は作成したSQLのバージョン管理は組み込まれておりません。\nクエリ管理はタグをご利用いただくことを想定。"}
{"category":"closed_qa","context":"","instruction":"グラフを作るときの Edit Visualization で設定する項目の意味を詳しく知りたいときはどこを見れば良いでしょう？例えば Group by はどんな感じで動くのかなど、例や説明があると嬉しい。","response":"サンプルダッシュボードから見ていくのが良さそうです。\n基本はGroup byしたものをX軸に選択して、評価したいものをY軸に選択する。\nサンプルダッシュボードの拡充をProductにリクエストしておく\nMarketplaceを通じたImportサンプルの拡充\""}
{"category":"closed_qa","context":"","instruction":"ダッシュボードに入れるグラフやテーブルのフォントサイズや色は、変更できませんか？","response":"こちらもサンプルダッシュボードをベースに説明するのが良さそう。\nグラフについては、作成時にColorタブで各系列で利用する色を変更可能。"}
{"category":"closed_qa","context":"","instruction":"Sankey や Sunburst の使い方の説明が知りたい。ドキュメントにはあまり記載がない気がします。","response":"確かにオンラインDocにない。\n該当のVisualizationを作成しようとしたときに必要なデータ系列の説明が表示されます。"}
{"category":"closed_qa","context":"","instruction":"GRANTの使い方を詳しく知りたい。","response":"この辺りご参考ください。\nhttps://docs.databricks.com/sql/language-manual/security-grant.html\nhttps://docs.databricks.com/sql/language-manual/sql-ref-privileges.html#privilege-types\nhttps://docs.databricks.com/sql/language-manual/sql-ref-privileges.html#privilege-matrix\n\nData Engineering Associateのコースでカバーされます。"}
{"category":"closed_qa","context":"","instruction":"ダッシュボードの Run as owner と Run as viewer の使い方のコツを知りたい。テーブルのGRANTの設定とうまく合わせないといけないと思うのですが、ややこしいです。","response":"基本はDashboardのOwner以外はRun as Viewerで良いですが、Dashboardに組み込まれているクエリーにQuery Based Dropdownが含まれている場合には、Run as Ownerを適用してください。"}
{"category":"closed_qa","context":"","instruction":"クエリフィルタの使い方、使い道、コツなどあれば知りたい。（こういう場面で有効みたいな）","response":"まず、DocのURLは以下の通り。\nhttps://docs.databricks.com/sql/user/queries/query-filters.html\nクエリパラメータとの比較でご説明するとわかりやすいかもしれません。\nクエリパラメータがクエリ実行時にパラメータを評価するのに対して、クエリフィルターはクエリー実行後ブラウザにデータがロードされたのちに表示される結果を制限します。クエリフィルタはクエリーを一度実行したのちにインタラクティブに表示範囲を変更するイメージとなります。"}
{"category":"closed_qa","context":"","instruction":"クエリパラメタを使うコツもあれば知りたい。使いだすとどんどん増えて分かりにくくなってしまったりします。","response":"まず、DocのURLは以下の通り。\nhttps://docs.databricks.com/sql/user/queries/query-parameters.html\n具体的な悩みをお伺いしてみる。"}
{"category":"closed_qa","context":"","instruction":"Query Based Dropdown List は、どういうタイミングでクエリをかけるのですか？（毎回かかるのではないようなので。）","response":"基本的にDropdownを作成しているクエリは毎回実行しているはず。具体的に事象を聞いてみる。\n参照先クエリでがスケジューリング設定によりクエリーが実行され、参照元クエリのDropdownで選択可能なListが変わってきます。\n特に参照元のクエリのDefault値の設定には注意が必要です。\n\n参照先クエリのスケジューリングと権限管理がポイントです。"}
{"category":"closed_qa","context":"","instruction":"User Function の基本的な使い方を知りたい。","response":"こちらのNotebookにUDF(SQL)の定義例がありますので、参考にしてください。\nhttps://databricks.com/notebooks/sql-user-defined-functions.html\nまた、DocのURLは以下の通りです。\nhttps://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html"}
{"category":"closed_qa","context":"","instruction":"ダッシュボードのスケジュールを指定するとき、例えば朝6時に一斉に動いても大丈夫か？時間をばらした方が良いか？","response":"基本は一斉に動作させても問題ないです。複数のクエリが重たくて、全体のリフレッシュが遅い場合には、Cluster Sizeの変更もしくはCluster自体を分けるなどの対処が有効となります。"}
{"category":"closed_qa","context":"","instruction":"Photonの詳しい使い方（活かし方）","response":"対象ファイルフォーマット：\nParquet, Delta （Json対応は開発中）\n処理：\nSQLやPysparkでのDataframeの取り扱いー通常ETL\nSQLでのUDF(python, ScalaのUDF対応は開発中）\n特徴：\n特にファイルIO系が高速化されている\n対象データセットが大規模な場合に非常に有効\nPhoto Introduction を共有させていただく\n\nJob化する前にAll purposeのClusterでPhotonの有効性を検証（通常のCluster vs Photon)\n→ Photonを有効化したJob ClusterでJobを実行するようにJob定義"}
{"category":"closed_qa","context":"","instruction":"複数のダッシュボードを作ったときに、あるダッシュボードからボタンを押すと別のダッシュボードに遷移するみたいなことを、実現する技はありませんか？","response":"残念ながら、その機能はございません。\nSI的に解決する手段はありそうですので、需要が高まった場合はご連絡ください。"}
{"category":"closed_qa","context":"","instruction":"PhotonはNotebookでもDBSQLでも使えますか？","response":"はい、お使いになれます"}
{"category":"closed_qa","context":"","instruction":"pyspark で、こういうことしたいときには このようにプログラムを書く、みたいな情報が集まっているサイトや本をご存じでしたら教えてください。","response":"一般的なPysparkの学習(Dataframeの取り扱いなど)：\nhttps://spark.apache.org/docs/latest/api/python/getting_started/index.html\n\n入門的な日本語書籍：\n入門Pysparkという本がありますが、最初の数章くらいしか役立ちませんでした（おすすめしない）\n\nDatabricks社が提供しているNotebookギャラリー：\nhttps://databricks.com/discover/notebook-gallery\n\nDatabricks solution Accelerator：\nhttps://databricks.com/solutions/accelerators"}
{"category":"closed_qa","context":"","instruction":"Delta Live Table を使う時に、OPTIMIZE や VACUUM はどのように（どのようなタイミングで）使ったら良いでしょうか。関係ないのかもしれませんが、Change Data Feed の説明のところに、VACUUM を使うと変更データが削除されてしまうと書いてあったので、気になっています。","response":"DLT使用時には24時間ごとにOPTIMIZEとVACUUMが自動的に実行されます。VACUUM実行時には保持期間（デフォルトでは7日間）を過ぎた過去のテーブルは削除されますので、それより前の状態のテーブルは参照不可となります。これを無効にしたい場合には、Table Properties（https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-python-ref.html#table-properties）の変更を行なってください。\n\n関連情報：\nhttps://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-faqs-issues.html#frequently-asked-questions-faq"}
{"category":"closed_qa","context":"","instruction":"前回のリストの Query Based Dropdown List の更新タイミングの続き： あるダッシュボードから呼び出しているドロップダウンリスト用のクエリがあるのですが、そのクエリを SQL Editor で自分で Run しないと変更されないケースがあります。ダッシュボードを使っているだけだと、１か月前のクエリの結果でも使い続けられます。（クエリの結果は毎日変わるはず。）何か必ずクエリを走らせられる設定はありますか？","response":"Dashboard内にQuery Based Dropdown Listが含まれている場合には、Dashboardの実行権限をRun as Ownerとしてください。"}
{"category":"closed_qa","context":"","instruction":"前回のリストの Run as owner / Run as viewer の続き：\nA)  select current_user() というクエリをダッシュボードから実行した場合、Run as owner だとクエリのオーナーの名前が、Run as viewer だとダッシュボードを Refresh したユーザの名前が、表示されると考えて正しいか？\nB)  ダッシュボードから呼び出すクエリの中で、INSERT/UPDATE/MERGE のようなデータの更新を行うと、Run as owner では動作するが、Run as viewer だと read-only モードだと言ってエラーになります。Run as viewer で更新を行うクエリを実行することはできませんか？（なんらかの設定やGRANTでできないか？）","response":"Dashboard内にQuery Based Dropdown Listが含まれている場合には、Dashboardの実行権限をRun as Ownerとしてください。（こちらプロダクト側では既存のRedashの制約事項だと認識しており、Mid termでは改善を予定しているとのことです）"}
{"category":"closed_qa","context":"","instruction":"SQLのQueryのShareをグループに出して共有しようと思っています。グループを追加するときに、Can Run は選べるのですが、Can Edit がグレーになって選べません。Can Edit を選べるようにするには、何か条件がありますか？","response":"https://docs.databricks.com/sql/user/security/access-control/query-acl.html#query-access-control\n上記と以下のオンラインドキュメントが参考になるかと思いますが、ヒヤリング必要かも。\nhttps://docs.databricks.com/sql/user/security/access-control/query-acl.html#manage-query-permissions-using-the-ui\n\nクエリ共有時にはRun as Viewを選択していただいた上で編集権限をお渡し下さい。"}
{"category":"closed_qa","context":"","instruction":"PythonでUDFを書いています。文字列のリスト（SQLだとARRAY）を結果として返すようなUDFは作れませんか？（結果はスカラーしかダメですか？）","response":"https://stackoverflow.com/questions/47682927/how-to-create-a-udf-in-pyspark-which-returns-an-array-of-strings\n上記が参考になります。"}
{"category":"closed_qa","context":"","instruction":"Databricks Academyの講座は日本語対応にできますか？","response":"はい、一部の研修は日本語対応になっています。"}
{"category":"closed_qa","context":"","instruction":"PysparkでCSVを出力する際に、UTF-8BOM付にエンコードして出力することはできますか？","response":"Pysparkでの実行には少しカスタマイズが必要ですが、こちらが参考になりそうです。（書き込み先を定義する際にencoding=\"utf_8_sig\"とする）"}
{"category":"closed_qa","context":"","instruction":"Zipとか圧縮形式マターで直接Readできる？","response":"こちらのBlogでまとめてくださっている方がいました（Databricksのパートナー会社の方です）\nhttps://qiita.com/manabian/items/04df5d5c88831590be89\n.gzはOKだけど、.gzipはダメとか癖があります。"}
{"category":"closed_qa","context":"","instruction":"NLP関連のExampleがありますか？","response":"参考になりそうなリンク：\n\nhttps://www.youtube.com/watch?v=3LJFDJwIxDE\nhttps://qiita.com/taka_yayoi/items/f34a5accaa8f7cad603a\nhttps://qiita.com/taka_yayoi/items/ebec25578a7cfe2f50b0\nhttps://qiita.com/taka_yayoi/items/fcf396cf75418e87ec76\nhttps://qiita.com/taka_yayoi/items/a6365e0f44d166ac343c\nhttps://qiita.com/taka_yayoi/items/35863e696c72f0d08909\nhttps://qiita.com/taka_yayoi/items/d639ec54a3fd751aad0a\nhttps://qiita.com/taka_yayoi/items/15d384b2497631dc97bc\nhttps://qiita.com/taka_yayoi/items/dc96cfcfa275adf5ce8a\nhttps://nlp.johnsnowlabs.com/demo"}
{"category":"closed_qa","context":"","instruction":"1日分のデータを毎日CSVにしてS3に置いています。そのデータをまとめるDelta Tableを作って、毎日その日のデータを追加するように運用したいと思っています。このような場合、1日分のデータをMERGEするNotebookを作る以外に、StreamingやDelta Live Tableを使えばスマートに実現できますか？","response":"そのケースだとStreamingは必要ないと想定しています。一方でDelta Live Tablesを使えば、後段での処理を含めてETLパイプライン全体のデータ品質向上が期待できます（パイプラインの可視化、Expectation句によるError Handlingを活用。"}
{"category":"closed_qa","context":"","instruction":"Databricks上で自然言語処理 (NLP)をしている例を知りたい。どんな処理に使っているのか、どんなツールを使っているのか、Databricksで全部やっているのか、など。『今さら聞けないシリーズ : はじめての自然言語処理 (NLP)』を聞いたときに話が出ていたSpark NLPも気になっていて、Databricks上でのインストールの方法や使い方、日本語処理への対応など、情報があれば知りたい。","response":"参考になりそうなリンク：\n\nhttps://www.youtube.com/watch?v=3LJFDJwIxDE\nhttps://qiita.com/taka_yayoi/items/f34a5accaa8f7cad603a\nhttps://qiita.com/taka_yayoi/items/ebec25578a7cfe2f50b0\nhttps://qiita.com/taka_yayoi/items/fcf396cf75418e87ec76\nhttps://qiita.com/taka_yayoi/items/a6365e0f44d166ac343c\nhttps://qiita.com/taka_yayoi/items/35863e696c72f0d08909\nhttps://qiita.com/taka_yayoi/items/d639ec54a3fd751aad0a\nhttps://qiita.com/taka_yayoi/items/15d384b2497631dc97bc\nhttps://qiita.com/taka_yayoi/items/dc96cfcfa275adf5ce8a\nhttps://nlp.johnsnowlabs.com/demo"}
{"category":"closed_qa","context":"","instruction":"driver も worker も i3.2xlargeでクラスタを組んでいます。AWSのEC2のコンソールで見ると、ときどき、そのうちの1台が i3.4xlarge になっていることがあります。これは普通に起こることでしょうか？（SQL endpoint で起こっているのかもしれません）","response":"いえ、基本的にはあり得ません。該当のタイミングでSQL Endpointが動作していないかご確認ください。\n参考：https://docs.databricks.com/sql/admin/sql-endpoints.html#cluster-size"}
{"category":"closed_qa","context":"","instruction":"AWS上のDatabricksのクラスタから、外部（インターネット）に対して発生した通信の記録（ログ）をとりたいと思います。おそらくDatabricks自体の機能ではなく、AWSの機能を使わないといけないと思うのですが、どうやるのがお勧めですか？","response":"（これがBestなのか不明ですが、、）VPCフローログをご利用いただくのではいかがでしょうか？\nhttps://docs.aws.amazon.com/ja_jp/vpc/latest/userguide/flow-logs.html"}
{"category":"closed_qa","context":"","instruction":"定期的にCSVで受け取るデータがあります。前回からの差分ではなく、データベース全体をCSVに落としたもので、多くのレコードは前回と同じなのですが、前回以降に変更されたレコードや、新規に追加されたレコードを含んでいます。今は毎回このようなデータ全体を、Delta Tableに変換して上書きしています。今後DLTで使うことを考えると、テーブル全体を上書きするのではなく、変更分だけUPDATEあるいはINSERTするようにするのが良いのでしょうか？それとも、全体を上書きしても、DLT的にはうまく変更分を検出して動作することができるでしょうか。前者（変更分だけUPDATE/INSERTする）の場合、定石の処理パターンみたいなものはありますか？（これを使えば楽なコマンドなど）","response":"DLTの利用を考慮しない場合は、MERGE句を使っていただくのが良いです。（\nhttps://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-merge-into.html\nhttps://docs.databricks.com/delta/delta-update.html#slowly-changing-data-scd-type-2-operation-into-delta-tables\nhttps://docs.databricks.com/delta/delta-update.html#write-change-data-into-a-delta-table\n）\nただし、MERGEはDLTのStreaming Live Tableではサポートされません。この時は、APPLY CHANGES INTO句を使っていただく必要があります。（https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-cdc.html#language-sql）"}
{"category":"closed_qa","context":"","instruction":"Repos を自社の GitLab に接続して使いたいと思っています。設定方法や使い方の分かりやすい解説等があれば教えてください。（CI/CDの話に出てくるかな？）","response":"セットアップの方法：https://docs.databricks.com/repos/set-up-git-integration.html\nReposを使ったときのワークフロー：https://docs.databricks.com/repos/ci-cd-best-practices-with-repos.html\n具体的な使い方：https://docs.databricks.com/repos/work-with-notebooks-other-files.html\nhttps://docs.databricks.com/repos/sync-remote-repo.html"}
{"category":"closed_qa","context":"","instruction":"運用上クラスタを作成する権限をユーザに公開していないのですが、ジョブを作りたいユーザに対して、限定された範囲でジョブクラスタを作成できるようにする方法はありますか？","response":"Cluster Policyを設定して、該当のユーザが特定の範囲内でClusterを作成できるようにする運用でいかがでしょうか？\nhttps://docs.databricks.com/clusters/configure.html#cluster-policy-1\nhttps://docs.databricks.com/administration-guide/clusters/policies.html"}
{"category":"closed_qa","context":"","instruction":"クラスタにタグをつけておいて、タグを使って部署ごとのAWSのコストを集計できると聞きましたが、具体的なやり方が分かれば教えてください。","response":"Clusterへのタグ付けの方法：https://docs.databricks.com/clusters/configure.html#cluster-tags\n利用量データのダウンロード方法：https://docs.databricks.com/administration-guide/account-settings-e2/usage.html#usage-download\n利用量データの分析方法：https://docs.databricks.com/administration-guide/account-settings/usage-analysis.html#analyze-usage-data-in-databricks"}
{"category":"closed_qa","context":"","instruction":"トレーニングの環境設定（DA）のところでで、Hive変数（Hive Variables）というのを使っていると言っていましたが、これはどういうものでしょうか？これを使えば、Pythonで定義した変数をSQLで参照できるような気がしたのですが、そういう使い方ができるものですか？","response":"UNIXの変数と同じイメージでHiveに変数定義できるものですので、おっしゃるように使用する言語を跨いで利用できます。\n参考URL：\nhttps://sparkbyexamples.com/apache-hive/how-to-set-variables-in-hive-scripts/"}
{"category":"closed_qa","context":"","instruction":"トレーニングで、MANAGED TABLEとUNMANAGED TABLEの説明がありましたが、どのように作るとMANAGEDになるのか/UNMANAGEDになるのかが、きちんと理解できなかったので、教えてください。CREATE TABLEでLOCATIONを指定するとUNMANAGEDになるという理解で正しいですか？（UNMANAGED ＝ EXTERNAL ？）","response":"おっしゃる通りです。LOCATIONを指定するとUnmanaged(=External) tableとなります。\n参考：https://docs.databricks.com/lakehouse/data-objects.html#what-is-a-table"}
{"category":"closed_qa","context":"","instruction":"CREATE DATABASEでLOCATIONを指定して、そのデータベースに対して CREATE TABLEをしたときはMANAGEDになるという理解で正しいですか？","response":"はい、そのご理解の通りです。該当のdatabaseにおいて、TableをLocationの指定なく作成した場合には、tableはManaged tableとしてdatabase作成時に指定したLocationに保存されます。\nhttps://docs.databricks.com/lakehouse/data-objects.html#what-is-a-database"}
{"category":"closed_qa","context":"","instruction":".outputMode() の Complete mode のことを教えてください。なんとなくは分かるのですが、具体的にどういう動作になるのか理解したいです。","response":"こちらのURLに紹介している各モードの違いがわかりやすいかと思います。\nhttps://docs.databricks.com/getting-started/spark/streaming.html#structured-streaming-tutorial\nサンプルノートブックで各outputModeを切り替えて実行していただき、Tableの内容を確認するとわかりやすいと思います。"}
{"category":"closed_qa","context":"","instruction":"SQLダッシュボードの編集画面でフィルタを追加というのが増えていますが、どのように使うのでしょうか？","response":"Dashboardで呼び出しているクエリがクエリーフィルタを使用している場合には、呼び出しもとのDashboard側でフィルタ条件を設定する必要があり、その場合に使用します。"}
{"category":"closed_qa","context":"","instruction":"Pysparkでのエスケープ処理について教えてください。「あいう\"えお」のようなダブルクォーテーションが一つだけ含まれている文字列のデータがある場合、そのままCSVへ書き出し処理を行うとダブルクォーテーションのエスケープ処理がうまくいかず、データがずれた状態でCSVが書き出されてしまいます。（次のダブルクォーテーションが含まれるデータまでずれている？）PysparkのCSV書き出し時にescapeに関するoptionがありますが、上記の場合、具体的にどのように扱えばよいでしょうか？","response":"試していませんが、、、以下の記事が参照になりますでしょうか？\nhttps://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas"}
{"category":"closed_qa","context":"","instruction":"Deltaフォーマットでは、同じに列にNULLと整数がある場合、整数のまま共存できますか？以前のPandasのようにNULL（NaN）はfloat型として取り扱われ、NULLを含む列のint型はfloatに変換されていましたが、DeltaないしPysparkでも同様でしょうか？","response":"Deltaフォーマットでは、データ部分のみはParquetフォーマットを利用します。また、ParquetフォーマットではNullを含む列も任意のData Typeとして定義できるようです。\nhttps://spark.apache.org/docs/2.3.1/sql-programming-guide.html#data-types"}
{"category":"closed_qa","context":"","instruction":"ジョブ実行のスケジュールや、ダッシュボード更新のスケジュールの指定で、日中（例えば8時から18時の間）に１時間おきに実行するみたいな指定は無理でしょうか。","response":"下記でご紹介しているようにジョブの実行スケジュールはcron形式で指定することができますので、ご指定のような柔軟なスケジュール指定が可能かと思います。\nhttps://docs.databricks.com/workflows/jobs/jobs.html#schedule-a-job\n\ncron設定方法の参考：\nhttp://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html"}
{"category":"closed_qa","context":"","instruction":"Databricksのテーブル情報参照ページで、コメントより右側に列を増やして、主キーやNot Null制約などの情報を拡張して表示することはできないでしょうか。","response":"ご要望のような拡張はサポートされておりません。GUI上から制約情報などの確認を行うことはできず、DESCRIBE DETAIL もしくは SHOW TBLPROPERTIES コマンドを使っていただく必要があります。"}
{"category":"closed_qa","context":"","instruction":"CREATE TABLE や CREATE VIEW の COMMENT 文で日本語のコメントを書くと、Data Explorer で見ると文字化けします。日本語のコメントはまだ使えませんか？","response":"私が試してみたところ、テーブルおよびカラムの両方のコメント共に日本語で正常に表示されておりました。次回に詳細を伺います。"}
{"category":"closed_qa","context":"","instruction":"ノートブックでマークダウンの記述をするとき、文字の色を変更することはできますか？よくあるマークダウンのTipsで、<font color=\"red\">～～～</font>のようなHTML記述をしても変わりません。","response":"Notebookのセル内でHTML記述を行う場合には、以下のURLにご紹介しているように、displayHTMLを利用する必要があります。\nhttps://docs.databricks.com/notebooks/visualizations/html-d3-and-svg.html\n（コード自体をクラスターを実行していただく必要があります。結果のみを表示させたい場合には、セル右上のメニューから”コードを非表示”を選択してください）\n具体的なご利用例は、以下のURLにもございます。\nhttps://docs.databricks.com/_static/notebooks/dashboards.html"}
{"category":"closed_qa","context":"","instruction":"ノートブックでマークダウンの記述をするとき、表を作成した中で文字の右寄せなどをすることはできますか？表の2行目に | ---: | のような記述をしても反映されません。","response":"上記のご質問にも関連しますが、Notebookのセルでマークダウン記法を使って表現できることには限界がございます。具体的には、以下をご参照ください。\nhttps://docs.databricks.com/notebooks/notebooks-use.html#include-documentation"}
{"category":"closed_qa","context":"","instruction":"DBSQLのパラメタ指定で Date and Time Range を指定したとき、 today, yesterday, this week, last week, last month みたいな dynamic values が使えますが、この種類を自分で定義して増やすことはできませんか？例えば、Last 24 Months みたいなのを追加したいです。","response":"上記のご質問にも関連しますが、Notebookのセルでマークダウン記法を使って表現できることには限界がございます。具体的には、以下をご参照ください。\nhttps://docs.databricks.com/notebooks/notebooks-use.html#include-documentation"}
{"category":"closed_qa","context":"","instruction":"DBSQLのパラメタ指定で Date and Time Range を指定したとき、 today, yesterday, this week, last week, last month みたいな dynamic values が使えますが、この種類を自分で定義して増やすことはできませんか？例えば、Last 24 Months みたいなのを追加したいです。","response":"申し訳ありません。現状、Data and TIme Rangeで選択可能なパラメータは自動的に制御され、ユーザ定義はできないようです。プロダクト側にも要望をあげてみます。"}
{"category":"closed_qa","context":"","instruction":"BIツールを接続する場合、クラスタに接続する方法と、SQL endpoint に接続する方法があると思いますが、どのように選ぶのが良いでしょうか？ 今後 Serverless SQL が使えるようになった場合には、SQL endpoint を使うと起動が速くなりますか？","response":"外部BIツールからの接続はSQL Endpointとなります。また、今後Serverless SQLがご利用可能となった場合には、おっしゃるように起動時間の大幅な短縮が期待できます。"}
{"category":"closed_qa","context":"","instruction":"CDCとCDFはどういう関係でしょうか？ DLTで重要そうなので、詳しく知りたいと思っています。","response":"下記のオンラインドキュメントと該当ページ最下段にあるNotebookが最も具体的な理解しやすいかと思います。\nhttps://docs.databricks.com/delta/delta-change-data-feed.html"}
{"category":"closed_qa","context":"","instruction":"DLTのエラー処理の典型的なパターンのようなものはありますか？（DLTの枠組みでできること、その外でやらないといけないこと、などあるような気がしています）","response":"(DLT Jobのエラー処理について)\nDLTのProduction Modeで実施することにより、Clusterの動作が不安定になったり、起動できなかった場合に自動でリトライを行います。これは、DLTを使用して実行するJobでCluster起因によるエラーが発生した場合の対策に利用できます。\nhttps://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-concepts.html#development-and-production-modes\n\nエラー処理というよりもパイプラインのモニタリングという観点になりますが、DLTで出力されるイベントログを表示するダッシュボードをDBSQLで作成して、必要に応じてアラート設定するのがBest Practiceとなります。\nhttps://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-event-log.html#monitor-pipelines-with-the-delta-live-tables-event-log"}
{"category":"closed_qa","context":"","instruction":"【謎】 WorkspacesのタブでNotebookを見ると、自分のフォルダの下に、DBSQLのクエリがいくつか（全部ではない）見えていて、クリックするとクエリエディタが開きます。これは新しい機能でしょうか？ここにクエリが表示されるのはどんな場合でしょうか？ 見えているクエリの名前は、なぜか頭に </> がついています。","response":"下記の新機能リリースによるものです。\nhttps://docs.databricks.com/release-notes/product/2022/september.html#view-and-organize-assets-in-the-workspace-browser-across-personas"}
{"category":"closed_qa","context":"","instruction":"DBSQLのCREATE FUNCTIONで定義した関数内で、日本語の文字列を使うと文字化けするみたいです。たとえば、INTを渡して、それをSTRINGにして '円' を || でくっつけたものを値をして返すと、「円」の部分が「?」と表示されます。こういう使い方は無理でしょうか？ いまは「円」もパラメタで渡して、関数内で日本語の定数を持たないようにして回避しています。","response":"DBSQLの日本語対応はまだ部分的になっているようです。（一部の文字列がUTF-8ではなく、Latinで保存される）完全に日本語対応がされるようにプロダクト側にリクエストをあげます。"}
{"category":"closed_qa","context":"","instruction":"SQLの質問で恐縮ですが、Window関数の利用・活用方法について教えていただきたいです。","response":"以下に詳細がございますが、Tableを特定のグループに分割して、そのグループごとに計算を行う場合に特に活用可能です。\nhttps://docs.databricks.com/sql/language-manual/sql-ref-window-functions.html"}
{"category":"closed_qa","context":"","instruction":"WINDOW句やPIVOT句の使い方の解説などあれば教えてください。","response":"WINDOW句の使い方：\nhttps://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-named-window.html\nPIVOT句の使い方：\nhttps://docs.databricks.com/sql/language-manual/sql-ref-syntax-qry-select-pivot.html"}
{"category":"closed_qa","context":"","instruction":"photonは追加料金が不要になったのですか？","response":"いいえ、Photonを利用すると単位時間あたりのDBUは増えます。ただし、処理時間を短縮することにより、DBUおよびお客様のVM利用料金を削減させることを狙いとしています。料金計算方法は以下のサイトをご覧ください。\nhttps://www.databricks.com/product/aws-pricing/instance-types"}
{"category":"closed_qa","context":"","instruction":"ipywidgets や bamboolib が使えるようになったようですが、初心者向けの解説記事などはありませんか？（特に bamboolib）","response":"bamboolibの利用マニュアルを少し前に作成しましたので、共有いたします。\n\nipywidgetsのマニュアルは、以下のURLにございます。サンプルコードも同ページにございます。\nhttps://docs.databricks.com/notebooks/ipywidgets.html"}
{"category":"closed_qa","context":"","instruction":"従来のテーブルアクセスコントロールがディセーブルになっているワークスペースを、\nUnity Catalog 導入にあわせて、テーブルアクセスコントロールをイネーブルにしようと\n思っているのですが、どのような手順で進めたらよいか、ベストプラクティスを教えてください。\nいろいろなところに影響が出そうで、なかなか思い切れません。","response":"Unity Catalogのベストプラクティスをまとめて提供したいと思います、少しお待ちください。"}
{"category":"closed_qa","context":"","instruction":"やろうとしているのは、新しいランタイムで ipywidgets が使えるようになったので、\nボタンを押すと別の Notebook を実行するようなプログラムを作りたいと思い、\n以下のようなプログラムを作ってみました。\n\nimport ipywidgets as widgets \nbutton = widgets.Button(description='ボタン')\nlabel = widgets.Label(description='結果', value='no value')\n\ndef run(b):\n  try:\n    label.value = dbutils.notebook.run('remote notebook', 1000, {})\n  except Exception as e:\n    label.value = 'ERROR! ' + str(e) \nbutton.on_click(run)\nwidgets.HBox([ button, label ])\n\nこのプログラムを実行すると、dbutils.notebook.run() がエラーになるようで、label に表示させたエラーメッセージを見ると、\nERROR! Context not valid. If you are calling this outside the main thread, you must set the Notebook context via dbutils.notebook.setContext(ctx), where ctx is a value retrieved from the main thread (and the same cell) via dbutils.notebook.getContext().\nとなっています。\n\nContext というのを設定しないといけないのだと思うのですが、メッセージにある\ndbutils.notebook.getContext()　を実行すると、\nAttributeError: 'NotebookHandler' object has no attribute 'getContext'\nと言われます。\nContext というのがどのようなものかよく理解していないので、どう対処したらよいか迷っています。\nちなみに、dbutils.notebook.run('remote notebook', 1000, {})　だけセルに書き込んで実行すると、問題なく実行できます。\nもし参考になるドキュメントなどをご存じであれば、今日の夕方の打ち合わせの時にでも教えていただけませんでしょうか。","response":"https://docs.databricks.com/notebooks/ipywidgets.html#best-practices-for-using-ipywidgets-and-databricks-widgets\n上記に紹介されているように、Notebookをまたがった処理はipywidgetsではまだ実現できないようです。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogについて。現在弊社では、複数のワークスペースとAzure ADを\n連携していますが、今後Unity CatalogにアクセスするためのアカウントレベルのID\nを作成するためにAzure ADと連携する場合の手順書があれば教えてください。","response":"以下のURLに手順がございます。\nhttps://docs.databricks.com/administration-guide/users-groups/scim/aad.html#provision-identities-to-your-databricks-account-using-azure-active-directory-azure-ad"}
{"category":"closed_qa","context":"","instruction":"DLTの部分アップデートについての解説","response":"選択したテーブルからの下流のアップデートのみを実施する方法\nhttps://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-ui.html#start-a-pipeline-update-for-selected-tables\n\n処理に失敗したテーブルからアップデートを実施する方法\nhttps://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-ui.html#start-a-pipeline-update-for-failed-tables"}
{"category":"closed_qa","context":"","instruction":"ReposからGitLabを使えるようになりましたので、Reposの使い方のレクチャーをお願いできませんでしょうか。","response":"日本語ブログ（過去のDoc翻訳）：\nhttps://qiita.com/taka_yayoi/items/b89f199ff0d3a4c16140\n\n導入にあたっての設定：\nhttps://docs.databricks.com/repos/set-up-git-integration.html\nhttps://docs.databricks.com/repos/set-up-git-provider-access.html\n\nReposの基本的な操作方法：\nhttps://docs.databricks.com/repos/work-with-notebooks-other-files.html\nhttps://docs.databricks.com/repos/sync-remote-repo.html"}
{"category":"closed_qa","context":"","instruction":"CI/CDのことを教えてほしい。","response":"Reposを使ったCI/CDの実現について：\nhttps://docs.databricks.com/repos/ci-cd-best-practices-with-repos.html\n\nまた、以下のBest Practiceも参考になると思います。\nhttps://docs.databricks.com/notebooks/best-practices.html"}
{"category":"closed_qa","context":"","instruction":"notebookでのSQLやpythonコードの入力のコツがあれば教えてください(キーワードの補完など)","response":"こちらに説明があるオートコンプリートの機能をご利用ください\nhttps://docs.databricks.com/notebooks/notebooks-use.html#autocomplete\n\n別の開発環境(IDE)からの接続方法をご案内：\nhttps://docs.databricks.com/getting-started/connect/index.html#ides-and-other-developer-tools"}
{"category":"closed_qa","context":"","instruction":"Databricks-Labelbox連携について教えてほしい。","response":"連携方法は、以下に情報がござます。（Partner Connectを利用する方が簡単）\nhttps://docs.databricks.com/partners/ml/labelbox.html#connect-to-labelbox\n\nまた、こちらに連携のチュートリアルのNotebookもございます。\nhttps://github.com/Labelbox/labelbox-python/blob/develop/examples/integrations/databricks/readme.md"}
{"category":"closed_qa","context":"","instruction":"Databricks supportより↓の”Improve the security of your DBFS Root S3 bucket and KMS key(s) for E2 workspaces”というメールが届きました。これについて教えてください。\n\n＝＝\nWe are reaching out to notify you of a recently introduced mitigation to the confused deputy problem impacting AWS cloud resources. Please follow the instructions below to improve the security of your DBFS Root S3 bucket and KMS key(s) for E2 workspaces and help address the confused deputy problem.\n\nAction Required\n\nGet a list of S3 buckets and KMS keys used for your Databricks account. They can be accessed from the account console (click the ‘Cloud resources’ icon in the left bar) or by using the Account API.\nConfirm if the existing clusters in all your workspaces have been restarted after 2022-04-11. If not, please restart the clusters before moving to the next step.\nGet your databricks-account-id. Go to the account console and click the in the bottom left corner. Under Account ID, select and copy the ID.\nUpdate the IAM permissions associated with the S3 buckets and KMS keys in AWS console by adding the following condition to your resource policy:\n\"\"Condition\"\": { \"\"StringEquals\"\": { \"\"aws:PrincipalTag/DatabricksAccountId\"\": [\"\"(s)\"\"] } }\n\nBackground\nToday, when customers set up AWS S3 storage bucket and KMS key (used for Customer Managed Key feature) for their Databricks E2 workspaces, they are required to grant Databricks IAM permissions to access their S3 bucket and KMS key. When customers set up these IAM roles, these permissions missed the condition that the S3 bucket and KMS key can only be used within that customer’s Databricks account. This could lead to the confused deputy issue, where your S3 bucket or KMS key can be used by other Databricks customers in their Databricks account. This could lead to a user outside your organization, with knowledge of your Amazon Resource Names, reading/writing data to the specific workspace directory under your S3 bucket. This issue would not provide them access to your existing data.\n\nFor questions related to this issue, please contact Databricks support at help@databricks.com.","response":"社内で確認したところ、以下のAWSのガイダンスに基づく修正で、Customer Managed Keyを利用しているか否かに関わらず、推奨させていただいている事項になるとのことです。\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html\n\n以下のオンラインドキュメントにあるワークスペース作成時のバケットボリシーもお知らせにあるような内容に変更されています。\nhttps://docs.databricks.com/administration-guide/account-api/aws-storage.html#step-2-apply-bucket-policy-workspace-creation-only\n\n以下、Bucket PolicyのJSON定義を抜粋。太字の部分が追加されています。\n{\n  \"\"Version\"\": \"\"2012-10-17\"\",\n  \"\"Statement\"\": [\n    {\n      \"\"Sid\"\": \"\"Grant Databricks Access\"\",\n      \"\"Effect\"\": \"\"Allow\"\",\n      \"\"Principal\"\": {\n        \"\"AWS\"\": \"\"arn:aws:iam::414351767826:root\"\"\n      },\n      \"\"Action\"\": [\n        \"\"s3:GetObject\"\",\n        \"\"s3:GetObjectVersion\"\",\n        \"\"s3:PutObject\"\",\n        \"\"s3:DeleteObject\"\",\n        \"\"s3:ListBucket\"\",\n        \"\"s3:GetBucketLocation\"\"\n      ],\n      \"\"Resource\"\": [\n        \"\"arn:aws:s3:::<s3-bucket-name>/*\"\",\n        \"\"arn:aws:s3:::<s3-bucket-name>\"\"\n      ],\n      \"\"Condition\"\": {\n         \"\"StringEquals\"\": {\n            \"\"aws:PrincipalTag/DatabricksAccountId\"\": [\"\"YOUR_DATABRICKS_ACCOUNT_ID\"\"]\n         }\n      }\n    }\n  ]\n}"}
{"category":"closed_qa","context":"","instruction":"Single User / Shared Clusterのご説明","response":"Cluster Access Modeについて以下の情報をご参照ください。\nhttps://docs.databricks.com/data-governance/unity-catalog/compute.html#what-is-cluster-access-mode"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogへの移行について。レガシーワークスペースにおいてUnity Catalogを有効化する際、既存テーブルもhive_metastoreカタログ経由でそのままアクセスできる（対応不要）という理解ですが、有効化した際に動かなくなる可能性のあるもの（有効化する前にやっておかないといけないこと）はありますか？ （IdP連携のDatabricksアカウントレベルへの移行は有効化前に実施する前提です。）","response":"hive_metastoreでカタログ管理されているテーブルはそのまま利用可能ですが、いくつか注意事項がございます。\n・hive_metastoreがワークスペース レベルのオブジェクトであるため、hive_metastoreカタログ内で定義された権限は、常にワークスペースのローカル ユーザーとグループに対して適用されます\n・上記に対して、Unity Catalogのアクセス制御ポリシーはアカウントグループに適用されます。（アカウントグループとワークスペースローカルグループの違いは以下の通りです）\nアカウントグループは、アカウント管理者のみが作成できます。アカウント グループは、IDフェデレーションを実施したワークスペースに割り当て可能で、アカウント管理者とワークスペース 管理者の両方がワークスペースへの割り当てを実施可能です。\nワークスペース ローカル グループを作成できるのは、ワークスペース管理者のみです。これらのグループは、ワークスペース管理コンソールおよびアカウント コンソールの [ワークスペース のアクセス権] タブで workspace-local として識別されます。\n\n\n（詳細は、以下のURLもご参照ください）\nhttps://docs.databricks.com/data-governance/unity-catalog/hive-metastore.html\nhttps://docs.databricks.com/administration-guide/users-groups/index.html#special-groups"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogへの移行について。HiveメタストアからExternalテーブルをUnityカタログメタストアにアップグレードするウィザードの使い方を教えてください。","response":"https://docs.databricks.com/data-governance/unity-catalog/migrate.html#upgrade-tables-and-views-to-unity-catalog\n上記URLに説明がございます。\nデモもお見せする予定。"}
{"category":"closed_qa","context":"","instruction":"最近アナウンスされた Serverless SQL と Databricks SQL Pro について教えて下さい。\n　Serverless SQL はもう東京リージョンで使えますか？\n　SQL Pro はどんなものですか？ Serverless SQLと同じケーパビリティだと書いてありますが、Serverless との違いは？","response":"従来のDatabricks SQLとSQL Pro及びServerless SQLの機能比較は以下のURLにございます。\nhttps://docs.databricks.com/sql/admin/warehouse-type.html#comparing-features-in-serverless-pro-and-classic\n\nまた価格の違いについては、以下のURLをご参照ください。\nhttps://www.databricks.com/product/sql-pricing-aws\n\nSQL ProはSeverless SQLと違って、お客様VPC上にSQL用のClusterを作成するため、Cluster作成から起動までの時間は従来のDatabricks SQLと変わりがありません。"}
{"category":"closed_qa","context":"","instruction":"SAT（Security Analysis Tool）はもう使えますか？","response":"はい、Githubにある手順で設定が必要ですが、ご利用可能です。\nhttps://github.com/databricks-industry-solutions/security-analysis-tool/blob/main/docs/setup.md"}
{"category":"closed_qa","context":"","instruction":"行、列レベルのアクセス制御の例と方式","response":"こちらに案内しているDynamic ViewのColumn Level PermissionsおよびRow Level Permissionsをご参照ください。\nhttps://docs.databricks.com/security/access-control/table-acls/object-privileges.html#dynamic-view-functions"}
{"category":"closed_qa","context":"","instruction":"クラスタが知らない間に消えてしまいました。作成権限なくても消せたりするのでしょうか。","response":"Clusterに対するCan Manage権限がなければユーザ操作で削除することはできません。\n一方で下記で説明がございますように、30日間利用されていないClusterは自動的に削除されてしまいます。これを防ぐためにはClusterをPinしていただくようお願いします。\nhttps://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster"}
{"category":"closed_qa","context":"","instruction":"これから開始するETLのDLT化に関連して：\n1.  構造的ストリーミングと、Delta Live Tableの仕組みと、CDC(CDF)の関係が理解しきれていないので、3者の関係を教えてもらえませんか","response":"＃　ソースがStreamingになる。なので、CDF/CDCで処理したい。ここでDLT使うと運用便利そう。\n＃　DLTで上記要素技術をどこまでカバーしているか？\nソースからCDFが来るかどうかがPoint\nDLTでCDC/CDFが実現される範囲。どんな時にApply changes使うか？でソース用意する。\n\nDLT利用時にCDCを実現する際には、ターゲットテーブルに対して、Apply Changesを使用する必要があります。（https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html#example-queries）\n\n一方でDLTを利用せずにCDCを実現する際には、元テーブル(もしくはView）の最新バージョンを取得するViewを作成して、そのViewをもとにターゲットに対して、MERGE INTOを使用する必要があり、DLTを使用する場合と比較して、煩雑なバージョン指定等の考慮が必要になります。\n（https://docs.databricks.com/delta/delta-change-data-feed.html#change-data-feed-notebook　のNotebookの”Propagate changes from silver to gold table”以降の二つのセルを参照ください。）"}
{"category":"closed_qa","context":"","instruction":"Unity Catalog関連：IdP連携について、基本的にはIdPでグループ管理にも対応できてないとまずいか。","response":"idpでユーザ・グループを集中管理し、それを各システム（今回はDatabricks）に連携するのがBest Practiceだと思っていますが、グループはDatabricksだけで独自管理でも運用はできると思っています。その際はidpから連携されたユーザをdatabricks account consoleでグループに追加する作業が必要になると思っています。"}
{"category":"closed_qa","context":"","instruction":"Personal Compute Default Policyについて教えてください。","response":"Personal ComputeのデフォルトのCluster Policyはこちらのページが参考にあると思います。\nhttps://qiita.com/taka_yayoi/items/0234a88ec5a73afb0fb4#%E7%AE%A1%E7%90%86%E8%80%85%E3%81%A8%E3%81%97%E3%81%A6%E3%83%91%E3%83%BC%E3%82%BD%E3%83%8A%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%83%88%E3%82%92%E7%AE%A1%E7%90%86%E3%81%99%E3%82%8B\n\n既存のCluster Policyの設定変更例はこちらにございます。\nhttps://qiita.com/taka_yayoi/items/199cc6574ab0782d5d55#%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%B9%E3%83%9A%E3%83%BC%E3%82%B9%E3%81%AE%E8%A8%AD%E5%AE%9A\n\n無効化手順\nhttps://qiita.com/taka_yayoi/items/199cc6574ab0782d5d55#%E3%82%A2%E3%82%AB%E3%82%A6%E3%83%B3%E3%83%88%E3%82%B3%E3%83%B3%E3%82%BD%E3%83%BC%E3%83%AB%E3%81%AE%E8%A8%AD%E5%AE%9A"}
{"category":"closed_qa","context":"","instruction":"テーブルの中で複雑な構造のデータ(struct,json等)を扱っている時に、それらの差分などをわかりやすく見たいケースが多い。いい方法はありませんか？","response":"以下、二つの情報（Semi structured Dataの扱い方、Nested JsonのFlatten化方法）が参考になりそうです。\nhttps://docs.databricks.com/optimizations/semi-structured.html\nhttps://docs.databricks.com/_static/notebooks/kb/scala/nested-json-to-dataframe.html"}
{"category":"closed_qa","context":"","instruction":"ワークフロー時に、後続のタスクに情報を引き継ぐ方法などありますか？\n意図としては先頭でパラメタ管理などを１つのノートブックで行い、\n後続のタスクはそのパラメタ管理ノートブックを参考に、変数情報を扱う、\nみたいなことがしたいです。","response":"タスク間の変数の受け渡しには、こちらの機能が使えると思います。お試しください。\nhttps://docs.databricks.com/workflows/jobs/jobs.html#task-parameter-variables"}
{"category":"closed_qa","context":"","instruction":"実行した人が自身のユーザ名(~.co.jp)を取得するコマンドはありますでしょうか？","response":"クエリーを実行したユーザ名はAuditログから確認可能ですが、Notebook上のコマンドから取得することはできません。\n\n下記のように、dbutilsのコマンドを組み合わせて取得可能です。\nusername = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')"}
{"category":"closed_qa","context":"","instruction":"ワークフローについて質問です。ワークフロー時に枝分かれしているフローがある際に、\n何かの設定で片方だけ実行させる、みたいな設定はできるのでしょうか？\n意図としてはタスクA の後続に枝分かれでタスクB、タスクCがあるときに、\nパラメタ設定などでタスクCだけのフローにする、みたいなことです。","response":"Private Previewにて該当する機能をご利用可能です。Google Formでの申し込みとなるため、弊社側で代理記入をして良いか相談させてください。"}
{"category":"closed_qa","context":"","instruction":"Repos(GitLab)の使い方で、複数の個人が開発用に作業をするリポジトリと、\n運用するNotebookを置くリポジトリを、どのように管理するのが良いか、\nお勧めの使い方があれば知りたい。","response":"通常のCICDのフローに則るのが良いと考えます。\n・運用に使用するNotebookをGitlab上に配置\n・各開発者は該当のNotebookをCloneし、運用本番用のNotebookに反映させたい場合には、コミット・プッシュを行う。\n・承認者は必要に応じてマスターブランチへの反映を行う\n以下のページも参考になると思いますのでご覧ください。\nhttps://qiita.com/taka_yayoi/items/b89f199ff0d3a4c16140"}
{"category":"closed_qa","context":"","instruction":"DatabricksのVPCから、別のVPCにあるRDBをアクセスする場合、\nVPC Peering と Transit Gateway あたりを使えないか検討しているのですが、\nそれぞれのメリット／デメリットみたいなのはありますか？\nほかに何か良いやり方はありますか？","response":"VPCペアリングは１:１の紐付けになる。一方でTransit GatewayはHub &Spoke型で接続できる。\n今後複数のRDBアクセスを追加していく場合には、Transit Gatewayの方が設定が楽である。\nTransit Gatewayはコストが高い。"}
{"category":"closed_qa","context":"","instruction":"現在計画中のKinesis連携について、少しご相談させてください。","response":"・Kinesisの共有ファンアウト、拡張ファンアウト両方にdatabricksは対応している。今回取り込むレベルのデータ量だと共有ファンアウトで十分捌けそう。\n・クラスターのオートスケールアウトは自動で実施されるが、スケールダウンは自動では実施されず、その場合はJobを再起動する必要がある。\n・DLTを使えば、スケールアウト・ダウン共に自動的に実施可能。\n・今回のデータ量であれば、Worker Node２台の構成でもスケールアウトの必要はなさそう（ただし、検証は必要）\n・受信済みデータの削除については、受信済みデータをBronzeテーブルとして保存して、該当のBronzeテーブルに対する更新・削除Operationを行うのがおすすめ。\n・ワークスペース１と２でAWSアカウントが違っていても大丈夫。\n\n・Kinesis連携のサンプルコードをDBXから提供する。"}
{"category":"closed_qa","context":"","instruction":"毎朝２時に動くようにスケジュールしているジョブがあります。このジョブは外部から S3 経由でデータを受け取って Delta Table に追加します。朝 ２時にジョブを開始したときに、万一 S3 へのデータがまだ到着していなかった場合に、自分のジョブを 2時30分にもう一度実行するようにスケジュールを登録したいと考えています。良い方法はありませんか？","response":"外部のワークフロー管理ツールと連携しない場合は、Workflow内のTaskに対して、Retry Policyを設定するのが良いと思います。（画面イメージはE列参照）\nTaskに対するRetry Policyの設定はAdvanced Optionで設定可能で、Retry回数とRetry時の時間間隔を設定可能です。\n\n外部のワークフロー管理ツールを使用する場合には、Jobの失敗を外部ツールにWebhookで通知して、その結果を受けて、該当Jobを起動するJobAPIを叩く方法が良いと思います。\n\n案1. Job failureさせるコード（対象ファイルがない場合）\nPython notebook であれば以下のようなコードで例外をスローして、NotebookのJobをFailさせることができます。\n\nif condition: \n  raise Exception(\"\"Custom message\"\")\n\n案2. EC2でのファイル格納処理と連携して、JobAPI叩く"}
{"category":"closed_qa","context":"","instruction":"DBSQLのダッシュボードにある「Schedule」と、クエリにある「Schedule」は、どのような関係になりますか？（どう使い分けるものなのか？）","response":"DBSQLのダッシュボードにあるScheduleはダッシュボードのスケジュール更新に利用します。\n参考：この時、ダッシュボードの共有設定で、実行時の権限\"\"Query execution credentials\"\"がRun as Owner（デフォルト設定）になっている場合には、所有者のCredentialでダッシュボードのクエリーが実行され、結果が表示されます。（画面イメージはE列参照）\nhttps://docs.databricks.com/security/access-control/dashboard-acl.html#sharing-settings\n\nクエリのSchedulerは該当のクエリのみがスケジュール更新されます。ただ、現状クエリーエディターのUI上で最新の実行時刻が反映されないバグがあります。（クエリ自体は正しく実行され、結果が反映されています）また、SQL Warehouseのクエリーヒストリーからも正しく実行されていることを確認することができます。"}
{"category":"closed_qa","context":"","instruction":"Notebookの「Schedule」と、ワークフローの「Schedule」は、どのような関係になりますか？","response":"NotebookのScheduleはNotebookで定義された処理を定期実行します。これはワークフローのTaskをTypeをNotebookとして実行した時と同じことになります。（入口が違う）"}
{"category":"closed_qa","context":"","instruction":"Shuffle Partition のことを教えてほしい。Notebookを実行したときにヒントが出てきて気になっている。","response":"https://www.databricks.com/blog/2020/10/21/faster-sql-adaptive-query-execution-in-databricks.html\n上記のBlogが参考になりそう。以下、追加説明\n- 日本語のBest Practice資料 : Databricks Platformベストプラクティスの紹介\n- Join Hints strategyの資料 : Join Optimization Hints.pdf\n- Cluster Sizingに関するオンラインドキュメント https://docs.databricks.com/clusters/cluster-config-best-practices.html#cluster-sizing-considerations"}
{"category":"closed_qa","context":"","instruction":"ジョブクラスタの作り方に関して、以下のような認識で合っていますか？\n\n1.     ジョブクラスタは Workflowの画面でジョブを定義するときに New Job Cluster で作成するか、既存のジョブクラスタを選択する。\n\n2.     ジョブ定義の画面で既存のオールパーパスクラスタを選ぶと、そのジョブはオールパーパスクラスタで動くので、オールパーパスクラスタの課金になる。","response":"はい、ご記載の認識であっています。"}
{"category":"closed_qa","context":"","instruction":"ジョブの定義時に既存のジョブクラスタを選択した場合、同じ時間に同じジョブクラスタを使うジョブがスケジュールされると、どのように実行されますか？","response":"Workflow内のTask間でジョブクラスターを共有することは可能です。https://docs.databricks.com/workflows/jobs/jobs.html#use-shared-job-clusters\n基本的に複数のジョブでジョブクラスターを共有する使い方は推奨しておりません。（処理時間が期待値と大きく外れてしまう要因となりうるため）\n\n一方で異なるWorkflow間でジョブクラスターを共通的に利用することはできません。"}
{"category":"closed_qa","context":"","instruction":"  複数のユーザがジョブを作ってスケジュールするようになってきています。ジョブクラスタの作り方や運用のしかたの、 ベストプラクティス みたいなものがあれば知りたい。","response":"ジョブクラスターの構成の方法のベストプラクティスが以下にございます。\nhttps://docs.databricks.com/workflows/jobs/jobs.html#best-practices\n\nジョブクラスターのサイジングについては、All purpose Clusterで検証を実施し、スループットを確認した後に同等のクラスターをジョブクラスタとして構成することをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Pools をうまく使って効率良いクラスタの使い方ができないかと考えているのですが、どういうケースで Pools のメリットが出てきますか？","response":"クラスターの起動を早めたい時に利用されますが、一方でクラウドサービスプロバイダーからのインスタンス利用料金は発生しますので、起動時間とインスタンス利用料金のトレードオフになります。\nhttps://docs.databricks.com/clusters/instance-pools/index.html"}
{"category":"closed_qa","context":"","instruction":"ジョブクラスタのサイズをどのように決めるか、試行錯誤しています。良いやり方があれば知りたい。チューニングに使える情報（CPUの負荷、メモリの使用量、ネットワークの転送量、ストレージのアクセス、など）を知る方法がありますか？","response":"サイジング自体は、All purpose Clusterと同じ。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tableで管理しているテーブルで、更新されたレコード数が表示されるものと表示されないものがある。APPLY CHANGES INTOで生成(更新)したテーブルは表示されなかった。(基本的なことを勘違いしているかもしれませんが) 表示されるものとされないもの、基準があれば教えてください。","response":"APPLY CHANGES INTOのターゲットテーブルには、現状、以下の制約事項がございます。\n(https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html#create-a-target-table-for-output-records)\n\nYou must ensure that a target table is created before you execute the APPLY CHANGES INTO query or apply_changes function. See the example queries.\n\nMetrics for the target table, such as number of output rows, are not available.\n\nSCD type 2 updates will add a history row for every input row, even if no columns have changed.\n\nThe target of the APPLY CHANGES INTO query or apply_changes function cannot be used as a source for a streaming live table. A table that reads from the target of an APPLY CHANGES INTO query or apply_changes function must be a live table.\n\nExpectations are not supported in an APPLY CHANGES INTO query or apply_changes() function. To use expectations for the source or target dataset:\n\nAdd expectations on source data by defining an intermediate table with the required expectations and use this dataset as the source for the target table.\n\nAdd expectations on target data with a downstream table that reads input data from the target table."}
{"category":"closed_qa","context":"","instruction":"Collibraのサイトに、Unity Catalog周りの連携をしていると書いてあったが、どういうすみ分けなのか、どう連携しているのか教えてほしい。https://www.collibra.com/us/en/blog/leveling-up-recent-developments-between-databricks-and-collibra","response":"Databircks Unity CatalogのAPIをCollibra側がたたいて、Unity Catalogで定義されているデータカタログ・リネージュの情報を取得し、Callibra側で統合的に表示できるようです。今後Unity CatalogのAPI拡充に合わせて、連携を強化するようです。"}
{"category":"closed_qa","context":"","instruction":"DBXのセキュリティについて","response":"https://www.databricks.com/wp-content/uploads/2022/02/databricks-security-and-trust-overview-whitepaper.pdf"}
{"category":"closed_qa","context":"","instruction":"Kinesisでストリームを読み込むNotebookを作るときに、メンテナンスで停止したり、クラスタのアップグレードのため止めたりすることが必要になると思うのですが、どのようにNotebookを作りこんだらよいでしょうか？","response":"Delta Live Tables(DLT)の利用をおすすめしたい。\nDLTの実行自体をJobとして実施する。このときに通常のJobと同じ考慮（Retry設定等）を行う。\nどこまでStreaming処理が完了したかは、Check Point（DBX内の構成要素としてはRocksDB:意識する必要はない）で保存・管理されている。どのStreaming実装を使う場合においても、Check Pointを意識する必要がある。\n\n構成Option：\n1-1) Kinesis -> Databricks Structured streaming -> Save as Delta Table （ここが静止点）\n1-2) Kinesis -> DLT -> Save as Delta Table （ここが静止点）\n2) Kinesis -> (firehose) -> S3（ここが静止点） -> Autoloader -> DLT\n検討ポイントは、a)リアルタイム要件とb)障害発生（想定しない停止）時のデータ品質要件\n1のOptionの場合は、リアルタイム要件○、耐障害要件▲\n2のOptionの場合は、上記と逆（リアルタイム要件はFirehoseマター）\n\n参考：\n想定される障害パターン”Kinesis側の障害（インフラレイヤ）”、”Databricks側の障害（インフラレイヤ）”、”ストリームデータの異常”\nデータのValidationについては、DLTのExpectations/Constrainの活用が有効\n\n参考:\nCheckPointの方式は以下2 Optionある。同期処理の場合はMicro Batch処理と同期的にCheck Pointへの書き込みが同期させる。\n1) 特定DirectoryにファイルでCheckPointを管理：同期 - - - バッチの間隔が長くなるため、1Batchあたりのデータ量が増える。急激な流量の変化に弱い。\n2) RocksDB : 非同期 - - - 実際の書き込み処理とCheck Pointで管理されている情報にずれが生じるためリスク"}
{"category":"closed_qa","context":"","instruction":"いまはNotebookを作った人が自分でジョブを登録して実行するような運用をしています。しかし、これだと個人にバインドしてしまうので、たとえば、共通で使うようなジョブに関して、専用のアカウントを作って実行するようなことは現実的でしょうか？なにか良い運用のノウハウがあれば教えてください。","response":"Gitと連携したCI/CDを将来的には検討。\n\nJob自体が個人にバインドされることによるRiskを排除したい。以下の方針がある。\n1) Workflow自体をIaaC的に実装および管理する。例えば、Terraformで管理。このときにService Principleのトークンを利用してAPIをたたくことを推奨しています。\n2) Job as Service（現状はPrivate Preview）を利用して、Jobの定義自体を共通部品として管理する。"}
{"category":"closed_qa","context":"","instruction":"SQLで約８０億レコードのテーブルと、７５万レコードのテーブルを結合していますが、速度がかなり遅い状況です。zorderでインデックスをはる以外で何か良い解決方法はありますでしょうか。また結合条件が特殊で、相互のテーブルでカラムAがNullであればカラムBで結合するような条件となっています。この場合でもインデックスは効果がありますでしょうか。","response":"現状、どのくらい時間がかかっているか？\nZorder以外に何か実施しているか？（PhotonやDBR Versionの確認）\nBHJが有効かもしれない\n※：上記事象自体は解消した\nーー\n\n・Zorderでカラム定義を更新する方法\nhttps://qiita.com/taka_yayoi/items/3d41a022c0d81b78afaf\n\n・複数カラムの定義方法\n以下のようにカンマ区切りで定義対象のカラムを指定します\nOPTIMIZE table_name [WHERE predicate]\n  [ZORDER BY (col_name1 [, ...] ) ]\n\n・Bloomfilter Indexのご紹介\nhttps://docs.databricks.com/optimizations/bloom-filters.html#bloom-filter-indexes"}
{"category":"closed_qa","context":"","instruction":"Notebook側のDataメニューの動きが変更された？\n\n1.     いままでは表示する／しないをトグルで切り替えられたが、ページ遷移してしまうようになった。（SQLDB側と似た動き）\n\n2.     クラスタではなくSQL warehouseにバインドされるようになって、クラスタのアクセスコントロールがディセーブルになっている場合に、見えるものが違ってくる。","response":"Data Explorerは、すべてのペルソナで同じユーザーエクスペリエンスが得られるように、画面が統合されました。\n\nこれにともなって、ご指摘のようにSQL Warehouseに紐づいて、Data Explorerの制御がされますのでご注意ください。"}
{"category":"closed_qa","context":"","instruction":"DBSQLではpythonで書いたUDFを使えるようになったと思いますが、Notebook側のSQLでpythonのUDFを使う方法はありますか？ @udf でpythonのUDFを定義した場合、pysparkからは呼び出せますが、SQLからは呼び出せません。\n\n1.     具体的には、unicodeのnormalizeを行いたいのですが、SQLにはその関数が見つからないので、pythonの関数を作ってunicodedata.normalize()を呼び出そうと思っています。","response":"こちらのDocumentをご参照ください。\nhttps://docs.databricks.com/udf/python.html#call-the-udf-in-spark-sql"}
{"category":"closed_qa","context":"","instruction":"サービスプリンシパルは、どのようなケースで使うものですか？","response":"サービス プリンシパルは、自動化ツール/ジョブ/アプリケーションで使用するために作成する ID です。サービス プリンシパルは、外部ツールから Databricks リソース（ワークスペースやクラスター）への API のみのアクセスを提供し、ユーザーまたはグループを使用するよりも優れたセキュリティを提供します。また、ユーザーが組織を離れたり、グループが変更されたりした場合に、外部からのジョブや自動化スクリプト起動が失敗するのを防ぎます。\n個人用アクセス トークンの代わりに、サービス プリンシパルとそのアクセス トークンを使用して、外部の自動化ツールとシステムに Databricks リソースへのアクセス権を付与することをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Snorkel AIについて。Databricksと連携しているようなことがあれば教えてください。","response":"具体的な連携例はまだ聞いたことがありませんが、Snorkelの2022のイベントでDatabricksのCTOであるMateiが、SnorkelとMlflowとの連携をシンプルに行えるようにしたいとコメントしておりました。\n（2022/10くらいから具体的なIntegrationに向けた議論がスタートしているようです）"}
{"category":"closed_qa","context":"","instruction":"Ray が Databricks でも動くという記事を見ました。詳しい話を知りたいです。今後、Databricks 上でサポートされて行きますか？","response":"おそらく、以下のBlogをご覧になったかと思います。\nhttps://qiita.com/taka_yayoi/items/d0c2f12673de2ddbe42e\nhttps://qiita.com/taka_yayoi/items/f919ad6aee56539f5309\n\n以下のリリースノートにあるように、現状はPublic Previewですが今後正式サポートされると想定しています。\nhttps://docs.databricks.com/release-notes/product/2023/february.html#ray-on-databricks-public-preview\n\n具体的な利用例として、過去のData ＆ AI summitで以下の事例がございます。\nhttps://www.databricks.com/session_na21/manipulating-geospatial-data-at-massive-scale\n\n\n補足：\n12.0以降の最新のDBRは12.2LTSとなります。\nRayとSparkの分散処理の考え方・区分の違いについて、以下の情報がございます。\nhttps://www.databricks.com/session_eu20/ray-and-its-growing-ecosystem"}
{"category":"closed_qa","context":"","instruction":"Habu（データクリーンムーム）とDatabricksの連携について教えていただけますでしょうか？","response":"昨年11月末にPartnershipを結んで、ハイレベルの会話を始めたばかりの状況です。\nDatabricksとしては、クリーンルームソリューション企業との統合を示す必要がある一方で、HabuはSnowFlakeから戦略的な投資を受けているという受け止め方もございます。\n今後、共同のGo to market戦略を相談する予定のようです。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tables で、毎日テーブル全体がParquetファイルで送られてくるのをautoloaderで読んで、そのデータでDelta Table全体を上書きしたいのですが、どのように書けばよいでしょうか？ dlt.apply_changes で apply_as_truncate を使えばできそうな気がしていますが、どのように条件を書けばよいのか分からず。もっとシンプルな方法がある気もしています。","response":"Change data feed demo\nhttps://docs.databricks.com/delta/delta-change-data-feed.html#enable-change-data-feed\nhttps://docs.databricks.com/_extras/notebooks/source/delta/cdf-demo.html\n\n自前でCDC情報を作成するか、DebeziumもしくはAWS DMSなどのCDC情報作成ツールを使うか？\nCDC情報作成ツールから出力された情報をAuto Loaderで読み込む場合は、DLTのパイプラインを作りやすい。\n\nDelta Live Tablesによるチェンジデータキャプチャ\nhttps://qiita.com/taka_yayoi/items/5319b9b010ce4703e98a\n\n上記のサンプルコードとブログをベースに説明いたします。"}
{"category":"closed_qa","context":"","instruction":"DLTでapply_changesを実行したときに、スキーマには「ターゲットテーブル名」と「__apply_changes_storage_ターゲットテーブル名」という２つのテーブルが登録されます。「__apply_changes_storage_ターゲットテーブル名」にはロケーションがついていて、「ターゲットテーブル名」は「__apply_changes_storage_ターゲットテーブル名」に対するビューになっています。これはこういうものですか？「__apply_changes_storage_ターゲットテーブル名」というテーブルが見えないほうが気持ち良いのですが、何か方法はありますか？","response":"https://docs.databricks.com/delta-live-tables/cdc.html#what-data-objects-are-used-for-delta-live-tables-cdc-processing\n上記に、ご案内しているように仕様通りの動作となります。\n（DLTにおけるターゲットテーブルでは、ターゲットテーブルの名称のView、CDC処理を管理するためにDLTによって使用する内部的なテーブル\"\"__apply_changes_storage_TARGET_TABLE_NAME\"\"が作成されます）"}
{"category":"closed_qa","context":"","instruction":"ストリーミングテーブルとマテリアライズドビューの違い／使い分けについて教えてください。pythonではどちらもdlt.table()で宣言するので混乱しています。dlt.read()で読むのとdlt.read_stream()で読むのの違い？","response":"https://docs.databricks.com/delta-live-tables/transform.html#when-to-use-views-materialized-views-and-streaming-tables\n上記URLにご案内があります。\ndlt.read()で読んだ場合はマテリアライズドビュー、dlt.read_stream()で読んだ場合には、ストリーミングテーブルとなります。"}
{"category":"closed_qa","context":"","instruction":"temporaryのdlt.table()とdlt.view()の違い／使い分けを教えてください。（次の質問とも関係しています。）","response":"使い分けについては、上記項番126をご参照ください。"}
{"category":"closed_qa","context":"","instruction":"上と関連するのですが、autoloaderで読み込んだテーブルをソーステーブルとしてapply_changes()をやりたいのですが、そのソーステーブルは、dlt.table()で宣言するのか、dlt.view()で宣言するのか、どちらが良いのか迷っています。また、同じソーステーブルを使ってapply_changes()で更新するターゲットテーブルが１つの場合と、２つ以上ある場合で異なりますか？","response":"ソーステーブルの利用先（ターゲットテーブル）が複数ある場合には、dlt.table()で宣言することをお勧めしています。\n理由は項番126のURLをご参照ください。\nAuto Loaderで外部ファイルを読み込む時の静止点構築という観点から、Autoloaderのターゲットテーブルはdlt.table()で宣言することをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"あるワークスペースのDLTジョブ（ワークフローと呼ぶのが正しい？）でS3上に作成したテーブルを、同じワークスペースの別のDLT DLTジョブのなかでライブテーブルとして参照することはできますか？ 可能な場合、読み出し方を知りたいです。 同様に、別のワークスペースのDLTジョブからもライブテーブルとして参照できますか？","response":"同一のワークスペースであれば、他のDLTパイプラインで作成されたテーブルを参照可能です。他のワークスペースのDLTパイプラインのテーブルは参照不可です。（現状、DLTパイプラインから生成するテーブル情報はワークスペースレベルのHive メタストアに格納されるため）\n他のパイプライン等から参照させる場合には、DLTパイプラインの設定画面でTarget schemaを設定し、呼び出し元では、Target schema.テーブル名で呼び出します。\nhttps://docs.databricks.com/delta-live-tables/publish.html#how-to-publish-delta-live-tables-datasets-to-a-schema"}
{"category":"closed_qa","context":"","instruction":"DLTのフローグラフの表示で、dlt.tableで作ったテーブルには書き込まれたり削除されたレコード数が表示されますが、dlt.apply_changesで更新したテーブルには表示されません。同様の情報を簡単に知る方法はありませんか？","response":"現状、Apply changes Intoのターゲットテーブルについては、メトリクス表示はサポートされておりません。\n\n''__apply_changes_storage.テーブル名”のテーブルには、タイムスタンプや削除、追加のバージョン情報があるため、このテーブルに対してクエリーを発行すれば同等の情報が得られそうです。"}
{"category":"closed_qa","context":"","instruction":"pysparkで書いたDLTのNotebookの中で、javaで書いたUDFを呼び出すことはできますか？ 64ビットの整数値のビットリバースを計算したいのですが、Pythonだと書きにくいので、javaで書けないかと考えています。","response":"試していないが、以下の案内のとおり、JavaでUDF定義して呼び出せそうです。\nhttps://docs.databricks.com/sql/language-manual/sql-ref-functions-udf-scalar.html#language-java"}
{"category":"closed_qa","context":"","instruction":"日本語環境でジョブの成功のアイコンが赤いバツ印になっている。英語だと正しくチェックになる。バグかな？","response":"申し訳ございません、おっしゃるとおりになってしまっています。\n社内の対応状況を確認したところ、先週この問題が発見され、2週間くらいで修正予定とのことです。"}
{"category":"closed_qa","context":"","instruction":"テストの支援に何か良いツールや方法論はないですか？","response":"Notebookのテスト自体の手法：\nhttps://docs.databricks.com/notebooks/test-notebooks.html#separate-test-code-from-the-notebook\n特に単体テストにフォーカスした詳細：\nhttps://docs.databricks.com/notebooks/testing.html\n\nCICDのパイプラインの中でPull requestあげたときのGitアクションとかのサンプルコードあるといい。\n>> Github actionsの例は以下にございます。\nhttps://github.com/marketplace/actions/run-databricks-notebook"}
{"category":"closed_qa","context":"","instruction":"ジョブ作成でソースに「Gitプロバイダー(main)」を選択して、Git上のノートブックを使用するように指定した場合について。処理内でdbutils.notebook.runの機能を使用して他のDatabricksワークスペース上ワークブックを参照している箇所がエラーになりますが、よい回避策はありますでしょうか。","response":"dbutils.notebook.run()で呼び出す子NotebookをRepos配下に格納し、相対Pathで指定することで問題は解消しますでしょうか？（“/Repos/reponame/test-hello world”ではなく、“test-hello world” としてPathを指定する）\n\nこれは、呼び出し先のNotebookがWorkspace上にあるケース。Git上にあるケースがNext Option：どれかでできる方法を追加検討する。\n\n>>　現状は、先日お伝えしたようにGit上の子NotebookをWorkspaceのReposのCloneして呼び出していただく方法となります。"}
{"category":"closed_qa","context":"","instruction":"Delta Sharing のクライアントとして Tableau Desktop はまだ使えませんか？ （PowerBIは接続する方法を見つけたが、Tableauはまだ見つけられず。）","response":"Tableau DesktopからのDelta Sharingへの接続はTableauのWeb Data Connector（https://help.tableau.com/current/pro/desktop/en-us/examples_web_data_connector.htm）を使うとのことですが、このWDCは2023/1時点で廃止予定となっており、後継のConnectorでサポートされる予定のようです。"}
{"category":"closed_qa","context":"","instruction":"DataLiveTablesで特定のテーブル(処理)だけフルリフレッシュの操作をよく行います。処理が失敗した時に、テーブルの依存関係の図が消えてしまうので、特定のテーブルの選択ができなくなって上記の処理が行えなくなります。図が表示されていなくても特定の処理だけ再実行する方法はありますか？","response":"テーブルの依存関係情報（DAG）が消えてしまうような処理の失敗のケースでは、特定のテーブルだけリフレッシュすることはできません。"}
{"category":"closed_qa","context":"","instruction":"（上の質問に関連して：）特定のテーブルを指定してリフレッシュしたとき、そのテーブルを参照している他のテーブルも自動的にリフレッシュされますか？","response":"自動的にリフレッシュすることはなく、更新するテーブルを選択する際にどうじに更新させたいテーブル（この場合は参照しているテーブル）を手動で選択する必要があります。"}
{"category":"closed_qa","context":"","instruction":"DLTのNotebookを実行したときの Spark UI の Event Log を見ると、以下の画面コピーのように最後の方に resize がかかって、終わる直前に UPSIZE_COMPLETED で Cluster upsized completed but 5 containers could not be added. と表示されて、すぐに TERMINATING になっています。これは、ワーカーを増やし始めたけれど、すぐに処理が終わったので間に合わなかったという事でしょうか？ そうだとすれば、このケースでは、ワーカーは 1 で十分だったと考えてよいですか。あるいはシングルノードクラスタでも大丈夫だと考えても大丈夫ですか？（ちなみにクラスタは、Enhanced autoscaling で Min workers: 1, Max workers: 6 にしています。）そのほか、うまいクラスタの設定方法があれば教えてください。","response":"この状況は何度も頻発している状況でしょうか？\"\"Containers could not be added\"\"というメッセージが出力されるのは、ワーカーの数を増やそうとしたが、クラウドサービスプロバイダーからインスタンスが調達できなかった場合が多いです。（もしかしたら、該当のイベントログのメッセージの続きになにか理由が表示されているかもしれません）\n\nDLTをご利用になる場合のClusterの設定方法としては、Enhanced autoscaling（スケールアウトし、かつ利用率の低いノードを監視し停止させる）をご利用いただくのがBest Practiceとなります。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tables の apply_changes() の apply_as_truncates は、どんなケースで使うのでしょうか？ 例えば、ストリーム処理の途中で、データのタイムスタンプを見て、日付が変わったらテーブルをリセットして、前日までのデータは捨てて今日のデータを蓄積し始めるみたいな使い方はできますか？","response":"https://docs.databricks.com/delta-live-tables/cdc.html#process-scd-type-1-updates\n上記に例がありますが、apply_as_truncateを使うと、テーブルの全レコードを削除できるため、適用条件を定義することにより、定期的にレコードをリセットすることが可能です。（質問いただいている使い方が正しいです）"}
{"category":"closed_qa","context":"","instruction":"autoloader を使う際、途中で読み込むデータのスキーマが変更される場合に備えて、どのように対策しておくのが良いでしょうか？ schema evolution でうまく処理してくれますか？ また、スキーマの変更を検出したときに、何らかの方法で知らせる（アラームを上げる）方法はありますか？","response":"https://docs.databricks.com/ingestion/auto-loader/schema.html#how-does-auto-loader-schema-evolution-work\n上記に記載がありますが、要件に応じて、設定可能なModeのOptionがございます。\nデフォルトの\"\"addNewColumns\"\"というModeだと、スキーマ変更が検出された場合に新しいスキーマをテーブルに反映し、ストリーム処理はFailさせます。このことにより、スキーマの変更を検出し、かつスキーマ変更も自動で反映させることができます。ストリーム処理自体をWorkflowで定義することにより、アラームをあげることが可能です。"}
{"category":"closed_qa","context":"","instruction":"DLTのジョブの実行が失敗して、次のようなメッセージが出ていました。メンテナンスで処理ができない場合がありますか？その場合、メンテナンスの時間を知る方法は\nRun result unavailable: job failed with error message\nSERVICE_UNDER_MAINTENANCE: No instances available to service this request. Please try again shortly.","response":"メンテナンス作業については、DatabricksステータスページをSubscribeすることにより事前に把握することができます。実際のSubscribe方法をWalk Throughさせていただきます。\nhttps://docs.databricks.com/resources/status.html"}
{"category":"closed_qa","context":"","instruction":"Databricksと連携して利用できるProphecyというツールを見つけました。ノーコードで処理を記述できるのが特徴のようですが、なにかご存じでしたら教えてください。\nhttps://www.prophecy.io/prophecy-for-databricks","response":"https://docs.databricks.com/partners/prep/prophecy.html#connect-to-prophecy-using-partner-connect\n上記に記載があるようにProphecyをユーザーインターフェースとして、Databricks Clusterで処理をさせることができます。\nProphecyはビジュアルベースでパイプラインを定義でき、それをベースにPysparkのコードを生成してくれるため、Pysparkにハードルを感じる場合には有用なインテグレーションになると思います。"}
{"category":"closed_qa","context":"","instruction":"Generative AI（生成AI）とは？","response":"Generative AIとは、文章や画像、音楽など、一見独創的で新しい創造物を生み出すことができる技術です。これらのAIは、すでに行われたことを繰り返すだけではありません。大量の事例をもとに学習させたモデルは、人間がどのようにコンテンツを作り上げるかという基本的なパターンを学習し、これまで人間の創造性の表現として理解されてきたような、見たこともない、観察したこともないような構成に到達します。しかし、どの単語が次に続くかを判断したり、画像に様式化されたフィルタを適用したりするような過去の単純なAIとは異なり、これらのAIは、生成されている作品群の幅広い文脈を理解する能力を備えているため、形成されたコンテンツは人間の解釈者にとってはるかに一貫性があり説得力のあるものとなっています。"}
{"category":"closed_qa","context":"","instruction":"Generative AIはプライムタイムの準備ができたか？","response":"生成AIは非常に有望であり、企業はこれを試験的に導入する必要があります。一般的な知識の検索、視点の開発、ある種の情報の分析など、さまざまな能力を備えています。しかし、生成モデルには、商用利用が可能になる前に解決しなければならないいくつかの弱点があります。\n 正確でなくても、権威あるように聞こえます。同じトピックについて対照的な意見を述べるようLLMに求めると、LLMはそれを実行し、どちらにも自信を持っているように聞こえるでしょう。LLMは首尾一貫した文章を作成することができますが、より広い文脈を理解し、それを考慮した文章を作成することにはまだ苦労しています。LLMは訓練された情報に基づいてしか回答を生成できないため、訓練データ以外のトピックに関する知識や理解が限定的となる可能性があります。\n 知的財産権。多くの場合、LLMは、そのデータに対して法的権利を持つ個人または組織が所有するテキストデータでトレーニングされます。これらのデータ所有者は、データに対して所有権や著作権を有している場合があり、大規模な言語モデルの学習にデータを使用する際には、その権利を尊重することを要求される場合があります。また、生成型AIモデルによって生成されるコンテンツの所有権は不透明です。米国の最高裁は最近、AIが生成した画像には知的財産権保護がないとの判断を示しました。\n コストとエネルギー消費： LLMの最大の限界の1つは、モデルをトレーニングするためのコスト（主にコンピューティング）であった。1回のトレーニングに数百万ドル。これは多くの企業にとって経済的に実現不可能であるだけでなく、規模が大きくなれば、経済に大きな悪影響を与えることになります。(これが、私たちがDollyを開発した主な理由の1つです。）\n 偏見を助長する可能性がある。LLMは、学習データに存在するバイアスを再現することが知られています。これは、偏った言語生成や偏った入力に基づく意思決定につながる可能性があります。"}
{"category":"closed_qa","context":"","instruction":"Generative AIの人気のある使い方は何でしょうか？","response":"ChatGPTの知識ベースと同様にリストは無限大ですが、ここでは、小売業者がGenerative AIに投資すると聞いている、または期待している主要なカテゴリをいくつか紹介します：\n 1. パーソナライズされたマーケティング： すべての顧客はユニークです。生成AIを活用することで、顧客体験を向上させ、買い物客に関連する商品をメールやオンラインで消費者に個別メッセージとして提供することができます。\n 2. バーチャル試着室。Generative AIは、買い物客の興味と利用可能な商品にマッチしたカスタム画像を生成するために使用することができます。買い物客は、自分の写真に、カタログから利用可能な商品を生成モデルでレンダリングすることができます。\n 3. チャットボット/カスタマーサービスの自動化。コールセンターでのやり取りから得られたトランスクリプト、商品情報、リアルタイムの注文情報を使ってLLMをチューニングしコールセンターのエージェントの負担を軽減することができる。\n 4. お客様の配送・設置のスケジュール管理。 新しいホームシアターを注文し、配送と設置が必要ですか？LLMは、モバイルアプリケーション、ウェブサイト、あるいは店内のキオスク端末と統合することで、小売業者が顧客と配送や設置のオプションを簡単に調整することができます。LLMは、構成要件、必要部品、リソース要件などにアクセスし、1回の訪問で配送と設置が完了するようにすることができます。\n 5. セキュリティ脅威やアカウント乗っ取りの試みをキャッチする。 生成AIを使用して、顧客のメッセージやその他の通信を分析し、フィッシングの試みやソーシャルエンジニアリングなど、不正行為の兆候を見つけることができます。\n 6. 店舗内アシスタント。ホームセンターを訪れたことのある人ならわかるように、「X製品はどの通路にありますか」と尋ねても、間違った答えが返ってくることが多い。LLMはウェブやモバイルのアプリケーションと統合し、店舗のレイアウトや現在の在庫情報などのデータを活用することで、このようなリクエストに正確に応えることができるようになります。\n 7. 在庫と需要の計画。 小売業にとって最大のペインポイントの一つである需要予測と在庫計画を、生成的AIによって最適化できる可能性は非常に高く、小売業者が過剰在庫や過小在庫を回避するのに役立ちます。Generative AIは、販売データを分析し、どのストーリーにどのレベルの在庫を置くべきかを提案したり、過去のデータ、消費者感情、競合データを分析することで小売業者のトレンド予測を支援することができます。\n 8.商品開発： 飽和状態の市場で際立つユニークで革新的な商品を作るために、小売業者はGenerative AIを活用して、顧客の好み、市場動向、その他のデータに基づいて新しい商品デザインを作ることができます。ここでは、プロダクトレビュー、ドキュメントディスカバリー、契約書レビューなどを活用して、プロダクツロードマップや開発を推進するために、ドキュメントレビューが大きな役割を果たすと予想されます。\n 9. 実用的なインサイトを生み出す。小売業者は、もはや従業員のレポート解釈能力に依存する必要はありません。LLMは、構造化された情報を解釈し、分析し、関連する洞察を平易な言葉で返す訓練を受けることができます。また、LLMを予測モデルと連携させることで、在庫状況、労働力配置、顧客対応など、さまざまな状況の分析を自動化することができます。\n 10. コード開発。Generative AIは、コードの開発に使用することができます！開発者は、ジェネレーティブモデルと連携してベースコードを開発し、それを強化することで、生産性を大幅に向上させることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksは、Generative AIに対しどのような位置づけなのでしょうか？","response":"Dollyの目的は、オープンソースの大規模言語モデル（LLM）を使って、高品質の学習データを使ってChatGPTのような命令追跡機能を持たせることができるという、生成AIの親しみやすさを示すことにあります。\n Dollyでは、数年前からあるオープンソースのLLMを、比較的小さなDatabricksクラスタで、比較的小さな（そして確実にアクセスできる）量のデータでトレーニングできることを証明することができたのです。このデモのポイントは、組織がこの技術にどのようにアプローチするかという選択肢があること、そしてこの機能を得るために外部に依存する必要がないことを示すことでした。\n しかし、Dollyの基盤となっているテクノロジーは、独自の命令追従モデルを安価に構築し、ビジネスに生成的AIを活用したいと考える小売業者にとって、エキサイティングな新しい機会を提供するものだと信じています。"}
{"category":"closed_qa","context":"","instruction":"この新しいテクノロジーを活用するために、どのような準備をすればいいのでしょうか。","response":"LLMやGenerative AIテクノロジーは、ビジネスの運営や価値創造のあり方を大きく変える可能性を秘めています。しかし、まだ道半ばです。小売業にとって重要なのは、この新しいテクノロジーの波に対して、今すぐビジネスを準備することです。小売業は、次のようにしてそれを行うことができます：\n 1. AI戦略の策定 - ビジネス全体と整合性のある明確なAI戦略を持つことが重要になります。小売企業は、顧客体験の向上、サプライチェーンと在庫管理の改善、製品イノベーションの推進にAIをどのように活用できるかを概説し、自動化と意思決定のための新しいユースケースを模索する必要があります。\n 2. データインフラへの投資 - 今後、生成型AIを利用する場合、効果的に運用するためのモデルを訓練するための膨大な量のデータが必要になります。小売業者は、あらゆる種類のデータ（構造化、半構造化、非構造化）を収集、保存、管理でき、この技術が活用するBIとAIのユースケースをサポートするデータレイクハウスの構築に投資する必要があります。\n 3. レイクハウスにデータを取り込む - 生成AIモデルは一般化されています。それらを強力で適切なものにするのは、企業に特有のデータです。企業は、こうした生成モデルを想定してデータのキャプチャとアノテーションに投資する必要があります。\n 4. パイロット - 生成AIはまだ商業的に使用する準備ができていないかもしれませんが、実験をサポートするのに十分な性能を持っています。今、これらのツールを試験的に導入することで、成熟したときに採用するための準備が整うことになります。"}
{"category":"closed_qa","context":"","instruction":"なぜ機械学習に合成データなのか？","response":"まず、誰もがより多くのデータを求めています。それは、より良い機械学習モデルを意味する（こともあるからです）。機械学習は現実の世界をモデル化するので、より多くのデータがあれば、その世界の全体像を把握することができます。角のあるケースで何が起こるのか、何が異常なのか、何が繰り返し観察されるのか。本物のデータは入手困難ですが、本物そっくりのデータを無限に入手するのは簡単です。\n しかし、合成データは、実際に存在するデータを模倣することしかできない。合成データは、実際のデータを模倣するだけであり、実際のデータセットにはない新たな機微を明らかにすることはできない。しかし、実際のデータが意味することを外挿することは可能であり、場合によっては有益であることもあります。\n 第二に、データは自由に共有できない場合があります。個人を特定できる機密情報（PII）が含まれている可能性があります。新しいチームとデータを共有し、探査や分析作業を迅速に行うことが望ましいかもしれませんが、共有するためには、長時間の再編集、特別な取り扱い、フォームへの記入など、官僚的な作業が必要になる可能性があります。\n 合成データは、機密データのようでありながら、実際のデータではないデータを共有することで、その中間的な役割を果たします。場合によっては、この方法にも問題があるかもしれません。合成データが実際のデータポイントに少し似ているとしたら、どうでしょうか。また、不十分な場合もあります。\n しかし、合成データを共有することで十分なデータセキュリティを確保しつつ、コラボレーションを加速させることができるユースケースはたくさんある。例えば、新しい問題を解決する信頼性の高い機械学習パイプラインを開発するために、請負業者のチームに協力してもらいたいと考えています。しかし、機密性の高いデータセットを彼らと共有することはできません。合成データを共有すれば、彼らが実際のデータで実行してもうまく機能するパイプラインを構築するのに十分すぎるほど役立つでしょう。"}
{"category":"closed_qa","context":"","instruction":"製造業におけるデータプロジェクトの70％以上が概念実証（PoC）の段階で停滞し、持続的な価値の実現に至っていません。なぜこのようなことが起こるか？","response":"その理由はいくつかあります：1. レガシーテクノロジー（オンプレミスやクラウドのデータウェアハウス）は、接続された製品やオペレーションから得られる大量のデータに対して、あまりにも複雑でコストが高い。2. 散発的なバッチ駆動のデータとアナリティクスは、リアルタイムの洞察とアクションを妨げ、重要なオペレーションの意思決定に大きな影響を及ぼしています。3. 構造化データと非構造化データのフォーマットとアーキテクチャが分かれており、顧客、業務、資産に関する見解がバラバラである。4. イノベーションをよりコストと時間のかかるものにする、断片的なツーリング"}
{"category":"closed_qa","context":"","instruction":"Databricksの製造業向けソリューションアクセラレータはありますか？","response":"はい、Lakehouse for Manufacturingの上に構築されたDatabricksとパートナーのエコシステムは、業界で最も一般的で価値の高いユースケースに取り組む組織を支援するパッケージソリューションアクセラレータを提供しています。人気のアクセラレータは以下の通りです。1. デジタルツインズ： 実世界のデータをリアルタイムで処理し、大規模に洞察を計算し、データ駆動型の意思決定のために複数の下流アプリケーションに配信します。2. 部品レベルの予測： 在庫切れを防ぎ、リードタイムを短縮し、売上を最大化するために、総量レベルではなく、部品レベルで需要予測を行う。3. 設備全体の有効性： さまざまな形式のセンサー/IoTデバイスからのデータをインクリメンタルに取り込んで処理し、グローバルな製造ネットワークでKPIレポートへの一貫したアプローチを提供します。4. コンピューター・ビジョン :  重要な製造プロセスを自動化するためのコンピュータビジョンアプリケーションの開発と実装、品質の向上、無駄の削減、再加工コストの削減、フローの最適化。 5. プレディクティブ・メンテナンス（IoT）： フィールドデバイスからリアルタイムにIIoTデータを取り込み、複雑な時系列処理を行うことで、稼働率を最大化し、メンテナンスコストを最小化する。"}
{"category":"closed_qa","context":"","instruction":"MLOps Stacksとは？","response":"MLOps Stacksとはコードデプロイパターンを活用しており、モデルのアーティファクトではなくトレーニングのコードをstagingやproductionへ昇格させます。（デプロイコードとデプロイモデルの比較についてはBig Book of MLOpsで詳しく知ることができます。）これは、機械学習モデルの本番移行のためのInfrastructure-as-Code (IaC)やCI/CDを構築するためのcookiecutterテンプレートを提供しています。cookiecutterのプロンプトを介して、DatabricksワークスペースURL、Azureストレージアカウント名などのAzure Databricksの環境値についてテンプレートを設定しました。Stacksはデフォルトでstagingとproductionで異なるDatabricksワークスペースを仮定します。そのため、Azureサービスプリンシパルの生成についてカスタマイズして、同じワークスペースにstaging-sp and prod-spという２つのサービスプリンシパルを持つことにしました。これでCI/CDパイプラインが備わったので、cookiecuterテンプレートに従っての機械学習コードの採り入れへと進みました。下図は私たちが実装した機械学習の開発と自動的な本番移行のための全体的なアーキテクチャです。*注釈：StacksはDatabricksのプライベートプレビュー中の製品であり、将来のモデル開発をさらに簡単にすべく継続的に改良中です。今後のリリースにご期待ください！"}
{"category":"closed_qa","context":"","instruction":"Dollyとは","response":"Dollyは、EleutherAIの既存のオープンソース 60 億パラメータモデルを、Alpaca のデータを用いて、元のモデルにはないブレインストーミングやテキスト生成などの指示に従う機能を引き出すために、ほんの少し修正することで動作します。\n Dolly の基礎となるモデルは、GPT-3 の1750 億個のパラメータに対して60 億個しかなく、しかも 2 年前のものであるため、これほどうまく機能するのは特に驚くべきことです。このことは、ChatGPT のような最先端のモデルの質的向上の多くが、より大きく、よりよくチューニングされた基本モデルではなく、指導に従ったトレーニングデータの集中コーパスのおかげである可能性を示唆しています。このモデルは、LLaMA にインスパイアされた Alpaca のオープンソースクローンであることから、最初のクローン哺乳類である「Dolly the sheep」（羊のドリー）にちなんで 「Dolly」と呼んでいます。企業向け AI の民主化の初期段階であり、まだ多くの課題が残っていますが、Dolly の基盤技術は、独自の命令追従型モデルを安価に構築したい企業にとって、エキサイティングな新しい機会であると信じています。\n ChatGPT のベースとなっている InstructGPT 論文に記載されている命令追従機能について Dolly を評価したところ、テキスト生成、ブレインストーミング、オープン Q&A など、多くの質的機能を発揮していることがわかりました。これらのサンプルで特に注目すべきは、生成されたテキストの品質ではなく、数年前のオープンソースモデルを小規模で高品質なデータセットでファインチューニングした結果、命令追従能力が大幅に向上したことです。"}
{"category":"closed_qa","context":"","instruction":"なぜオープンモデルなのか？","response":"企業が、API の背後にある独自のモデルを提供する集中型 LLM プロバイダにデータを送るのではなく、独自のモデルを構築することを好む理由はたくさんあります。多くの企業にとって、AI から恩恵を受ける可能性が最も高い問題やデータセットは、最も秘密性の高い独自の知的財産であり、それを第三者に渡すことは好ましくないかもしれません。さらに、モデルの品質、コスト、望ましい動作など、組織によってトレードオフが異なる場合もあります。私たちは、ほとんどの ML ユーザーがモデルを直接所有することが、長期的に最も良い結果をもたらすと信じています。\n Databricks で Dolly を自作するために使用できる簡単な Databricksノートブックをオープンソース化しています。モデルをダウンロードして実験するには、Databricks Hugging Face Page をご覧ください！"}
{"category":"closed_qa","context":"","instruction":"T5 (Text-to-Text Transfer Transformer)は何ですか？","response":"T5 (Text-to-Text Transfer Transformer)は、Googleが提供する汎用LLMのファミリーです。要約、分類、翻訳など多くのタスクに役立ち、「小さい」（～60Mパラメータ）からかなり大きい（～11Bパラメータ）ものまで、いくつかのサイズが用意されている。これらのサイズは、ますます強力になる一方で、扱うにはますます高価になっています。これらのLLMを扱う際の重要なテーマは、「シンプルに保つ」ことです。小さいモデルで十分であれば、それを使用し、可能であれば既製のリソースで始める。大きなモデルを使った推論には時間とコストがかかるので、レイテンシーや予算の制約から、最初から大きなモデルは使えないかもしれません。"}
{"category":"closed_qa","context":"","instruction":"学習済みモデルのファインチューニングとは？","response":"ファインチューニングとは、あらかじめ訓練されたモデルを新しいデータでさらに訓練し、ある特定のタスクの性能を向上させることを意味します。T5のようなモデルは、ある単語列を別の単語に変換するような多くのことを行うように訓練されています。ここで、T5が得意とするのは、多くの商品レビューをレビュー要約に変換することです。これはT5が得意とする要約作業のようなもので、手元にある実際のレビューデータによりよく合うように要約を調整することを望んでいるだけです。これは、T5アーキテクチャをゼロからトレーニングするのとは違います。時間がかかるだけでなく、事前に訓練されたT5モデルが持っている言語に関する学習がすべて失われてしまうからです。\n 上記の例は、既存のモデルと、その作成に費やした研究、データ、計算能力を再利用しているため、起こっていることの複雑さに比べれば、非常にシンプルです。しかし、fine-Tuningはモデルのトレーニングであり、経験豊富な実務家であっても、トレーニングプロセスを継続するために必要なPyTorchやTensorflowのコードを書くことは簡単ではありません。"}
{"category":"closed_qa","context":"","instruction":"DeepSpeedとは？","response":"MicrosoftのDeepSpeedは、洗練された最適化を多数実装することで、既存の深層学習トレーニングや推論ジョブをほとんど変更することなく高速化することができます。特に興味深いのは、メモリ使用量を減らそうとする最適化セットであるZeROです。詳細や論文については、DeepSpeedのサイトを参照してください。DeepSpeedは、Hugging FaceのTrainer APIを使用するFine-Tuningジョブを自動的に最適化することができ、既存のFine-Tuningスクリプトを実行するためのドロップイン代替スクリプトを提供しています。これは、既製のトレーニングスクリプトを再利用することが有利である理由の1つです。"}
{"category":"closed_qa","context":"","instruction":"レイクハウスとは？","response":"レイクハウスはこれまでのデータプラットフォームの課題を解決するために、データウェアハウスとデータレイクの長所を組み合わせた新たなデータプラットフォームである。以下の図に示しているように、レイクハウスではテーブルなどの構造化データ、ログやJSONのような半構造化データ、さらには、画像・音声・テキストのような非構造化データすべてを格納することができ、データウェアハウスを活用して行われていたBIや、データレイクの主なユースケースであるデータサイエンスや機械学習の取り組みなどをすべて一つのプラットフォームで実施することできる。\n レイクハウスにおいては、データをオープンなデータレイクに格納することでデータの種類やサイズを問わずそのままの状態で保持することが可能である。これによってベンダーロックインを回避できることに加え、将来的に取り組むであろうユースケースに備えて柔軟にデータを蓄積することが可能となる。\n しかし、データレイクには上述の長所がある一方で、以前の記事でも触れたようにデータ品質やパフォーマンスの課題がある。どのようなデータを格納できたとしても、そのままではゴミだらけになってしまう。このため、従来のデータレイクはデータスワンプ(沼)と揶揄されることもあった。"}
{"category":"closed_qa","context":"","instruction":"データ×AIプロジェクトとは？","response":"これまでは、購買履歴、CRMデータなどの構造化データをデータウェアハウスで集計、加工し、BIダッシュボードに表示することでビジネスの意思決定に役立てるというのが、データ活用の主なユースケースであった。しかし、競争優位性を高めるために、過去にのみ目を向けるのではなく、データとAIを活用することで将来を見通そうというニーズが高まっている。\n このため、購買履歴やCRMのような構造化データだけではなく、Webサイトの利用履歴、ソーシャルメディアへの投稿、電話の通話記録などの半構造化データ、非構造化データにも着目し、これらのデータに機械学習・人工知能を適用することで、これまでとは比較にならないビジネス価値を生み出すことをゴールとして多くの企業でデータ×AIプロジェクトが推進されている。このような取り組みに関してはこちらのウェビナーでも説明しているので参照願いたい。そして、このような取り組みでは、多くの場合、データウェアハウスやデータレイクのようなデータ管理プラットフォームが活用されている。"}
{"category":"closed_qa","context":"","instruction":"サイバーセキュリティはなぜ難しいのか？","response":"メーカーによる運用技術のサイバーセキュリティへの投資は、従来情報技術への投資に比べ遅れていました。しかし ITとOTの融合を考えるとそんな事を言ってられません。つまりメーカーはサイバーセキュリティを総合的に考える必要があるのです。\n 技術面では、OTのサイバーセキュリティはITのサイバーセキュリティとは異なり、ICSやSCADA機器にEDR（Endpoint Detection and Response）エージェントをインストールすることは、なかなか難しいか実現不可能です。そのため、ICSやSCADAシステムのセキュリティは、通常これらのシステムを行き来するネットワークトラフィックの監視のみに依存しています。膨大なネットワークデータの取得、保存、監視は技術面でもコスト面でも課題となっています。\n また、製造業は資本集約型であるため、製造装置の寿命は数十年に及び、ソフトウェアやファームウェアの更新が制限されることが多いという課題もあります。技術のライフサイクルが長く、更新が限られているため、機器のソフトウェアやファームウェアにパッチが適用されていない脆弱性が存在するリスクが高まります。パッチが適用されていない脆弱性は、通常、脅威者が環境に侵入する足がかりとなる。そのため、OTのセキュリティ対策では、レガシーな機器やインフラからのリスクを軽減するためにロギングや監視を追加する必要があり、データインフラにさらなる負担を強いることになる場合が多いのです。\n 投資の不足と産業機器の性質、さらに本質的にビッグデータの問題が相まって、今日直面している製造業向けのサイバーセキュリティツールの少なさを生み出しています。"}
{"category":"closed_qa","context":"","instruction":"UDFのパフォーマンスチューニングのポイントは何？","response":"UDFのパフォーマンスチューニングには、2つのポイントがあります。1つ目は、各GPUを効率よく使いたいということで、これはTransformersパイプラインによってGPUに送られるアイテムのバッチサイズを変更することで調整することができます。もう1つは、データフレームをうまく分割してクラスタ全体を活用することです。"}
{"category":"closed_qa","context":"","instruction":"Prophetとは？","response":"Prophetは、時系列予測のためのオープンソースライブラリです。Facebook 社の Core Data Science チームが開発・リリースしており、年毎、週毎、日毎の周期性に加え、休日の影響などを考慮して非線形な傾向を持つ時系列データをシンプルにモデル化できるという特長があります。さらに、異常値や欠損データの扱いにも強く、また、人間が理解しやすいパラメタやドメイン知識などを加えることで、モデルの精度を向上させる機能も備えています。"}
{"category":"closed_qa","context":"","instruction":"Community Editionの機能と制限について教えてください。","response":"Community Editionでは、Databricksのフルバージョンの機能に対して以下の制限があります。\n 1. 作成できるクラスターは15GB RAM, 2 Core CPUのシングルノードのみ \n 2. ワークスペースに追加できるユーザー数は最大3名まで \n 3. クラスターのリージョンはus-westのみ \n 使用できない機能 \n 1. ジョブのスケジュール機能 \n 2. クラスターのオートスケーリング機能 \n 3. Git連携 \n 4. MLflowの一部モデル管理機能(レジストリ、RESTサービング) \n 5. REST APIによるワークスペースの制御 \n 5. セキュリティ、ロールベースのアクセス制御、監査、シングルサインオン \n 6. BIツール連携のサポート"}
{"category":"closed_qa","context":"","instruction":"Community Editionの使用期間の制限はありますか？","response":"使用期間の制限はありません。そのため、Databricks機能の使用感の確認や、Databricksのトレーニング教材を実行していただく環境として広くご利用いただけます。例えば、以下の機能・環境がCommunity Edition上で利用することができます。\n 1. Databricks Workspace/Notebook環境(共同編集・コラボレーション機能) \n 2. Spark (Databricks Runtime版) \n 3. Delta Lake (Databricks版) \n 4. MLflow(モデルトラッキング機能) \n 5. Koalas(SparkのPandas API) \n 6. MLlib(Sparkネイティブの機械学習ライブラリ) \n 7. 一般的な機械学習ライブラリ環境(TensorFlow, Keras,  Pytorch, Scikit Learn, XGBoostなど) \n 8. 一般的なデータサイエンス環境(Scipy, Numpy, Pandas, Matplotlibなど)"}
{"category":"closed_qa","context":"","instruction":"Unity Catalog でのデータリネージの主な機能は何か？","response":"1. 自動化されたリアルタイムのリネージ：Unity Catalog は、あらゆる言語（Python、SQL、R、Scala）、実行モード（バッチ、ストリーミング）で実行されたクエリのデータフロー図を自動的に取得し、リアルタイムに表示します。リアルタイムのリネージにより、データフローの証跡を手動で作成する運用上のオーバーヘッドを削減します。Unity Catalog メタストアに接続されたすべてのワークスペースにおいて、データリネージは自動的に集約されます。これは、あるワークスペースで取得したリネージを、同じメタストアを共有する他のワークスペースで見ることができることを意味します。\n 2. 統一されたカラムとテーブルの系統グラフ：Unity Catalog では、1 つの系統グラフで列と表の両方の系統を確認できるようになり、特定の表や列が何で構成され、データがどこから来るのかをよりよく理解できるようになりました。ユーザーは、数回のクリックで、上流または下流の系統グラフを移動し、完全なデータフロー図を見ることができます。\n 3. テーブルやカラムの枠を超える：Unity Catalog は、ノートブック、ワークフロー、およびダッシュボードの系統も追跡します。これにより、組織内でのデータの使用方法に関するエンドツーエンドの可視性が向上し、データの変更が下流の消費者に与える影響を把握することができます。\n 4. ビルトインセキュリティ：系統グラフはデフォルトで安全であり、Unity Catalog の共通権限モデルを使用しています。ユーザーは、系統データフロー図を表示するために正しい権限を持っている必要があり、セキュリティのレイヤーを追加して、意図しないデータ侵害のリスクを低減します。たとえば、ユーザーがテーブルの SELECT 権限を持っていない場合、そのテーブルの血統を調べることはできません。同様に、ノートブック、ワークフロー、ダッシュボードの系統情報も、閲覧権限を持っているユーザーだけが見ることができます。\n 5. パートナーとの連携：Unity Catalog は、Unity Catalog REST API を通じて、さまざまなデータガバナンスパートナーとの豊富な連携を実現しており、系統情報を容易にエクスポートすることが可能です。"}
{"category":"closed_qa","context":"","instruction":"Data Vault とは？","response":"Data Vault は、Kimball や Inmon の手法に比べ、企業規模の分析用データウェアハウスを構築するために用いられる、より新しいデータモデリングのデザインパターンです。\n Data Vault は、データをハブ、リンク、サテライトの 3 つのタイプに整理しています。ハブはコアのビジネスエンティティを表し、リンクはハブ間の関係を表し、サテライトはハブやリンクに関する属性を格納します。\n Data Vault は、拡張性、データ統合/ETL、開発スピードが重要視されるアジャイルデータウェアハウス開発に重点を置いています。ほとんどのお客様は、ランディングゾーン、Vaultゾーン、データマートゾーンを、Databricks の組織パラダイムのブロンズ、シルバー、ゴールドレイヤーに対応させています。Data Vault のハブ、リンク、サテライトテーブルのモデリングスタイルは、通常 Databricks レイクハウスのシルバーレイヤーによく合います。"}
{"category":"closed_qa","context":"","instruction":"ディメンショナルモデリング(次元モデリング)とは？","response":"次元モデリングは、データウェアハウスを分析用に最適化するために設計するボトムアップのアプローチです。次元モデルは、ビジネスデータを次元（時間や商品など）とファクト（金額や数量の取引など）に非正規化し、異なる対象領域を適合した次元で接続して、異なるファクトテーブルにナビゲートするために使用されます。\n 次元モデリングの最も一般的な形式は、スタースキーマです。スタースキーマは多次元データモデルで、データを整理して理解しやすく、分析しやすく、またレポートの実行が非常に簡単で直感的にできるようにするために使用されます。Kimballスタイルのスタースキーマや次元モデルは、データウェアハウスやデータマートのプレゼンテーション層、さらにはセマンティック層やレポート層におけるゴールドスタンダードと言えます。スタースキーマの設計は、大規模なデータセットに対するクエリに最適化されています。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeとは？","response":"Delta Lake は、データレイクのテーブルに対して挿入、更新、削除、ACID トランザクションの追加を容易にし、メンテナンスと履歴管理を簡素化するオープンなストレージ形式レイヤーです。Delta Lake は、動的ファイルプルーニングを実行して、より高速な SQL クエリに最適化する機能も提供しています。"}
{"category":"closed_qa","context":"","instruction":"DLT が大手企業のデータエンジニアやアナリストをどのように支援し、本番環境に対応したストリーミングとバッチパイプラインの簡単な構築や、大規模なインフラストラクチャの自動管理、および、新世代のデータ、分析、AI アプリケーションの提供に役立つか？","response":"DLT は、既に世界中の大手企業において本番運用でのユースケースを支援しています。スタートアップからエンタープライズまで、ADP、シェル、H&R Block、Jumbo、Bread Finance、JLL を含む 400 社以上の企業が、DLT を使用して次世代のセルフサービス分析とデータアプリケーションを強化しています。\n Delta Live Tables を用いることで、アナリストとデータエンジニアは、SQL や Python を使用して本番環境に対応したストリーミング、またはバッチ ETL パイプラインを簡単に構築できます。完全なデータパイプラインの宣言型記述を一位にキャプチャして依存関係をライブで理解し、内在する運用の複雑性を自動化することで ETL 開発をシンプルにします。DLT を使用すると、エンジニアはパイプラインの運用や保守ではなくデータの配信に集中することができ、次の主要なメリットを活用できます。\n DLT のプレビューをリリースして以来、Databricks はいくつかのエンタープライズ向け機能と UX の改善を行ってきました。UI を拡張することで、DLT パイプラインのスケジュール設定、エラーの参照、ACL（アクセス制御リスト）の管理、テーブルリネージュの可視化の改善、データ品質の可観測性 UI とメトリクスの追加をシンプルにしました。さらに、継続的に到着するデータを効率的かつ簡単にキャプチャするためにの変更データキャプチャ（CDC）のサポートと、ストリーミングワークロード向けに優れた性能を提供する拡張オートスケーリングのプレビューをリリースしました。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQL とは何？","response":"Databricks SQL は、 Databricks のレイクハウスプラットフォームにおける SQL のデータウェアハウス機能とファーストクラスのサポートを提供します。これにより、アナリストは従来のクラウドデータウェアハウスよりもわずかなコストで新たな知見を迅速に発見し、共有できるようになります。"}
{"category":"closed_qa","context":"","instruction":"クエリプロファイルはどのような機能でしょうか？","response":"クエリプロファイルは、クエリの実行の詳細や、時間とコンピューティングリソースが費やされている場所を確認するための詳細なメトリクスを提供します。"}
{"category":"closed_qa","context":"","instruction":"クエリプロファイルにはどういう機能を提供していますか？","response":"クエリプロファイルには、次の主要な機能が含まれています。\n\nクエリの実行と関連するメトリクスにおける主なコンポーネントの詳細化：タスクに費やされた時間、処理された行、メモリ消費量\n複数のグラフィック表現：最も遅い処理を一目で確認できる要約されたツリービュー、クエリ演算子間のデータフローを理解するためのグラフビュー\n結合の展開や全表スキャンといったクエリのよくある間違いを簡単に検出する機能\nクエリプロファイルをダウンロードして共有する機能でコラボレーションを向上"}
{"category":"closed_qa","context":"","instruction":"TPC-DS とは","response":"TPC-DS は、トランザクション処理性能評議会（TPC）によって定義されるデータウェアハウスのベンチマークです。TPC は、1980 年代後半にデータベースコミュニティによって設立された非営利団体で、リアルワールドのシナリオを模倣したベンチマークの作成に注力しています。そのため、TPC は、データベースシステムの性能を客観的に測定する目的で使用されています。また、TPC はデータベースの領域にも大きな影響を与え、Oracle、Microsoft、IBM などの確立されたベンダーによる 10 年に及ぶ「ベンチマーク戦争」が、この領域の発展を推進してきました。\n\nTPC-DS の “DS” は、意思決定支援（Decision Support）を意味します。極めてシンプルな集計から複雑なパターンマイニングまで、さまざまな複雑さを持つ 99 のクエリが含まれています。これは、複雑さが増す分析を反映した 2000 年代半ばからスタートした比較的新たな取り組みのベンチマークです。TPC-DS は、この 10 年間でデータウェアハウスのベンチマークの事実上の標準となり、ほぼ全てのベンダーで採用されています。"}
{"category":"closed_qa","context":"","instruction":"Databricks Repos はどのような背景で開発されましたか？","response":"Databricks Repos は、データチームが常に抱えている課題を解決するために開発されました。データエンジニア、データサイエンティストが使用しているツールの多くは、Git のバージョン管理システムとの連携が不十分、あるいは全くありません。コードをレビュー、コミットするだけでも、数多くのファイル、ステップ、UIをナビゲートする必要がありました。これでは時間がかかるだけでなく、エラーを発生しやすくします。"}
{"category":"closed_qa","context":"","instruction":"Databricks Reposはどういう機能ですか？","response":"Databricks Repos は、Databricks と一般的な Git プロバイダーを直接リポジトリレベルで統合することで、データの実践者は新規の Git リポジトリや既存リポジトリをクローンの作成、Git オペレーションの実行、開発のベストプラクティスに従うことを容易にします。\n\nDatabricks Repos によって、ブランチの管理、リモートの変更のプル、コミット前の変更点の確認など、使い慣れた Git の機能にアクセスができるため、Git ベースの開発ワークフローを容易に活用できるようになります。さらに、Repos は Github、Bitbucket、Gitlab、Microsoft Azure DevOps のような幅広い Git プロバイダをサポートしており、CI/CDシステムと統合を可能にする一連の　API も提供しています。"}
{"category":"closed_qa","context":"","instruction":"「Files in Repos」と呼ばれるこの機能で何か実現できるか？","response":"「Files in Repos」と呼ばれるこの機能は、Databricks 上で Python ソースコード、ライブラリファイル、設定ファイル、環境設定ファイル、小規模のデータファイルなど、ノートブックファイル以外のファイルを扱うことができる Repos の新機能です。この機能は、コードの再利用、環境管理、デプロイメントの自動化を容易にします。ユーザーはローカルファイルシステムと同じように、Databricks Repos のファイルに対して、インポート（クローン）、読み込み、編集が可能です。"}
{"category":"closed_qa","context":"","instruction":"bamboolibとは？","response":"bamboolib は、Python コード（Excel のマクロを想像してみてください）をエクスポートする、拡張可能な GUI を提供します。ユーザーによるコーディングなしで、高速かつシンプルなデータ探索と変換を可能にします。UI ベースのワークフローにより、市民データサイエンティストとエキスパートの両方が容易に Databricks にアクセスできるようになり、社員のオンボーディングやトレーニングのコストを削減できます。ノーコードのユースケースには次のようなものがあります。"}
{"category":"closed_qa","context":"","instruction":"Databricks Marketplaceとは何？","response":"Databricks MarketplaceはオープンソースのDelta Sharing規格を利用した、あらゆるデータ、アナリティクス、AIのためのオープンマーケットプレイスです。データ消費者とデータ提供者の広大なエコシステムを結集し、データセット、ノートブック、MLモデルなど、さまざまなデータ資産を、プラットフォームの依存関係や複雑なETL、高価なレプリケーションなしに共有・共同利用できます。データコンシューマーは、組織のAI、ML、アナリティクスイニシアチブを革新・推進し、ベンダーロックインすることなく、より迅速なインサイトを提供することができます。データプロバイダーは、ビジネスを拡大し、新しいユーザーを獲得し、収益を上げることができます。"}
{"category":"closed_qa","context":"","instruction":"データコンシューマーはDatabricks Marketplaceで何かできますか？","response":"データコンシューマーはDatabricks Marketplaceで、組織のAI、ML、アナリティクスイニシアチブを革新・推進し、ベンダーロックインすることなく、より迅速なインサイトを提供することができます。"}
{"category":"closed_qa","context":"","instruction":"データプロバイダーはDatabricks Marketplaceで何かできますか？","response":"データプロバイダーはDatabricks Marketplaceで、ビジネスを拡大し、新しいユーザーを獲得し、収益を上げることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Marketplaceでは何を提供しているか？","response":"オープンスタンダードに基づき構築されたDatabricks Marketplaceは、データプロバイダーとあらゆるクラウド上の最も豊富なデータツールを接続し、データプロバイダーがプラットフォーム依存、ETL、レプリケーションなしに、データセットとモデル、ノートブック、ダッシュボードなどのデータ資産の両方を収益化し、データ消費者が最速かつ最もシンプルに洞察を得ることができるよう独自に提供します。"}
{"category":"closed_qa","context":"","instruction":"Databricks Marketplaceはデータコンシューマーにとって何かいいことがありますか？","response":"Databricks Marketplaceは、データコンシューマーがイノベーションを実現し、すべてのアナリティクスとAIイニシアチブを前進させる機会を劇的に拡大するものです。データコンシューマーは、独自のプラットフォームの依存関係や複雑なETL、高価なレプリケーションなしに、データセットだけでなく、MLモデル、ノートブック、アプリケーション、ダッシュボードなどのAIや分析資産も独自に入手できます。このオープンなアプローチにより、あらゆるクラウドで、お好みのツールを使って、より迅速にデータを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"Hugging FaceデータセットにSparkデータフレームをどのようにロードすればいいですか？","response":"最新のHugging Faceリリースでは、Datasetsの新しい \"from_spark \"関数を呼び出すだけで、ユーザーが同じタスクを達成するのをより簡単にできるようにしました。これにより、ユーザーはSparkを使用して、モデルのトレーニングやファインチューニングのためにデータを効率的にロードして変換し、SparkデータフレームをHugging Faceデータセットに簡単にマッピングして、トレーニングパイプラインに超シンプルに統合することができます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksはオープンソースLLMに対してどんなサポートを提供していますか？","response":"私たちはモデルのトレーニング、チューニング、デプロイを成功させるための最高のツールを提供するために努力しています。Hugging Faceへの貢献を続けるだけでなく、他のオープンソースプロジェクトへの改善もリリースしはじめました。最近のMLflowのリリースでは、transformersライブラリのサポート、OpenAIの統合、Langchainのサポートが追加されました。また、Databricks SQLのAI Functionsも発表し、OpenAI（あるいは将来的には独自のデプロイメントモデル）をクエリに簡単に統合できるようにしました。さらに、Databricks上での分散PyTorchトレーニングを簡素化するために、Spark用のPyTorch distributorもリリースしました。 "}
{"category":"closed_qa","context":"","instruction":"セマンティック・コンシステントとはどういう意味ですか？","response":"通常、データレイクに取り込まれたデータは、取り込み層からキュレーション層へと移動する際にクレンジングされます。データを意味的に一貫したものにするためには、ビジネスコンテキストの観点からも整合性をとる必要があります。例えば、異なるシステムからの顧客データや収益データは、両方のレイヤーで同じ意味を持たせる必要があります。そして、適切なレベルで、データの重複を排除する必要があります。\n\nこの追加ステップにより、キュレーション層の意味的に一貫したデータをインテグレーション層のデータと安全に結合し、ゴールド層のビジネス対応テーブルを提供できるようになります。"}
{"category":"closed_qa","context":"","instruction":"DWH層内のデータと結合するDWH層外のデータセットにセマンティック一貫性を適用することで、何かのメリットがありますか？","response":"DWH層内のデータと結合するDWH層外のデータセットにセマンティック一貫性を適用することで、いくつかのメリットがあります：\n\nDWHレイヤーの外にあるデータのセマンティック一貫性を実現することは、そのデータをDWHに統合するよりもはるかに簡単です。\nその結果、コアとなるDWHが安定したまま、DWHの開発をより速く、より探索的に行うことができます。また、ある結果が永続的な価値を持つことが証明された場合、そのデータセットを後日DWHに統合することができます。\nデータサイエンティストは、ビジネスKPIなど組織のビジネス情報モデルに由来するデータに簡単にアクセスでき、意味的に一貫性のあるデータと組み合わせることができます。\n最後に、すべてのデータが意味的に一貫している必要はないので、データレイクアプローチの高速で探索的な性質は、現代のユースケースでも利用可能です。"}
{"category":"closed_qa","context":"","instruction":"機械学習モデルではどんなことを行うか？","response":"機械学習モデルは、データセットに対して学習（教師ありまたは教師なし）され、以前に見たことのないデータセットからパターンを見つけたり予測を行ったりするように最適化されます。"}
{"category":"closed_qa","context":"","instruction":"ディープラーニングは何？","response":"ディープラーニングは、機械学習のサブセットで、人間の脳の構造と機能から着想を得たアルゴリズムを使用します。ディープラーニングは、従来のタスクに特化したアルゴリズムではなく、データ表現の学習に基づく、より広範な機械学習手法の一群に属します。"}
{"category":"closed_qa","context":"","instruction":"データレイク上でのモダンなユースケースと、データウェアハウス上での従来のBIユースケースの区別を知りたいです。","response":"データレイク上でのモダンなユースケースと、データウェアハウス上での従来のBIユースケースを区別して説明します：\n\nモダンなユースケースのためのデータレイク 現代のユースケースは、通常、データレイクに保存されたデータに対して機械学習やAIを使用します。どちらのアプローチも、行動や起こりうる結果を予測するモデルを作成し、ビジネスユーザーが事前に適切な行動を取れるようにすることを目的としています。\n\n機械学習モデルは、データセットに対して学習（教師ありまたは教師なし）され、以前に見たことのないデータセットからパターンを見つけたり予測を行ったりするように最適化されます。ディープラーニングは、機械学習のサブセットで、人間の脳の構造と機能から着想を得たアルゴリズムを使用します。ディープラーニングは、従来のタスクに特化したアルゴリズムではなく、データ表現の学習に基づく、より広範な機械学習手法の一群に属します。機械学習では、ユースケースのためにデータレイクに明示的に取り込まれた大規模なデータセットを使用することがよくあります。\n\nデータウェアハウスは、複数のコアビジネスソリューションから現在および過去のデータを保存するデータ管理システムです。データウェアハウスは、組織がどのようにビジネスを行っているかをモデル化したもので、洞察やレポーティングを容易に行うことができます。データウェアハウスは、BI、レポーティング、データ分析のために、運用データベースからデータを抽出・集約します。データウェアハウスの利点は、多くのソースからのデータを統合し、履歴情報を提供し、分析処理をトランザクションデータベースから切り離すことです。データウェアハウスは、データの品質と正確性を確保し、例えば、異なる製品タイプ、言語、通貨に対する命名規則やコードによって一貫性を持たせることでデータを標準化します。"}
{"category":"closed_qa","context":"","instruction":"データウェアハウスは何を管理するか？","response":"データウェアハウスは、複数のコアビジネスソリューションから現在および過去のデータを保存するデータ管理システムです。"}
{"category":"closed_qa","context":"","instruction":"Databricks Lakehouse Platformのガバナンスモデルはどのように実現しているか？","response":"Databricks Lakehouse Platformは、Unity Catalogを通じて、統一されたガバナンスモデルを実現する手段を提供します。Unity Catalogでは、データアーティファクトにきめ細かい権限を付与し、ユーザーをグループ化して一元的に管理することが可能です。どのようなデータやリソースにアクセスできるのか、チームやユーザーを簡単に管理できることは、持続可能な運営モデルやコスト管理のために重要なことなのです。"}
{"category":"closed_qa","context":"","instruction":"AIの運用モデルの最終的な目的は何？","response":"AIの運用モデルの最終的な目的は、データからビジネス成果までの価値生成プロセスを合理化することです。"}
{"category":"closed_qa","context":"","instruction":"DatabricksはAIモデルの実行と運用に何を提供しているか？","response":"Databricksプラットフォームは、探索と革新から実験、追跡、展開まで、機械学習運用のエンドツーエンドのライフサイクルをカバーする環境を提供します。"}
{"category":"closed_qa","context":"","instruction":"DatabricksはMLOpsに関して何を提供しているか？","response":"Databricksは、データ中心のAIに焦点を当てたMLOpsフレームワークとツールセットを組織に提供し、チームがそれに従うことで、開発、テスト、提供のための明確な基準を持つことができます。"}
{"category":"closed_qa","context":"","instruction":"なぜAIの運用モデルの追跡と実行は重要ですか？","response":"追跡と実行というテーマでは、AIの運用モデルの成功の重要な部分は、データとAIの利用を測定可能なビジネス成果に効果的に結びつけることができることであり、DatabricksはそうしたKPIを収集、管理、維持できる基盤を提供する。ビジネスにおけるデータの価値を表すKPIの1つが、RODA（Return on Data Assets）です。"}
{"category":"closed_qa","context":"","instruction":"Databricksはユースケースから本番までどのようにサポートしているか？","response":"Databricksは、ユースケースから本番までの開発サイクルを追跡・定量化し、ワークロードの開発・実行方法に関する標準を確立する能力を組織に提供し、強制、所有、検証、報告することができます。"}
{"category":"closed_qa","context":"","instruction":"モデルの監視、更新と維持にどのようなサポートがありますか？","response":"Databricksは、モデル管理に関する明確なプロセスを構築し、モデルの出力を効果的にビジネス成果に結びつけるために必要なツールと構造をチームに提供し、データをビジネスパフォーマンスの向上につなげる好循環を実現します。"}
{"category":"closed_qa","context":"","instruction":"製品カタログの検索にLLMを活用したソリューションはどんなもの？","response":"製品カタログの検索にLLMを使えば、商品説明や文章、音声の記録などに目を通し、ユーザーの検索に応えて、その内容に関連するものを提案するよう、モデルに課すことができます。ユーザーは、探しているものを見つけるために正確な用語を必要とせず、LLMがニーズに合わせて方向付けることができる一般的な説明だけでよいのです。その結果、ユーザーがサイトを利用する際に、まるでパーソナライズされた専門家のガイダンスを受けたかのような感覚に陥る、パワフルな新しい体験が得られました。"}
{"category":"closed_qa","context":"","instruction":"LLMを活用して特定の文書群を効果的に検索するために何か必要ですか？","response":"特定の文書群を効果的に検索するためには、その文書に特化して学習させる必要すらありません。"}
{"category":"closed_qa","context":"","instruction":"モデルのファインチューニングで何かできるか？","response":"ファインチューニングを行うことで、モデルの方向性を、それが意図する特定のコンテンツに対して調整することができます。事前にトレーニングしたモデルを、特定分野の情報などで追加トレーニングすることで、このコンテンツとより一致した方法でユーザーのプロンプトに応答するモデルの能力が向上し、多くの組織にとって実行する価値のあるステップとなります。"}
{"category":"closed_qa","context":"","instruction":"訓練済みのオープンソースのLLMはどこにあるのでしょうか？","response":"Dolly 2.0はそのようなモデルの1つで、ダウンロードサイトに提示されているライセンス条項に従って自由にダウンロードし、広く使用することができます。Hugging Face は、AIコミュニティがセマンティック検索と呼ぶものに最適な言語モデル（大規模なもの、そうでないものも）を発見できる人気の場所です。"}
{"category":"closed_qa","context":"","instruction":"エンベッディングとは何でしょうか？","response":"エンベッディングとは、文章、段落、文書を数値で表したものです。どのように生成されるかの仕組みは、モデルの中に埋もれていますが、理解すべき重要なことは、モデルが2つの文書をエンベッディングに変換したとき、数値の間の数学的距離（差）が、それらの間の類似性の程度について何かを教えてくれることです。"}
{"category":"closed_qa","context":"","instruction":"エンベッディングはどのようにモデルと結合されるのか？","response":"オープンソースのlangchainのようなツールは、このためのビルディングブロックを提供しています。重要なのは、例えば検索したい商品カタログの詳細を形成する埋め込みデータは、従来のリレーショナルデータベースやNoSQLデータストアの中からは検索できないことです。その代わりに、特殊なベクトルストアを使用する必要があります。"}
{"category":"closed_qa","context":"","instruction":"マイクロサービスとは何でしょうか？","response":"マイクロサービスとは、検索フレーズなどのリクエストを受け取り、レスポンスを返す軽量なアプリケーションのことです。モデルと検索する埋め込みをマイクロサービス内にパッケージ化することで、マイクロサービスが提供する検索機能にアプリケーションから広くアクセスできるようになるだけでなく、ほとんどのマイクロサービス基盤ソリューションは弾力的なスケーラビリティをサポートしているので、需要の増減に対応できるようにサービスにリソースを割り当てることができる。これは、コストを抑えながらアップタイムを管理するために不可欠です。"}
{"category":"closed_qa","context":"","instruction":"Predictive I/Oは追加料金かかりますか？","response":"Predictive I/Oは、DB SQL ProとServerlessではデフォルトでオンになっており、追加コストなしで機能します。"}
{"category":"closed_qa","context":"","instruction":"ポイントルックアップとは何か、なぜ高いのか？","response":"選択的クエリやポイント・ルックアップは、BIやアナリティクスのユースケースでよく見られる、大きなデータセットから少量または単一の結果を返そうとするものです。BIやアナリティクスのユースケースでよく見られるもので、「針の穴を通すような」クエリーと呼ばれることもありますが、こうしたクエリーをコストを抑えながら高速化することは困難です。なぜなら、クラウドデータウェアハウス（CDW）でポイント検索を高速化するためには、インデックスを作成するか、追加の最適化サービスを使用しなければならないからです。さらに、これらのオプションはノブであり、各アプローチを有効にする前に、いつ、どのように使用するかを理解する必要があることを意味します。"}
{"category":"closed_qa","context":"","instruction":"Predictive I/Oのメリットを教えてください。","response":"一つの例をみてみましょう。例えCDWにデータセットをロードした後、クエリに8.7秒かかっています。もしこれが十分な速度でない場合は、高価な最適化サービスを利用すれば、3.6秒まで短縮することが可能です。しかし、忘れてはならないのは、このためにお金を払っていることです。テーブルが変わるたびに、最適化サービスはメンテナンス作業を行わなければならないのです。もっと簡単で安価な方法として、Predictive I/Oを使えば、Lakehouseのデータを3.7秒でロードしてクエリすることができます！次に、Predictive I/Oの初期の顧客による実世界のワークロードを見てみましょう。データをDatabricksにロードし、選択的なクエリを実行するというシンプルなプロセスでした。Predictive I/Oは、このお客様のユースケースにおいて、CDWよりも35倍高速でした。ここでも、優れたパフォーマンスを発揮するために必要なノブはありません。"}
{"category":"closed_qa","context":"","instruction":"Predictive I/Oはどのように機能するのでしょうか？","response":"Delta LakeとParquetテーブルに対して、Predictive I/Oは、ヒューリスティック、モデリング、ファイルプロパティに基づくスキャンパフォーマンスの予測など、さまざまな形態の機械学習と機械知能を使用して、さまざまな最適化をインテリジェントに有効または無効化します。"}
{"category":"closed_qa","context":"","instruction":"マルチストリーム処理するためにどの機能を使えますか？","response":"Delta Live Tablesを使用して、複数のストリーミングプラットフォームから同時にデータを処理することができます。"}
{"category":"closed_qa","context":"","instruction":"Workspaceにノートブックとフォルダー以外のファイルを作成や保存することができますか？","response":"はい、200MB以下のファイルをWorkspaceに作成・保存できます。"}
{"category":"closed_qa","context":"","instruction":"ワークスペース・ファイルを使用する利点は何？","response":"ワークスペース・ファイルを使用する利点は以下の通りです：\n\nモジュール化されたコードと再利用： ファイルのサポートにより、大規模で多くのセルがあるノートブックをより小さく、より理解しやすいモジュールにリファクタリングすることができます。ノートブックでは、「import」ステートメントを使用して、これらのモジュールを参照することができます。\nテスト： ノートブックやモジュールのコードに対してユニットテストを作成し、ソースコードと一緒にファイルとしてパッケージすることができます。\n初期化スクリプト： クラスタに対応した初期化スクリプトをワークスペース・ファイルに格納することができます。これらのスクリプトはアクセス制御され、許可されたユーザーだけが変更できるようになります。\nライブラリとアーティファクトの再利用： Wheel、Jar、および共有Pythonライブラリのソースファイルをノートブックに保存できるため、ノートブック内の作業の共有、配布、および複製が容易になります。\nソフトウェアの依存関係管理の改善： requirements.txtファイルにより、ワークスペース内のノートブックやその他のPythonコード資産のソフトウェア依存関係をファイルにカプセル化し、そのソフトウェア環境の将来の複製を単純な`%pip -r`呼び出しを減らすことができるようになりました。"}
{"category":"closed_qa","context":"","instruction":"ファイルやフォルダのアクセス管理はどのように行うか？","response":"オブジェクトのアクセス制御リスト（ACL）を使用して、個々のファイルまたはフォルダへのアクセスを保護します。個々のファイルまたはフォルダへのアクセスを、持つべきユーザーまたはユーザーのグループのみにアクセス権を制限することができます。ACL は、Workspace ブラウザまたはオブジェクト内部から直接制御することができます。"}
{"category":"closed_qa","context":"","instruction":"ノートブックやファイルの作成にオートコンプリート（自動入力）がサポートされているか？","response":"はい、サポートしています。入力中にオートコンプリートの候補ボックスが自動的に表示されます。"}
{"category":"closed_qa","context":"","instruction":"ファイルのバージョンの差分を表示できるか？","response":"はい、以前のバージョンのファイルを表示する際、何が変更されたかを簡単に確認できるように、サイドバイサイドの差分を表示します。"}
{"category":"closed_qa","context":"","instruction":"Spark NLPとは？","response":"Spark NLPは、Apache Spark™の上に構築された最先端の自然言語処理ライブラリです。分散環境で容易にスケールする機械学習パイプラインのために、シンプルでパフォーマンスと精度の高いNLPアノテーションを提供します。Spark NLPには、200以上の言語に対応した7000以上の事前学習済みパイプラインとモデルが付属しています。また、トークン化、単語分割、品詞タグ付け、単語と文の埋め込み、名前付き固有表現認識、依存関係解析、スペルチェック、テキスト分類、感情分析、トークン分類、機械翻訳（+180言語）、要約と質問応答、テキスト生成、画像分類（ViT）、その他多くのNLPタスクが提供されています。"}
{"category":"closed_qa","context":"","instruction":"Spark Connectとは？","response":"Spark Connectは、クライアントとサーバーを分離したアーキテクチャで、あらゆるアプリケーションからSparkクラスタへのリモート接続を可能にし、どこでも実行できるようにします。このクライアントとサーバーの分離により、最新のデータアプリケーション、IDE、ノートブック、およびプログラミング言語がSparkにインタラクティブにアクセスできるようになります。"}
{"category":"closed_qa","context":"","instruction":"Spark Connectの使用方法を教えてください。","response":"Spark Connectを使用するには、コードを変更することなく、アプリケーションに環境変数（SPARK_REMOTE）を設定するだけで拾えるようにするか、Sparkセッションを作成する際に明示的にSpark Connectをコードに含めることができます。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark 3.4でサポートされるpySpark APIは何？","response":" Spark 3.4では、Spark ConnectはDataFrame, Functions, Columnを含むほとんどのPySpark APIをサポートしています。サポートされているPySpark APIは、API referenceドキュメントで「Supports Spark Connect」と表示されるので、既存のコードをSpark Connectに移行する前に、使用しているAPIが利用可能かどうかを確認できます。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark 3.4でサポートされるScala APIは何？","response":"Spark 3.4では、Spark ConnectはDataset, functions, Columnなど、ほとんどのScala APIをサポートしています。"}
{"category":"closed_qa","context":"","instruction":"Delta Sharingとは何ですか？","response":"Delta SharingはDatabricksが開発したオープンなデータ共有プロトコルです。同じプラットフォームを使用していなくても、セキュアにデータを共有することができます。"}
{"category":"closed_qa","context":"スケジュール処理に加えて、多くのお客様は特定のイベントが生じた際にワークフローを起動したいと考えています。","instruction":"Delta Sharingのデータ共有手順を教えてください。","response":"ここでは、Databricksのデータを共有して、ローカルマシンのVSCodeから共有データを参照します。以下の３ステップで実施します。1: データの共有\n2:受信者の作成\n3:共有データの参照"}
{"category":"closed_qa","context":"","instruction":"クラウドストレージにファイルが到着した際にジョブを起動することができますか？","response":"ファイル到着トリガー」というパワフルな新機能を導入します。この機能を用いることで、クラウドストレージにファイルが到着した際にジョブを起動するように設定することができます。これらの新たなトリガーを用いることで、ワークフローはファイルが到着した際に、データを取り込み、機械学習推論や任意のタイプの分析を即座に実行することができます。この機能の活用をシンプルにするために、クラウドストレージへのアクセスを管理するUnity Catalogの外部ロケーションを活用できます。現状はファイル到着トリガーはAzureとAWSでパブリックプレビューとなっています。"}
{"category":"closed_qa","context":"","instruction":"内部結合 (INNER JOIN)とは？","response":"内部結合 (INNER JOIN)は、結合のキーとなる列の値がマッチする行同士を連結することで2つのデータフレームを結合します。キーの列の値が片方のデータフレームにしか存在しない場合は、その行は出力されません。"}
{"category":"closed_qa","context":"","instruction":"内部結合 (INNER JOIN)に使うメソッドを教えてください。","response":"内部結合 (INNER JOIN)には、データフレームのjoin()メソッドを使い、最初の引数として内部結合をしたい相手のデータフレームを指定します。"}
{"category":"closed_qa","context":"","instruction":"左外部結合 (LEFT OUTER JOIN)とは？","response":"左外部結合 (LEFT OUTER JOIN)は、結合の基準となる左側のデータフレームの行に、キーとなる列の値がマッチする右側のデータフレームの行を連結することで2つのデータフレームを結合します。左側のデータフレームのキーとなる列の値が右側のデータフレームに存在しない場合でも、左側のデータフレームの該当行は出力されます（右側のデータフレームの該当の情報はnullとして出力されます）。"}
{"category":"closed_qa","context":"","instruction":"左外部結合 (LEFT OUTER JOIN)に使うメソッドを教えてください。","response":"左外部結合の基準となる左側のデータフレームのjoin()メソッドを使い、最初の引数として相手の（右側の）データフレームを指定します。"}
{"category":"closed_qa","context":"","instruction":"クロス結合 (CROSS JOIN)とは？","response":"クロス結合 (CROSS JOIN)は、結合する両方のデータフレームの全ての行の組み合わせを出力します。結果として、両側のデータフレームそれぞれの行数を掛け合わせた数の行が出力されます。"}
{"category":"closed_qa","context":"","instruction":"クロス結合 (CROSS JOIN)に使うメソッドを教えてください。","response":"データフレームのcrossJoin()メソッドを使い、引数としてクロス結合の相手のデータフレームを指定します。結合のキーは不要なため、引数は1つだけです。"}
{"category":"closed_qa","context":"","instruction":"Unionとは？","response":"2つのデータフレームを縦方向に結合(Union結合)します。SQLのUNION ALLとは異なり重複行がある場合でも許容されます。そのため、結合結果から重複を削除したい場合はUnion結合したデータフレームに対してさらにdistinct()を実行する必要があります。また、列の結合は、列の順番のみが考慮されます。列名が同じもの同士をUnion結合したい場合はunionByName()を使う必要があります。"}
{"category":"closed_qa","context":"","instruction":"データフレームを縦方向に結合するやり方を教えてください。","response":"2つのデータフレームを縦方向に結合(Union結合)します。SQLのUNION ALLとは異なり重複行がある場合でも許容されます。そのため、結合結果から重複を削除したい場合はUnion結合したデータフレームに対してさらにdistinct()を実行する必要があります。また、列の結合は、列の順番のみが考慮されます。列名が同じもの同士をUnion結合したい場合はunionByName()を使う必要があります。\nデータフレームのunion()メソッドを使い、最初の引数としてUnion結合をしたい相手のデータフレームを指定します。"}
{"category":"closed_qa","context":"","instruction":"Databricksのレガシーグローバルinitスクリプト移行方法を知りたいです。","response":"２つのやり方があります。まず手動で移行手順は以下となります。レガシーグローバルinitスクリプトを新たなグローバルinitスクリプトに移行するには:　\n既存のレガシーグローバルinitスクリプトをコピーし、UIあるいはREST APIを用いて新たなグローバルinitスクリプトフレームワークに追加します。\nすべてのレガシーグローバルinitスクリプトを無効化します。\n新たなグローバルinitスクリプトを有効化します。\nすべてのクラスターを再起動します。"}
{"category":"closed_qa","context":"","instruction":"機械学習とは何か？","response":"機械学習は明示的なプログラミングを行うことなしにデータからパターンを学習します。特徴量をアウトプットにマッピングする関数です。"}
{"category":"closed_qa","context":"","instruction":"AutoMLにサポートしているアルゴリズムは何か？","response":"分類モデル：Decision trees、Random forests、Logistic regression、XGBoost、LightGBM。\n回帰モデル: Decision trees、Random forests、Linear regression with stochastic gradient descent、XGBoost、LightGBM \n予測モデル：Prophet、Auto-ARIMA(Databricks Runtime 10.3 ML以降で利用可能)"}
{"category":"closed_qa","context":"","instruction":"AutoMLでは分類問題における不均衡データセットをサポートしていますか？","response":"はい、サポートしています。Databricks Runtime 11.2 ML以降において、AutoMLがデータセットの不均衡を検知すると主要なクラスをダウンサンプルし、クラスの重みを追加することで、トレーニングデータセットの不均衡を削減しようとします。AutoMLはトレーニングデータセットのバランスのみを取り、テストデータセットや検証用データセットのバランスは取りません。このようにすることで、補強されておらず真のクラス分布を持つデータセットを用いて常にモデルのパフォーマンスが評価される様になります。"}
{"category":"closed_qa","context":"","instruction":"AutoMLではモデルの説明ができますか？","response":"はい、AutoMLの回帰、分類ランによって生成されるノートブックには、Shapley値を計算するコードが含まれています。Shapley値はゲーム理論に基づいており、モデルの予測に対するそれぞれの特徴量の重要度を推定します。"}
{"category":"closed_qa","context":"","instruction":"AutoMLではFeature Storeのテーブルデータを利用できますか？","response":"はい。Databricks Runtime 11.3 LTS ML以降では、分類問題あるいは回帰問題で使用するオリジナルのデータセットを拡張するために、Feature Storeにある既存の特徴量テーブルを活用することができます。\nDatabricks Runtime 12.2 LTS ML以降では、AutoMLのすべての問題: 分類、回帰、予測において、オリジナルの入力データセットを拡張するために、Feature Storeにある既存の特徴量テーブルを活用することができます。\n特徴量テーブルを作成するには、Databricks Feature Store(https://qiita.com/taka_yayoi/items/88ddec323537febf7784)をご覧ください。"}
{"category":"closed_qa","context":"Azure Databricks でのクラスター起動が Instances Unreachable エラーで失敗しました。","instruction":" Instances Unreachable エラーが出力される場合の対応策は何か？","response":"1. クラスター名の確認\n クラスター名に日本語 (マルチバイト文字) が含まれているとクラスター作成時にエラーが発生します。\nAzure Databricks でクラスターを作成するとき、デフォルトでは<ユーザー名>'s Clusterなので、ユーザー名が日本語の場合は注意する必要があります。\n2. ネットワーク構成の確認 \nDatabricks で VNet Injection を設定すると、クラスター ノードからコントロール プレーンへの接続方式に様々な選択肢があります。4 つの接続方式例でのトラブルシューティングについて説明していきます。\n3. Init Script の確認\nInit Script で hosts 等のネットワーク設定を入れているかを確認します。なお、Init Script が実行される前にコントロール プレーンへの接続が発生します。 ですので、adb-XXXXX.XX.azuredatabricks.net のプライベート エンドポイントを Init Script で追記した hosts で名前解決を行おうとしても、クラスター起動に失敗する可能性があるので注意しましょう。"}
{"category":"closed_qa","context":"","instruction":"なぜDatabricksはオープンなモデルをリリースしていますか？","response":"企業がAPIの先にあるプロプライエタリモデルを提供する集中管理されたLLMにデータ送信するのではなく、自身のモデルを構築することを好む理由は多数存在します。多くの企業において、AIによるメリットを享受できる問題やデータセットは、自分たちにとって最もセンシティブでプロプライエタリな知的財産であり、サードバーティにそれを引き渡すことは受け入れ難いことかもしれません。さらに、企業においては、モデルの品質、コスト、望まれる挙動に関してそれぞれのトレードオフを持っているかもしれません。多くのMLユーザーは、自分たちのモデルを直接所有することが長期的にベストであると我々は信じています。\n Databricks上でDollyを構築するために活用できるシンプルなDatabricksノートブックをオープンソース化します。学習された重み情報にアクセスしたいのであれば hello-dolly@databricks.com にコンタクトしてください。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogとは？","response":"Azure Databricksにおけるガバナンスソリューションです。デフォルトでは有効化されていないので、有効化する必要があります。詳細はこちらご覧ください。https://learn.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/"}
{"category":"closed_qa","context":"実際のところ、多くの企業においてデータストリーミングは非常に困難なものです。","instruction":"データストリーミング導入の課題は何か？","response":"いくつかの課題があります。1. 特殊なAPIと言語スキル: 新たな言語、API、ツールを学ぶ必要があるため、データ実践者はストリーミングのスキルセットを学ぶ際に障壁に遭遇します。\n 2.オペレーションの複雑性: 大規模なデータストリーミングを実装するには、データチームはストリーミング固有のツールとクラウドサービスをインテグレーションし、管理しなくてはなりません。また、彼らはこれらのシステムを手動で障害から復旧させ、処理の重複なしにワークロードを再起動し、パフォーマンスを最適化し、背後のインフラストラクチャをスケールさせる必要があります。\n 3.互換性のないガバナンスモデル: リアルタイムデータと履歴データに対して異なるガバナンス、セキュリティモデルは、適切なユーザーに適切なアクセスを提供することや、エンドツーエンドのリネージを参照することや、コンプライアンス要件を満たすことを困難にします。"}
{"category":"closed_qa","context":"","instruction":"Databricksレイクハウスプラットフォームはどのようにデータストリーミングをシンプルにするのか？","response":"Databricksレイクハウスプラットフォームは、データストリーミングをシンプルにする能力は３つあります。\n 1. ストリーミングパイプラインとアプリケーションを迅速に構築する能力\n 2. 自動化ツールによって簡素化されるオペレーション\n 3. リアルタイムデータ、履歴データに対して統合されたガバナンス"}
{"category":"closed_qa","context":"","instruction":"DatabricksからBigQueryへの接続設定はどうすればいいですか？","response":"DatabricksからBigqueryへの接続。Databricks(spark)ではさまざまなConnectorを用意しており、sparkで生成したDataFrameを簡単にBigQueryに書き込む OR BigQueryからtableの読み込みを行うことができます。\n手順としては簡単で \n BigQuery Storage APIの有効化 \n Service Accountの作成 \n temporary gcs bucketの作成 \n ということで簡単に行うことができます。"}
{"category":"closed_qa","context":"","instruction":"Hyperloglogとは何ですか？","response":"Hyperloglogは、webサイトのUU集計などでよく使われる確率的アルゴリズムを用いたデータのカーデナリティを推測するものになります。"}
{"category":"closed_qa","context":"","instruction":"spark-alchemyの使い方？","response":"Clusterへのinstall \n MavenからClusterにinstallします。\n library importします \n サンプルデータフレーム作成します。"}
{"category":"closed_qa","context":"","instruction":"Databricksのモデルサービングのメリットは何ですか？","response":"Databricksモデルサービングは、統合データ & AIプラットフォームで開発された初のサーバレスリアルタイムサービングソリューションです。このユニークなサービングソリューションは、デプロイメントをシンプルにし、インテグレーションされたツールにおける間違いを削減することで、データサイエンスチームのプロダクションに至るパスを加速します。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQL Statement Execution APIとは？","response":"Databricks SQL Statement Execution APIは、Databricksレイクハウスプラットフォームで管理されているでエータにアクセス、操作するためにREST API経由でDatabricks SQLのウェアハウスに接続することができるようになる機能です。\n Databricks SQL Statement Execution APIは、データへのアクセスをシンプルにし、皆様の要件に合わせたデータアプリケーションの構築を容易にします。このAPIは非同期型であり、JDBCやODBCの際に必要な接続の管理は不要です。さらに、ドライバーをインストールすることなしにSQLウェアハウスに接続できます。従来型のアプリケーションやクラウドベースのアプリケーション、サービス、デバイスをDatabricks SQLに接続するためにStatement Execution APIを活用することができます。また、お好きなプログラミング言語でカスタムクライアントライブラリを作成することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるデータウェアハウスとは？","response":"Databricksレイクハウスプラットフォームは、完全なエンドツーエンドのデータウェアハウスソリューションを提供します。Databricksレイクハウスプラットフォームはオープンな標準とAPI上に構築されています。Databricksレイクハウスは、エンタープライズデータウェアハウスのACIDトランザクションとデータガバナンスと、データレイクの柔軟性とコスト効率性を組み合わせます。Databricks SQLとは、Databricksレイクハウスプラットフォームに組み込まれている、ビジネスアナリティクスのための一般的な計算資源を提供するエンタープライズデータウェアハウスのことを指します。Databricks SQLが提供するコア機能はSQLウェアハウスとなります。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるデータモデリングとは？","response":"Databricksは、データベースのスキーマ、テーブル、ビューのような馴染みのあるリレーションを用いて、クラウドオブジェクトストレージに各王されているDelta Lakeのデータを整理します。Databricksでは、分析データの検証、クレンジング、変換にマルチレイヤーのアプローチを取ることを推奨しています。詳細は、メダリオンアーキテクチャ(https://qiita.com/taka_yayoi/items/f3d9028cf533e2c78b9d)をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLとは？","response":"Databricks SQLは、レイクハウスのテーブルに対するSQLクエリー、ビジュアライゼーション、ダッシュボードのための一般的な計算資源を提供します。Databricks SQLでは、これらのクエリー、ビジュアライゼーション、ダッシュボードはSQLエディタを用いて開発、実行されます。"}
{"category":"closed_qa","context":"","instruction":"SQLエディタとは？","response":"スキーマを探索し、馴染み深いSQL構文を用いてクエリーを記述、共有、再利用するためにビルトインのSQLエディタを活用します。定常的に使用されるSQLコードは、クイックに再利用するためにスニペットとして保存でき、実行時間を短縮するためにクエリーの結果はキャッシュされます。さらに、自動更新やデータに意味のある変化が生じた際にアラートを発呼するために、クエリーの更新をスケジュールすることができます。また、Databricks SQLによって、アナリストはクイックにアドホックな探索分析を行うために、ビジュアライゼーションやドラッグ&ドロップのダッシュボードを通じて、データから意味を抽出できるようになります。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLで利用可能なウェアハウスタイプは何か？","response":"Databricks SQLは、異なるレベルのパフォーマンスと機能サポートを持つ3つのウェアハウスのタイプをサポートしています。\n 1. サーバレス: pro SQLウェアハウスタイプのすべての機能、高度なDatabricks SQLパフォーマンス機能をサポートしています。サーバレスSQLウェアハウスタイプはデフォルトでは有効化されていません。サーバレスSQLウェアハウスタイプを有効化するには、Databricks SQL release notesをご覧ください。\n Pro: (Classicと比較して)高パフォーマンスなDatabricks SQLの追加機能をサポートしており、すべてのDatabricks SQLの機能をサポートしています。pro SQLウェアハウスタイプはデフォルトで有効化されています。\n Classic: エントリーレベルのパフォーマンス機能をサポートしており、限定的なDatabricks SQLの機能をサポートしています。Classic SQLウェアハウスタイプはデフォルトでは有効化されていません。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLのウェアハウスタイプのデフォルトは？","response":"サーバレスが有効化されている: UIあるいはAPIを用いてワークスペースでSQLウェアハウスを作成する際、サーバレスが有効化されていれば、デフォルトのSQLウェアハウスタイプはサーバレスとなります。\n サーバレスが有効化されておらずUIを使用している: UIを用いてワークスペースでSQLウェアハウスを作成する際、サーバレスが有効化されていない場合には、デフォルトのSQLウェアハウスタイプはproとなります。\n サーバレスが有効化されておらずAPIを使用している: APIを用いてワークスペースでSQLウェアハウスを作成する際、サーバレスが有効化されていない場合には、デフォルトのSQLウェアハウスタイプはclassicとなります。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLでサポートしているサードパーティBIツールは何か？","response":"Databricks SQLは、Databricksにおけるデータウェアハウスソリューションとして数多くのサードパーティBI、ビジュアライゼーションツール（https://docs.databricks.com/integrations/index.html#bi）をサポートしています。DatabricksとPowerBIやDatabricksとTableauもご覧ください。"}
{"category":"closed_qa","context":"","instruction":"カラムごとの欠損率を求める方法を教えてください。","response":"様々な方法があります。例えば、display()関数を使って、カラムごとの欠損率を求めます。カラムごとの統計情報を表示する表のmissingに欠損率が表示されます。欠損率以外にも、データ数、平均、標準偏差、最小最大やデータ分布の可視化等なども表示されるため便利です。"}
{"category":"closed_qa","context":"","instruction":"pygwalkerとは？","response":"PyGWalkerはJupyter Notebookのデータ解析とデータ可視化のワークフローを簡素化し、pandasデータフレームをTableauスタイルのユーザーインターフェースに変えて、視覚的に探索することができます。とのことです。\n 残念ながら Sparkデータフレームには対応していない模様です。Databricksにある bamboolibに近い製品かなと思いました。"}
{"category":"closed_qa","context":"","instruction":"Azure Databricks の使用量の確認方法は何ですか？","response":"Azure Consoleから Databricksサービス費用を確認する方法です。Azureサービスのコスト分析をする方法として、コスト分析 というサービスメニューがあります。\n クイック スタート:コスト分析を使用してコストを調査および分析する \n コスト分析でコストを確認するには、Azure portal でスコープを開き、メニューで [コスト分析] を選択します。 たとえば、 [サブスクリプション] に移動し、一覧からサブスクリプションを選択して、メニューから [コスト分析] を選択します。\n コスト分析で別のスコープに切り替えるには、 [スコープ] ピルを使用します。"}
{"category":"closed_qa","context":"","instruction":"DatabricksからLookerへ手動での接続方法を教えて！","response":"Lookerに接続するには以下を実行します。\n LookerでAdmin > Connections > Add Connectionをクリックします。\n ユニークな接続Nameを入力します。DialectではDatabricksを選択します。\n Remote Hostには、要件にあったServer Hostnameを入力します。\n Portには、要件にあったPortを入力します。\n Databaseには、接続を通じてアクセスしたいワークスペースのデータベース名を入力します(defaultなど)。\n Usernameにはtokenと入力します。\n Passwordには、要件にあったパーソナルアクセストークンを入力します。\n Additional Paramsには、transportMode=http;ssl=1;httpPath=<http-path>を入力し、<http-path>を要件にあったHTTP Pathを入力します。\n PDT And Datagroup Maintenance Scheduleには、PDTの再生成のデフォルト頻度を変更するために適切なcronエクスプレッションを入力します。デフォルトの頻度は5分ごとです。\n クエリーを別のタイムゾーンに変換したい場合には、Query Time Zoneを調整します。\n 残りのフィールドはデフォルトのままにします。特に:\n Max ConnectionsやConnection Pool Timeoutはデフォルトのままにします。\n (あなたがすべてをUTCで格納していることを前提として)Database Time Zoneはブランクのままとします。Test These Settingsをクリックします。\n テストが成功したらAdd Connectionをクリックします。"}
{"category":"closed_qa","context":"","instruction":"Azure Databricks と Azure Data Lake Storage Gen 2の接続方法は何か？","response":"Azure Databricks と Azure Data Lake Storage Gen 2上のデータを扱ううえでは、アクセス方法、認証方法、ネットワーク構成方法の3点が重要となるためこれを解説する。\n アクセス方法 \n Databricks では、データへのアクセス方法はテーブルアクセス or ファイルアクセス と大きく二つの方法にわかれる。 \n 認証について \n データ利用時のアクセス方式については以上の通りだが、それぞれの方式で認証方式がさらに選択可能 \n ネットワーク構成について \n対象のStorage Account がファイアウォールの背後にある場合、Azure Databricks のクラスターからのネットワークアクセスを構成する必要がある"}
{"category":"closed_qa","context":"","instruction":"Unity catalogとは？","response":"Unity Catalog は、主要なクラウド上のレイクハウスのファイル、テーブル、ML モデルなど、あらゆるデータと AI のための統合ガバナンスソリューションです。"}
{"category":"closed_qa","context":"","instruction":"Hiveとは？","response":"Apache Hiveは、Apache Hadoopのエコシステムの１つで、専門性の高いHadoopの分散処理をSQLに似た言語であるHiveQLで記述でき、使い勝手を高めることができます。\n HadoopではHDFSという分散ファイルシステムやAmazon S3などのオブジェクトストレージにあるファイルを処理対象にしていましたが、Hiveでは、「ディレクトリ」を「テーブル」に、「ファイル」を「データ」に対応させて、SQLのようにクエリでデータ処理ができます。"}
{"category":"closed_qa","context":"","instruction":"Hiveメタストアとは？","response":"テーブル定義（ファイルの構造やファイルがあるディレクトリとテーブルの対応関係）などのメタデータを保存する領域です。他には、テーブルを論理的にまとめるスキーマ2や権限情報（どのユーザ、グループがどのテーブルに何の操作をできるのか）も保存します。"}
{"category":"closed_qa","context":"","instruction":"HiveメタストアとしてのUnity Catalog？","response":"DatabricksもHiveと同様にHiveメタストアにテーブル定義を登録することで、Databricks SQLでデータを処理することができます。そのメタデータの保存場所としてUnity Catalogが使われます。\n ただし、Unity CatalogはデフォルトのHiveメタストアでない点に注意してください。\n デフォルトでは、Databricksワークスペース3に１つ、Unity CatalogではないHiveメタストアが作成されます。後述するデータガバナンスの観点や新機能の観点からUnity Catalogを利用することが推奨されています。\n AWSだと、AWS Glue Data CatalogがHiveメタストアのマネージドサービスが同様の機能を担っています。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogのアクセスコントロール（認可、監査）機能の詳細を知りたいです。","response":"Unity Catalogは、権限情報を保持しているため、認可に利用されます。また、認可の内容を記録することで監査に利用することができます。ここはイメージしやすいと思います。\n ただ、それだけではありません。\n Unity Catalogの凄さとしては、テーブルに対する認可（SQL Standard-based Authorization）モデルとファイルやディレクトリのパスに対する認可（Storage-Based Authorization）モデルを統合している点です。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogのデータカタログとは？","response":"メタデータを登録・管理し、データ利用者に公開する仕組みをデータカタログというようです。\n Databricksでも、テクニカルメタデータだけでなく、ビジネスメタデータやオペレーショナルメタデータを付与することができ、ユーザが検索、参照する機能を提供しています。\n ただし、テーブルやカラムにメタデータを付与できると言っても、現段階では、以下の例のように「Comment」欄に1024文字の文字列を記載できるだけです。"}
{"category":"closed_qa","context":"","instruction":"メタデータとはどのようなものがあるのでしょうか？","response":"DMBOK2ではメタデータを以下の３つに分類しています。\n テクニカルメタデータ: テーブルやカラムの物理名、データの型や桁、など \n ビジネスメタデータ: テーブルやカラムの論理名、データの意味や取りうる値の範囲、など \n オペレーショナルメタデータ: ジョブのログ、データ量増加パターン、削除基準、など \n Hiveメタストア機能でテクニカルデータを保存していることはお分かりだと思いますが、データカタログ機能では、テクニカルメタデータだけではなく、ビジネスメタデータやオペレーショナルメタデータも対象となります。\n また、Web UIなどでユーザが簡単にメタデータを検索や参照もできる必要があります。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでデータリネージを取得する方法は何か？","response":"メタデータの中には、データが、どこからきて、どのように加工されているのかを表すメタデータである「データリネージ」 があります。\n このデータリネージもUnity Catalogを有効化することで取得可能となります。ノートブック、ワークフロー、およびダッシュボード上で実行したデータ処理を上の図のように可視化することができ、データの来歴やデータの変更時の影響調査を容易に実施することができます。より詳細な点はマニュアルを参照ください。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでデータメッシュの実現は可能でしょうか？","response":"データメッシュは、各ドメイン毎にデータを管理を行い、他のドメインへデータの公開を行います。他ドメインからはデータの検索ができ、データの意味や使い方を理解できるようにしなければなりません。つまり、データカタログ機能が重要な役割を持ちます。\n このUnity Catalogのデータカタログ機能を活用することで、データメッシュアーキテクチャを実現することが可能となります。詳しくは以下のページをご覧ください。https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html"}
{"category":"closed_qa","context":"","instruction":"Rayとは","response":"RayはスケーラブルなAI、Pythonワークロードを実行するための有名な計算フレームワークであり、さまざまな分散機械学習ツール、大規模ハイパーパラメーターチューニング能力、強化学習アルゴリズム、モデルサービングなどを提供しています。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark™で提供している分散機械学習機能は何ですか？","response":"Apache Spark™は、Spark MLlibを通じたさまざまな分散機械学習のための高パフォーマンスアルゴリズムと、Spark MLlib、TensorFlow、PyTorchを含む機械学習フレームワークとの密なインテグレーションを提供しています。"}
{"category":"closed_qa","context":"ベストモデルを構築するために、機械学習の実践者は多くの場合、複数のアルゴリズムを探索する必要があり、RayとSparkを含む複数のプラットフォームを活用する必要が出てきます。","instruction":"DatabricksとApache SparkクラスターにおけるRayのサポートを知りたいです。","response":"RayのワークロードがDatabricksとSparkスタンドアローンクラスターでサポートされ、両方のプラットフォームでのモデル開発を劇的にシンプルにできるようになります。サポートしている機能は以下となります。\n 1: Databricks、SparkにおけるRayクラスターの作成 \n 2: Ray TrainとRay RLlibによるモデルのトレーニング \n 3: Ray Tuneによる最適モデルの発見 \n 4: Rayダッシュボードの参照"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeによる選択的なデータの上書きの詳細を知りたいです。","response":"Databricksでは選択的上書き処理で2つの異なるオプションをサポートしているDelta Lakeの機能を活用しています。\n replaceWhereは指定された述語にマッチするすべてのレコードを原子性を持って置き換えるオプションです。\n 動的パーティション上書きを用いることで、テーブルがどのようにパーティショニングされているのかに基づいてデータディレクトリを置き換えることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQL Connector for Pythonはどういう機能でしょうか？","response":"Databricks SQL Connector for PythonはDatabricksクラスターやDatabricks SQLウェアハウスでSQLコマンドを実行するために、Pythonコードを使用できるPythonライブラリです。pyodbcのようなPythonライブラリと同じように簡単にセットアップ、使用することができます。"}
{"category":"closed_qa","context":"","instruction":"小さいテーブルのパーティションを作成する必要はありますか？","response":"1TB以下のデータを持つテーブルのパーティションは作成しないことをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"テーブルにおけるパーティションの最小サイズは何ですか？","response":"すべてのパーティションには、最低1GBのデータが含まれていることをお勧めします。少数かつ大規模なパーティションを持つテーブルは、大量の小規模なパーティションを持つテーブルよりも性能が優れている傾向があります。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeのパーティションは他のデータレイクのパーティションと何が違いますか？","response":"DatabricksやDelta Lakeは、Apache SparkやParquet、Hive、Hadoopのようなオープンソーステクノロジーの上に構築されていますが、これらのテクノロジーで有用なパーティショニングの動機づけと戦略は、一般的にDatabricksで有用とは限りません。テーブルをパーティションする決定をしたのであれば、戦略を選択する前位に以下の事実を検討してください。\n パーティションの境界によってトランザクションは定義されません。Delta Lakeはトランザクションログを通じてACIDを保証するので、原子性を持つ検索を保証するために、パーティションごとにデータバッチを分割する必要はありません。\n Databricksの計算クラスターは物理的メディアに束縛されたデータ局所性を持ちません。レイクハウスに取り込まれたデータはクラウドオブジェクトストレージに格納されます。データ処理の際、データはローカルディスクストレージにキャッシュされますが、Databricksでは、並列のロードにおいて最低限のデータを特定するためにファイルベースの統計情報を使用します。"}
{"category":"closed_qa","context":"","instruction":"Z-orderとパーティションはどのように動作しますか？","response":"大規模データセットに対するクエリーをスピードアップするために、パーティションとともにZ-orderインデックスを活用することができます。\n パーティションの境界とZ-orderに基づいたクエリー最適化戦略を計画する際、以下のルールを意識することが重要となります。\n Z-orderはOPTIMIZEコマンドと連携して動作します。パーティション教会をまたがるファイルを結合することはできないので、Z-orderクラスタリングはパーティション内でのみ発生します。パーティションが作成されていないテーブルでは、テーブル全体でファイルを結合することが可能です。\n パーティションはカーディナリティの低い、あるいは既知のフィールド(日付のフィールドや物理的位置など)でのみうまく動作しますが、タイムスタンプのようにカーディナリティの高いフィールドではうまく動作しません。Z-orderは、カーディナリティの高いフィールド、無限に成長するフィールド(たとえば、トランザクション、注文テーブルにおけるタイムスタンプや顧客ID)を含むすべてのフィールドで動作します。\n パーティショニングで使用したフィールドにZ-orderを行うことはできません。"}
{"category":"closed_qa","context":"","instruction":"そんなにパーティションが悪いのであれば、なぜいくつかのDatabricksの機能はそれらを活用するのですか？","response":"パーティションは特に非常に大きなテーブルにおいては有効です。パーティショニングに関する数多くのパフォーマンス強化は、非常に大きなテーブル(数百TB以上)にフォーカスしています。\n 多くのお客様は、ParquetベースのデータレイクからDelta Lakeに移行しています。CONVERT TO DELTA文によって、既存のデータを再度書き込むことなしに、ParquetベースのテーブルをDeltaテーブルに変換することができます。このようにして、多くのお客様は以前のパーティション戦略を継承する大規模なテーブルを手にすることができます。Databricksによって開発されるいくつかの最適化処理は、可能な限りこれらのパーティションを活用しようとし、Delta Lake向けに最適化されていないパーティション戦略のいくかの副作用を軽減します。\n Delta LakeとApache Sparkはオープンソーステクノロジーです。Databricksでは、パーティションへの依存度を削減する機能の導入を進めますが、オープンソースコミュニティでは、複雑性を増加させるような新機能を構築し続けるかもしれません。"}
{"category":"closed_qa","context":"","instruction":"カスタムのパーティショニングを用いてDatabricksのビルトインの最適化処理を上回ることは可能ですか？","response":"Apache SparkやDelta Lakeの経験が豊富である何人かのユーザーは、取り込み時間クラスタリングよりも優れたパフォーマンスを提供するパターンを設計、実装することができるかもしれません。まずいパーティショニング戦略の実装は、後段におけるパフォーマンスに対する非常にネガティブな反動を引き起こす可能性があり、修正するためにはデータの完全な再書き込みが必要になるかもしれません。コストのかかる非効率性を招くことがないように、多くのユーザーにはデフォルト設定を使用することをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLとは何か？","response":"Databricks SQLは、Databricksレイクハウスプラットフォームにビルトインされているエンタープライズデータウェアハウスです。Databricks SQLのコアの提供機能はSQLウェアハウスと呼ばれる最適化計算リソースです。\n Databricks SQLは、Databricksレイクハウスプラットフォームにビルトインされているエンタープライズデータウェアハウスです。Databricks SQLのコアの提供機能はSQLウェアハウスと呼ばれる最適化計算リソースです。\n Databricksは、SQLクエリー、ビジュアライゼーション、ダッシュボードを構成、実行するためにSQLペルソナとして知られるUIツールのコレクションを提供します。\n SQLウェアハウスは、サードパーティのBI、可視化ツールを含む様々な環境で実行されるSQLクエリーに対する一般的な計算資源を提供します。また、Databricks SQLは堅牢なAPIを提供しています。\n Databricks SQLの最新機能に関しては、Databricks SQL release notesをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Spark SQLとは？","response":"Spark SQLという用語は、技術的にはSparkデータフレームを使用するすべてのオペレーションすべてに適用されます。Spark SQLはSQLクエリーのサポートとPython、Scala、R、Java用のDataFrame APIを導入したSpark 2.xにおけるSpark RDD APIを置き換えました。\n DatabricksのドキュメントではSQLのクエリーや機能を説明する際には、多くの場合Spark SQLを使用しています。Databricksランタイムの計算資源に対して実行されたSQLクエリーは、Delta Lakeに対するビルトインの拡張プロトコルと、プロプライエタリのDatabricksの機能と、オープンソースのApache Sparkの機能に密接にマッピングされます。SQL referenceで、利用可能なすべての機能と実行コンテキスト間での違いを学ぶことができます。"}
{"category":"closed_qa","context":"","instruction":"Spark SQLとDatabricks SQLの違いは何ですか？","response":"Databricks SQLは最適化された計算環境ですが、Spark SQLはApache Spark APIのコレクションを表現しています。DatabricksではSQL開発者がETLや分析のような作業を行い、ダッシュボードを作成するための様々なツールやテクノロジーを提供しています。エンタープライズデータウェアハウスに関しては、Databricks SQLをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"DatabricksにおけるSQLはオープンソースApache Sparkとどう違うのか？","response":"実行環境に関わらず、DatabricksにおけるSQLは他の場所で実行されるApache Spark SQLと大きな違いはありません。3つの異なる計算資源に対して、一般的なクエリーやステートメントを実行するために、DatabricksでSQLを使用することができます。これらの環境のすべては、オープンソースApache Sparkで利用できない数多くのコマンドを追加しており、Delta Lakeに関連するすべてのSQLクエリーを実行できるように事前設定されています。\n Unity Catalogが有効化されたワークスペースでは、数多くの追加キーワードやコマンドを利用できます。ビルトインのHiveメタストアではなく、Unity Catalogに対して実行する際、いくつかのSQLコマンドは異なる挙動をすることに注意してください。\n これらの計算資源の選択肢のそれぞれでは、若干異なるセマンティックスと文法を持っています。一般的にいうと: \n DatabricksランタイムコンピュートにおけるSQLは、オープンソースApache Sparkの処理と最も近いものになっています。\n Delta Live Tablesで実行されるSQLは、一般的にDatabricksランタイムの文法とセマンティクスを継承していますが、Delta Live Tables固有のキーワードや関数が追加されています。\n SQLウェアハウスで実行されるSQLは最もANSI標準に近いものとなっています。HiveQL、変数宣言、変数参照、DButilsウィジェットを含むDatabricksランタイムコンピュートに対してDatabricksノートブックで実行されるSQLでサポートされている多くの機能は、SQLウェアハウスでは動作しません。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるノーコードの機械学習の機能を知りたいです。","response":"Databricksは完全に自動化された機械学習機能と言われているAutoMLを提供しています。Databricks AutoMLはシチズンデータサイエンティストに対してガラスボックスアプローチを提供し、データの前処理、特徴量エンジニアリング、モデルトレーニング、チューニングのヘビーリフティングを自動化することで、チームはクイックに機械学習モデルを構築、トレーニング、デプロイすることができます。\n Databricksにおける機械学習に固有のものですが、UIで実行されたすべてのステップは、内部でプロダクションレベルのコードとして生成されます。\n エキスパートのデータサイエンティスト、機械学習エンジニアはコードを調査し、自身のカスタマイゼーションを追加することができ、規制に対応する部署は再現性や透明性が重要になるケースで、これらを参照することができます。Databricks Machine LearningはネイティブでMLflowとインテグレーションされており、前処理から特徴量エンジニアリング、トレーニング、デプロイメントに至るまで、きめ細かいエクスペリメント追跡やバージョン管理を実現します。\n 完全なリネージトラッキングや自動生成コードの登録に関するDatabricksのサポートは、皆様のデータサイエンスプロジェクトがセキュアで、法令に準拠しており、追跡可能であることを保証します。特徴量の説明可能性は、生成されたモデルにおいてどの入力が最も重要であるのかに関する洞察を提供します。これによって、ユーザー、データサイエンティスト、機械学習エンジニア、IT、リーガル、コンプライアンス部門などさまざまなチームがコラボレーションする基盤を構築することができます。"}
{"category":"closed_qa","context":"","instruction":"Apache SparkとMLflowを用いた分散ハイパーパラメータチューニング方法を知りたいです。","response":"チューニングを分散するには、fmin()の引数にSparkTrialsと呼ばれるTrialsクラスを追加します。\n 1: SparkTrialsは2つのオプションの引数を受け取ります: \n parallelism: 同時にフィット、評価するモデルの数。デフォルトは利用可能なSparkタスクのスロット数です。\n 2: timeout: fmin()を実行できる最大時間(秒数)です。デフォルトは時間制限はありません。"}
{"category":"closed_qa","context":"","instruction":"Hyperoptとは？","response":"HyperoptはハイパーパラメーターチューニングのためのPythonライブラリです。Databricks機械学習ランタイムには、最適化かつ強化されたバージョンのHyperopt、分散チューニング向けの自動MLflowトラッキング、SparkTrialsクラスが含まれています。"}
{"category":"closed_qa","context":"","instruction":"Hyperoptのワークフローを教えてください。","response":"Hyperoptのワークフローにおけるステップは以下の通りです: \n 1: 最小化する関数を定義 \n 2:ハイパーパラメーターに対する探索空間を定義 \n 3: 探索アルゴリズムを選択 \n 4: Hyperoptのfmin()を用いてチューニングアルゴリズムを実行 \n 詳細はHyperopt documentationをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"トランスフォーマーモデルとは？","response":"トランスフォーマーモデルは、これまでの手法よりも効果的かつ効率的にテキストのセマンティックを捕捉するセルフアテンションというニューラルネットワークアーキテクチャを活用しています。また、これらは、モデルの開発者によってmasked language modelingやnext sentence predictionのようなテクニックを用いて大規模テキストコーパスによってトレーニングされたことを意味する転送学習という形態を取ります。"}
{"category":"closed_qa","context":"","instruction":"Hugging Faceとは","response":"Hugging Faceはトランスフォーマーモデルを発見・アクセスしやすくすることにフォーカスしている企業です。多様なモデルやデータセットへのアクセスを提供しています。Hugging Faceによってメンテナンスされているtransformersライブラリを用いることで、お使いのDatabricksワークスペースでアーティファクトをダウンロードし、使用することができます。このライブラリはDatabricks機械学習ランタイムバージョン10.4以降に含まれており、それより新しいバージョンではpipでインストールすることができます。"}
{"category":"closed_qa","context":"","instruction":"トークナイザーは何のため使用しますか？","response":"トークナイザーはいくつかの前処理ステップを実行します。最初に、テキストをトークンに分割し、トークンをモデルのボキャブラリーにマッピングします。BERTのボキャブラリは、単語、単語の断片、数字、句読点や記号の30,522エントリーから構成されています。また、モデルには観測の開始地点([CLS])や文の区切り([SEP])のような情報を捕捉する特殊なトークンが含まれています。\n また、トークナイザーは入力シーケンスの切り取りとパディングを行います。それぞれのモデルには、許容できるトークン化シーケンスの最大長を持っています。BERTや多くの他のモデルでは、その長さは512トークンです。入力テキストをトークン化する際、最初の512トークン以降に生成されたすべてのトークンは削除、あるいは「切り取られ」ます。\n さらに、トークンのシーケンスは「パディング」されます。トランスフォーマーモデルは、一度のすべてのトレーニングデータセットに対してトレーニングするのではなく、データのバッチに対してトレーニングされます。それぞれのバッチは同じ長さである必要がありますが、観測されるテキストの長さは大きく異なります。トークン化されたあるシーケンスは512要素よりもはるかに長いかも知れませんし、他のものははるかに短いかもしれません。均一な長さにする必要がある際、パディングによってトークン化されたシーケンスにゼロが追加されます。ゼロの値はBERTのボキャブラリの特殊トークンである[PAD]を表します。\n トークナイザーの切り取りとパディングの挙動は設定可能であり、テストし比較することができる様々な戦略が存在します。トレーニング時間を短縮するために切り取りを行いますが、より長いシーケンスが一般的である場合、情報の損失によって予測パフォーマンスの妨げになることがあります。優れた汎用的な戦略としてdynamic paddingを検討してください。このテクニックはトークン化ではなくモデルトレーニングの際にシーケンスをパディングします。ダイナミックパディングは、バッチで最長の長さにそれぞれのバッチをパディングし、パディングされるトークンの数が最小になるように維持します。"}
{"category":"closed_qa","context":"トランスフォーマーモデルは、トレーニングし推論に適用するには大規模で膨大な計算量を必要とします。より最近のアーキテクチャのいくつかはさらに大規模なものとなっています。","instruction":"トランスフォーマーモデルの最適化の方法がありますか？","response":"モデルのトレーニング時間を削減し、推論をスピードアップする方法があります。\n モデルのファミリーにdistilled models(蒸留モデル)というものがあり、これは大規模なモデルである「教師」を小規模なバージョンである「生徒」に圧縮することで、モデルのサイズと計算処理の複雑度を削減します。Hugging Faceモデルハブにおいては、通常これらのモデルの名前にはdistilが含まれており、distilbert-base-uncasedといったものとなります。蒸留モデルはよりクイックにファインチューンし、大規模な教師よりも高速に推論を行うことができます。以下のエクスペリメントでは、映画のレビューと感情を含むIMDB datasetに対するモデルを比較しています。モデルサイズ、GPUメモリー消費、トレーニング時間、すべての評価用データセットのスコアリングに要した時間の大きな違いに注意してください。興味深いことに、予測パフォーマンスはモデル全体で大きな違いはありません。\n 蒸留の他に、トレーニングの設定とGPUタイプは大きなインパクトをもたらします。ファインチューニングのプロセスを管理するトランスフォーマーのTrainerクラスの設定を調整することで、トレーニングと推論の時間を劇的に削減することができます。IMDBデータセットに対してモデルのファインチューニングを行う際、バッチサイズやトレーニングにおける数値精度、勾配集積ステップに関係する設定の調整は、単一のトレーニングエポックにおけるトレーニングと推論の時間の大きな削減につながりました。\n さらに、GPUタイプの選択はトレーニングと推論の時間にインパクトを与えます。\n GPUインスタンスはトレーニングに必要で、推論も劇的に高速化しますが、GPUの推論はオプションです。CPUによる推論を加速するには量子化と蒸留モデルの使用を検討してください。量子化は、推論のレーテンシーを削減するために、高速でより精度の低い数値表現を使用します。これは、ファインチューンされたトランスフォーマーモデルに容易に適用することができます。"}
{"category":"closed_qa","context":"","instruction":"アンサンブルとは？","response":"複数のモデルを組み合わせてモデルを作ること、もしくは予測を行うことをアンサンブルといいます。\n Kaggleで勝つデータ分析の技術より \n 複数モデルの予測結果に対して平均をとるようなシンプルなアンサンブルから、スタッキングと呼ばれるモデルの予測値からさらにモデルを作る方法などがあります。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるリアルタイム特徴量計算処理のベストプラクティスがありますか？","response":"Databricks Feature StoreとMLflowを用いた最新かつ高精度なデータによる、リアルタイムモデルを提供するための最も効果的なアーキテクチャと技術を説明するためのサンプルを提供します。"}
{"category":"closed_qa","context":"","instruction":"Databricksによるリクエスト制限超過による停止への解決策を教えてください。","response":"正当なワークフローにおいて制限に到達した場合は、以下のことを行うことを推奨しています。\n ・数分後にリクエストをリトライします。\n ・計画された時間フレームで繰り返されるワークフローを均等に分散します。例えば、1時間ごとに全てのジョブをスケジュールするのではなく、1時間で異なる間隔にジョブを分散します。\n ・クラスターをより大きなノードタイプにし、ノード数を削減することを検討します。オートスケーリングクラスターを使用します。\n これらの選択肢がうまく行かない場合、コア数に対する制限増加をリクエストするためにDatabricksサポートにコンタクトしてください。\n Databricksによる他の停止理由に関しては、Termination Codeをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksクラスター起動時のクラスターのタイムアウトの解決策を教えてください。","response":"HiveのライブラリをDBFSに格納し、DBFSロケーションからローカルにアクセスしてください。Spark Options (https://docs.databricks.com/archive/external-metastores/external-hive-metastore.html#spark-options)をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksクラスター起動時のクラスターのタイムアウトエラーになる原因は何ですか？","response":"原因はいくつあります。\n 1: クラスターのタイムアウト。外部のHiveメタストアに接続しており、MavenリポジトリからすべてのHiveメタストアライブラリをダウンロードしようとする際、起動に失敗することがあります。クラスターは依存関係を含む約200個のJARファイルをダウンロードします。Databricksのクラスターマネージャがドライバーノードが5分以内に準備できたことを確認できない場合、起動に失敗します。これは、JARのダウンロードに時間がかかりすぎることによって発生します。\n 2: グローバルあるいはクラスターのinitスクリプト。クラスターの起動ステージにおいて実行されるinitスクリプトは、スクリプトをワーカーのローカルで実行するためにそれぞれのワーカーマシンにRPC(リモートプロシージャコール)を送信します。すべてのRPCはプロセスを継続する前に自分のステータスを返信しなくてはなりません。(ネットワークの問題などによって)いずれかのRPCに問題が生じて返信せず、1時間のタイムアウトになる場合があり、結果としてセットアップのジョブが失敗します。\n 3: クラスターUIでインストールしたライブラリが多すぎる。通常このエラーはクラウドプロバイダーによって引き起こされます。\n 4: クラウドプロバイダーによるシャットダウン。通常このエラーはクラウドプロバイダーによって引き起こされます。\n 5: インスタンスに到達できない(Azure)。通常このエラーはクラウドプロバイダーによって引き起こされます。特に、(新規のAzure Databricksワークスペースを起動する際に作成されるデフォルトのVNetではなく)自分のバーチャルネットワーク(VNet)にAzure Databricksワークスペースをデプロイした際に発生します。ワークスペースがデプロイされているバーチャルネットワークがすでにオンプレミスのリソースに対するExpressRoute接続とピアリングしてるあるいは接続を有している場合、Azure Databricksがクラスターを作成しようとした際、バーチャルネットワークはクラスターノードにssh接続を行うことができません。"}
{"category":"closed_qa","context":"","instruction":"dbdemosとは？","response":"Databricksのデモで使用するようなノートブックを簡単に入手することができます。\n 使い方は簡単です。Databricksノートブックで以下を実行してライブラリをインストールします。"}
{"category":"closed_qa","context":"","instruction":"Databricksワークフローにおけるdbtプロジェクトのオーケストレーションサポートを知りたいです。","response":"ワークフローにおけるdbtサポートによって、お使いのdbtプロジェクトがGitリポジトリから取得され、dbt-coreとプロジェクトの依存関係がインストールされたシングルノードクラスターが起動されます。dbtによって生成されたSQLはサーバレスSQLウェアハウスで実行され、容易なデバッグと優れたパフォーマンスを提供します。また、失敗したジョブの復旧や、dbtタスクが失敗した際にSlackやwebhookを通じたアラートの送信のような堅牢かつ運用向けの機能、Jobs APIを通じたログのようなdbtアーティファクトの取得やジョブの管理機能もあります。"}
{"category":"closed_qa","context":"","instruction":"Databricksソリューションアクセラレータとは？","response":"公式ページには以下の説明があります。\n Databricks ソリューションアクセラレータは、さまざまな業界・業種に共通の主要なユースケースに対応する、フル機能の Notebook やベストプラクティスを含む目的に特化したガイドです。\n 分析のプラットフォームだけでなく、分析のナレッジまで提供しているのはDatabricksの大きな強みだと言えます。"}
{"category":"closed_qa","context":"","instruction":"databricks.pixelsの特徴は何か？","response":"databricks.pixelsは以下の特徴があります。\n 1. DICOMデータを、SQLでアクセス可能なメタデータ、サムネイルに変換できる。\n 2. 画像処理を複数サーバで分散処理することで、何百万もの画像ファイルを処理できる。\n 3. Delta Lake & Delta Engineを分析に利用できる。つまり、ACIDトランザクションやタイムトラベルができる。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのGPU有効化クラスターの作成の方法を教えてください。","response":"GPUクラスターの作成は他のSparkクラスターの作成と同様です(Databricksクラスターをご覧ください)。以下に注意する必要があります。\n Databricks Runtime Versionは、**Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**のようにGPU有効化バージョンである必要があります。\n Worker TypeとDriver TypeはGPUインスタンスタイプである必要があります。\n Sparkを必要としないシングルマシンのワークフローにおいては、ワーカー数をゼロに設定することができます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでサポートしているGPU高速化インスタンスタイプは何か？","response":"Databricksでは以下のGPU高速化インスタンスタイプをサポートしています。\n ・P2 instance type series: p2.xlarge, p2.8xlarge, p2.16xlarge。P2インスタンスは特定のAWSリージョンでのみ使用できます。詳細については、Amazon EC2 Pricingをご覧ください。GPU有効化クラスターを起動するために、ご自身のDatabricksデプロイメントがサポートされているリージョンに存在している必要があります。P2インスタンスのオンデマンドのデフォルト制限数は1です。P2インスタンスではストレージのためのEBSが必要です。\n ・P3 instance type series: p3.2xlarge, p3.8xlarge, p3.16xlarge。P3インスタンスは特定のAWSリージョンでのみ使用できます。詳細については、Amazon EC2 Pricingをご覧ください。GPU有効化クラスターを起動するために、ご自身のDatabricksデプロイメントがサポートされているリージョンに存在している必要があります。\n ・P4d instance type series: p4d.24xlarge。P4dインスタンスではDatabricksランタイム9.1 LTS ML以降が必要です。\n ・プロダクションに機械学習モデルをデプロイすることに最適化されているG4 instance type series　\n ・画像を大量に使用する様々なユースケースや機械学習ユースケースで使用できるG5 instance type series。G5インスタンスではDatabricksランタイム9.1 LTS ML以降が必要です。\n すべてのGPU高速化インスタンスタイプでは、以下のことに留意してください。\n ・Amazonのスポットインスタンス価格の高騰によって、GPUスポットインスタンスを保持することは困難です。必要であればオンデマンドを使用してください。\n ・GPU有効化クラスターを作成するためには、制限の増加をリクエストする必要があるかもしれません。\n ・サポートされているGPUインスタンスタイプとそれらの特性の一覧については、Supported Instance Typesをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"DICOMとは？","response":"Wikipediaで調べると次のように書かれています。\n 名称は Digital Imaging and Communications in Medicine（医療におけるディジタル画像と通信） の略である。米国放射線学会（英語版）とアメリカ電機工業会が制定した規格で、異なる製造業者の医用画像機器間で画像転送を可能とすることを目的としている。\n 要するに、医療用の画像データのようです。拡張子は「dcm」。\n ダブルクリックして開けるようなものではなく、「DICOMビューア」というツールを使って画像を表示する必要があるようです。"}
{"category":"closed_qa","context":"","instruction":"PyDicomとは？","response":"PyDicomは、PythonでDICOMデータを取り扱うライブラリです。 先ほど記載した画像のようなサンプルデータも含まれています。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでサポートしているIDEは何か？","response":"VScodeをサポートしています。VS Code Extension for Databricksという機能によって、開発者はローカルでコードを記述し、IDEのパワフルなオーサリング機能を活用しつつも、リモートのDatabricksクラスターに接続することができます。IDEによって、開発者は大規模なコードベースにおいて必要となるソースコード管理、モジュール化されたコードのレイアウト、リファクタリングのサポート、インテグレーションされたユニットテストのようなベストプラクティスを活用することができます。\n これは開発プロセスでIDEを活用しているチーム向けに計画されている数多くのリリースの最初の一つとなります。Databricksでは、開発者のエコシステムに多大なる投資をしており、今後は他のIDEのサポートや追加のツールがロールアウトされます。"}
{"category":"closed_qa","context":"","instruction":"Databricksで使用できるデータソース、BIツール、開発ツールからなる大規模なラインナップにどのように接続するか","response":"様々なインテグレーションを提供しています。Partner Connect: Partner Connectは、お使いのDatabricksクラスターやSQLウェアハウスとよりクイックかつ簡単にインテグレーションするための検証済みソリューションに対するユーザーインタフェースです。\n データソース: DatabricksはCSV、Delta Lake、JSON、Parquet、XMLなどの様々なデータフォーマットや、Amazon S3、Google BigQueryやCloud Storage、Snowflakeなどのデータストレージプロバイダーのデータを読み書きすることができます。\n BIツール: Databricksでは、Power BI、Tableauなどを含むお使いのBIツールとのインテグレーションを検証しており、多くの場合、ローコードやノーコードの体験でDatabricksクラスターやSQLウェアハウスを通じてデータを操作することができます。\n その他のETLツール: すべてのタイプのデータソースに対するアクセスに加えて、Databricksではdbt、Prophecy、Azure Data FactoryのようなETL/ELTツール、Airflowのようなデータパイプラインオーケストレーションツール、DataGrip、DBeaver、SQL Workbench/JのようなSQLデータベースツールとのインテグレーションを提供しています。\n IDEとその他の開発ツール: Databricksでは、DataGrip、IntelliJ、PyCharm、Visual Studio Codeのような開発ツールをサポートしており、コードを記述することでDatabricksクラスターやSQLウェアハウスを通じてデータを操作することができます。\n Git: Databricks Reposは、お使いのGitプロバイダーとのリポジトリレベルのインテグレーションを提供するので、Databricksノートブックでコードを開発し、リモートのGitリポジトリと同期することができます。Databricks ReposによるGit連携をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Tableau CloudからDatabricksに接続する方法を知りたいです。","response":"Tableau CloudからDatabricksのデータにアクセスするには、以下のリソースと情報が必要となります。\n パーソナルアクセストークン \n SQLウェアハウス: サーバーのホスト名とHTTPパス"}
{"category":"closed_qa","context":"","instruction":"Databricksのパーソナルアクセストークンを取得したいですがどうしたらいいですか？","response":"Databricks外部からDatabricksにアクセスする際に必要になるトークンです。トークン作成権限がない場合には、Databricks管理者にこちらの手順に従って機能を有効化するよう依頼してください。\n 1: Databricksワークスペースにログインします。\n 2: サイドバーのペルソナスイッチャーからSQLを選択して、Databricks SQLにアクセスします。\n 3: トップバーのご自身のユーザー名をクリックし、ユーザー設定を選択します。\n 4: Personal access tokensタブをクリックします。\n 5: 新規トークンを生成ボタンをクリックします。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるパーソナルアクセストークンの管理をどうすればいいですか？","response":"ワークスペースでパーソナルアクセストークンを作成する機能が有効化されている場合、ワークスペース管理者は、トークンを監視し、どの非管理者ユーザーがトークンを作成、使用できるのかを管理し、新規トークンにおける最大有効期限を設定することもできます。\n ワークスペースでパーソナルアクセストークンを生成する機能が有効化されると、デフォルトではDatabricksワークスペースのすべてのユーザーがDatabricks REST APIにアクセスするためにパーソナルアクセストークンを生成することができ、無期限のトークンを含み好きな有効期限でトークンを生成することができます。\n Databricks管理者としては、よりきめ細かいレベルでトークン使用をコントロールするために、Token Management API 2.0やPermissions API 2.0を活用することができます。これらのAPIはそれぞれのワークスペースインスタンスで公開されています。どのようにAPIにアクセスし、認証を受けるのかに関しては、Databricksパーソナルアクセストークンを用いた認証をご覧ください。Databricks管理者としてAPIにアクセスする必要があります。\n いくつかのタスクに関しては、管理コンソールを活用することもできます。"}
{"category":"closed_qa","context":"","instruction":"databricksのhiveメタストアに保存されたテーブルの過去バージョンにアクセスする方法を知りたいです。","response":"databricksの標準ファイルフォーマットであるdeltaにはtime travelなる機能があり、この機能を使うと過去のバージョンのデータにアクセスできる。このdelta形式は実はhiveメタストアにも使われているので、hiveメタストアに保存されたテーブルにもtime travelは使える。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeのgenerated columnとは？","response":"Delta Lakeでは、Deltaテーブルの他のカラムに対するユーザー定義関数に基づいて自動的に生成される値を持つ特殊なタイプのカラムをサポートしています。自動生成されたカラムはgenerated columnと言います。generated columnを有するテーブルに書き込みを行う際、それらのカラムに対して明示的に値を指定する必要はなく、Delta Lakeは自動で値を計算します。例えば、(日付でテーブルをパーティショニングするために)タイムスタンプのカラムから自動で日付のカラムを生成することができます。テーブルに書き込みを行う際に必要なのはタイムスタンプのカラムのデータを指定することだけです。しかし、明示的に値を指定する際には、値は制約(<value> <=> <generation expression>) IS TRUEを満足する必要があり、そうでない場合には書き込みはエラーとなり失敗します。generated columnを持つテーブルでは、デフォルトよりも新しいテーブルライタープロトコルが必要となります。テーブルプロトコルのバージョンが何で、新しいテーブルプロトコルのバージョンを設定することが何を意味するのかを理解するためには、Delta Lakeのテーブルプロトコルのバージョン管理をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeのgenerated columnの活用方法を知りたいです。","response":"generated columnは通常のカラムと同じように格納されます。すなわち、ストレージを消費します。\n generated columnには以下の制約があります。\n ・生成のエクスプレッションには、以下のタイプの関数を除いて同じ引数を与えた際に同じ結果を常に返却するSparkにおけるSQL関数を使用することができます。\n ・ユーザー定義関数\n ・集計関数\n ・ウィンドウ関数\n ・複数の行を返す関数\n ・Databricksランタイム8.4以降でのPythonサポートでは、Delta Lakeはパーティションカラムが以下のエクスプレッションのいずれかで定義された際には常に、クエリーに対するパーティションフィルターを生成することができます。\n ・CAST(col AS DATE)およびcolの型がTIMESTAMP\n ・YEAR(col)およびcolの型がTIMESTAMP YEAR(col), MONTH(col)によって2つのパーティションカラムが定義され、colの型がTIMESTAMP\n ・YEAR(col), MONTH(col), DAY(col)によって3つのパーティションカラムが定義され、colの型がTIMESTAMP\n ・YEAR(col), MONTH(col), DAY(col), HOUR(col)によって4つのパーティションカラムが定義され、colの型がTIMESTAMP\n ・SUBSTRING(col, pos, len)であり、colの型がSTRING\n ・DATE_FORMAT(col, format)であり、colの型がTIMESTAMP\n ・上述のエクスプレッションのいずれかでパーティションカラムが定義され、生成エクスプレッションのベースとなるカラムを用いてクエリーがデータをフィルタリングする際、Delta Lakeはベースのカラムと生成されたカラムの関係性を参照し、可能であれば生成されたパーティションカラムに基づいてパーティションフィルターを生成します。例えば、以下のテーブルを考えます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのAutoMLではモデルの解釈を出せますか？","response":"はい、出せます。DatabricksのAutoMLは、そのモデルを作るためのNotebookを作ってくれます。\n また、そのNotebook内には、SHAPで特徴量重要度を計算する処理まで含んでいます。ただ、デフォルトだと計算しないように設定されているので、「shap_enabled = False」を探して有効化＆Notebookを再実行する必要があります。"}
{"category":"closed_qa","context":"","instruction":"以前のバージョンのテーブルへのクエリー(タイムトラベル)をどうすればいいですか？","response":"Delta Lakeのタイムトラベルによって、Deltaテーブルの過去のスナップショットに対してクエリーを行うことができます。\n テーブルの過去のバージョンにクエリーを行うには、SELECT文でバージョンあるいはタイムスタンプを指定します。"}
{"category":"closed_qa","context":"テーブルに対して多数の変更を加えると、大量の小規模なファイルが生成される場合があります。","instruction":"テーブルの最適化の機能を教えてください。","response":"クエリーの読み込みスピードを改善するには、小さなファイルを大きなファイルにまとめるためにOPTIMIZEを使用することができます。"}
{"category":"closed_qa","context":"","instruction":"テーブルの読み込み性能を改善するためにどうすればいいですか？","response":"様々な改善方法があります。ここでは2つの方法を紹介します。1. カラムによるZ-order。Z-Orderingによって関連する情報を同じファイルセットに共存させることができます。この局所性は読み込むべきデータの量を劇的に削減するために、Delta Lakeによるデータスキッピングアルゴリズムによって自動的に活用されます。データをZ-Orderするには、ZORDER BY句に並び替えるカラムを指定します。genderでZ-Orderするには以下を実行します。\n 2: VACUUMによるスナップショットのクリーンアップ。Delta Lakeは読み込みにおいてスナップショットのアイソレーションを提供し、これはユーザーやジョブがテーブルにクエリーを行っている際においてもOPTIMIZEを安全に実行できることを意味します。しかし、最終的には古いスナップショットを削除しなくてはならなくなります。VACUUMコマンドを実行することでこれを実現することができます。"}
{"category":"closed_qa","context":"","instruction":"ローカルのファイルをリモートのDatabricksワークスペースとリアルタイムで同期するやり方を知りたいです。","response":"Databricks Labsのdbxを活用することで、ご自身のローカル開発マシンにあるファイルの変更をDatabricksワークスペースで対応するファイルにリアルタイムで同期することができます。これらのワークスペースファイルはDBFSあるいはDatabricks Reposに格納することができます。\n dbxによるリアルタイムファイル同期(dbx sync)は、迅速なコード開発シナリオで有用です。例えば、シンタックスのハイライト、スマートなコードコンプリーション、コードリンティング、テスト、デバッグのような生産性を高める機能のためにローカルの統合開発環境(IDE)を活用することができます。そして、更新したコードをワークスペースですぐに実行することができます。\n 自動化ジョブやIDEでdbx syncを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"dbx syncの開発手順を教えてください。","response":"dbx syncの開発ワークフローには2つあり、DBFSを用いるものとDatabricks Reposを用いるものです。\n dbx syncとDBFSによる典型的な開発ワークフローは:\n 1. DBFSに同期したいファイルを格納するローカルディレクトリを特定します。\n 2. ローカルディレクトリを同期するDBFSのパスを特定します。\n 3. ローカルディレクトリをDBFSパスに同期するためにdbx sync dbfsを実行します。dbx syncがローカルディレクトリにおけるファイルの変更の監視をスタートします。\n 4. 必要に応じてローカルディレクトリのファイルを変更します。dbx syncがこれらの変更をリアルタイムでDBFSで対応するファイルに適用します。\n dbx syncとDatabricks Reposによる典型的な開発ワークフローは: \n 1. 使用できるリポジトリが無い場合には、Databricks ReposをサポートするGitプロバイダーのリポジトリを作成します。\n 2.Databricksワークスペースにリポジトリをクローンします。\n 3. ローカル開発マシンにリポジトリをクローンします。\n 4. ローカルにクローンされたリポジトリとワークスペースにクローンされたリポジトリを関連づけるためにdbx sync repoを実行します。dbx syncがローカルディレクトリにおけるファイルの変更の監視をスタートします。\n 5. 必要に応じてローカルにクローンしたリポジトリのファイルを変更します。dbx syncがこれらの変更をリアルタイムでDatabricks Reposで対応するファイルに適用します。\n 6. リポジトリがお使いのGitプロバイダーと同期されるように、ワークスペースにクローンしたリポジトリから定期的にGitプロバイダーにプッシュします。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでSynapseMLのLightGBMを使えますか？","response":"はい、使えます。Synapse MLはmicrosoftが開発した分散処理に対応した機械学習ライブラリ。LightGBMも実装されている。いつの間にかmlflowにも対応していたので、databricksにインストールしてLightGBMをmlflowでトラッキングすることができる。"}
{"category":"closed_qa","context":"","instruction":"Databricksワークスペース移行ツールは何ですか？","response":"あるワークスペースのアセットを別のワークスペースに移行する際に活用できる移行ツールがGithubで公開されています。https://github.com/databrickslabs/migrate"}
{"category":"closed_qa","context":"","instruction":"Databricksノートブックで変数エクスプローラがサポートされていますか","response":"はい、サポートしています。画面右のメニューにが{x},表示されるようになります。\nコードを実行し、変数が生成された状態で{x},をクリックして、変数エクスプローラを展開することで、変数の中身を確認することができるようになります。\n 値、データ型、形状が表示されます。なお、PySparkデータフレームに関しては、形状の計算に計算リソースを必要とするので、行数は?と表示されます。"}
{"category":"closed_qa","context":"","instruction":"ワークスペースのサブネットに関して質問です。通常、ユーザーはワークスペースにどの程度のサイズのサブネットを使用しますか？","response":"サブネットのサイズはお客様によって異なります。しかし、あなたのレイクハウスアーキテクチャで必要となるであろうApache Spark (™)ノード数を計算することで推定することは可能です。それぞれのノードでは2つのプライベートIPアドレスが必要となり、一つはトラフィックの管理、もう一つはApache Spark (™)アプリケーション用です。ワークスペースでネットマスク/26のサブネットを二つ作成すると、単一のサブネットで合計64個のIPアドレスを作成できることになり、AWSが最初の4つのIPアドレスと最後のIPアドレスを使用するので、59個が利用できることになります。これは、最大約32ノードを使用できることを意味します。最終的には、サブネットをのサイズを適切に設定するために、より小さい大量のサブネット(US-EAST-1での6サブネットなど)やより大きい少数のサブネットを経由するのかなど、皆様のユースケースから逆算することが可能です。"}
{"category":"closed_qa","context":"","instruction":"様々な開発用ワークスペースに単一の専用VPCを割り当てたい場合、これらを分割するための推奨はありますか？","response":"アドレス空間11.34.88.0/21のVPCを5つのワークスペースにブレークダウンしてみましょう。その後では、使用法に基づいて適切なワークスペースにユーザーを割り当てなくてはいけません。もし、ユーザーが数百Gバイトのデータを処理するために非常に大きいクラスターを必要としているのであれば、彼らをX-Largeに割り当ててください。もし、小規模なデータのサブセットに対してインタラクティブな分析を行うのであれば、smallワークスペースに割り当ててください。"}
{"category":"closed_qa","context":"","instruction":"ワークスペースのサブネットを他のワークスペースで使えないのはなぜですか？","response":"クラスターをデプロイやリサイズする際に、IP枯渇によってエンドユーザーがインパクトを受けないように、アカウントレベルでサブネットが重複しないことを強制しています。これが、企業のデプロイメントにおいて、同じサブネットに他のリソースを配置しないよう警告している理由です。単一のワークスペースでIPの利用を最適化するために、すべてのクラスターで自動アベイラビリティゾーン(auto-AZ)のオプションを使用することをお勧めしています。\n そして、顧客管理VPCではVPCやサブネットを移動するなど、必要に応じていつでもネットワーク設定を切り替えることができることを覚えておいてください。"}
{"category":"closed_qa","context":"","instruction":"今の所、セキュリテイグループのルールに問題はありません。しかし、Databricksのドキュメントを見たのですが、どうしてDatabricksのデプロイメントでは、インバウンドのネットワークACLサブネットルールでALLOW ALLが必要なのですか？","response":"我々はサブネットに対してデフォルトのNACLルールが使用される多層セキュリティモデルを使用していますが、EC2インスタンスに対して厳しいセキュリティグループルールによってさらにリスクを軽減することができます。さらに、多くのお客様は公衆インターネット向けのすべてのアウトバウンドアクセスを制限するために、ネットワークファイアウォールを追加しています。こちらに、不正あるいは意図しない攻撃者からデータを防御する方法を説明している、データ漏洩防御に関する記事があります。"}
{"category":"closed_qa","context":"","instruction":"我々のユーザーがトランジットVPCからコントロールプレーンのDatabricksワークスペースにルーティングされるように、PrivateLinkをフロントエンド接続でも使えますか？","response":"はい。PrivateLinkが有効化されたワークスペースを作成する際、プライベートアクセスオブジェクト、PASを作成する必要があります。PASには公衆回線によるアクセスを有効にするか、無効にするかのオプションがあります。ワークスペースのURLはインターネット上で有効ですが、PASの公衆回線アクセスがfalseに設定されており、ユーザーがVPCエンドポイント経由でルーティングされていない場合、不正なネットワークからワークスペースにアクセスしたと判断されサインインすることはできません。エンタープライズのお客様がフロントエンドPrivateLink接続の代わりに用いているのは、IPアクセスリストです。"}
{"category":"closed_qa","context":"","instruction":"適切なPrivateLink設定によって、コントロールプレーンへのルーティングをPrivateLink経由にし、EC2インスタンスが使用するインターネットゲートウェイを設けない場合、クラスターは完全にロックダウンされることになりますか？","response":"はい、可能です。しかし、依然としてS3ゲートウェイエンドポイント、Kinesisインタフェースエンドポイント、STSインタフェースエンドポイントは必要です。あと、DatabricksのデフォルトHiveメタストアや、PyPI、Mavenのようなリポジトリにアクセスできないことに注意してください。デフォルトHiveメタストアにアクセスできないので、自身のRDSインスタンスあるいはAWS Glue Catalogを使用する必要があります。そして、パッケージに関しては、ご自身の環境でリポジトリをホストする必要があります。これらの理由から、代わりにユーザーのクラスターが公衆インターネットのどの領域にアクセスできるのかを制限するために、外向け通信のファイアウォールをお勧めしています。"}
{"category":"closed_qa","context":"","instruction":"S3、Kinesis、STS、なぜこれらのエンドポイントが必要なのですか？","response":"S3エンドポイントはEC2がルートバケットに到達するために必要なだけではなく、あなたのデータを格納しているS3にアクセスするために必要となります。これによって、費用を削減し、S3へのトラフィックをAWSバックボーンに保持することで、さらなるセキュリティ層を追加することになります。Kinesisは、重要なセキュリティ情報、監査情報などを含むクラスターから収集される内部ログのためのものです。STSはEC2インスタンスに引き渡される一時的な資格情報のためのものです。"}
{"category":"closed_qa","context":"","instruction":"クロスアカウントロールは、我々の環境におけるPaaSソリューションで標準となっているので問題ありません。しかし、VPC、サブネットを制限したクロスアカウントロールのサンプルはありますか？","response":"はい、必要なリソースに対してのみクロスアカウントを制限することをお勧めしています。ドキュメント(https://docs.databricks.com/administration-guide/cloud-configurations/aws/iam-role.html#customer-managed-vpc-with-custom-policy-restrictions)でポリシーを参照することができます。他の適用できる制限として、同じページで説明されていますがソースとなるEC2 AMIを制限するということです。あと、ドキュメントでUnity Catalog(https://qiita.com/taka_yayoi/items/3f2df6ddea81521ee786#aws%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E3%82%B9%E3%83%88%E3%83%AC%E3%83%BC%E3%82%B8%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%A8iam%E3%83%AD%E3%83%BC%E3%83%AB%E3%81%AE%E8%A8%AD%E5%AE%9A)とサーバレス向けの適切なIAMロールを参照することができます。"}
{"category":"closed_qa","context":"","instruction":"AWSアカウントにあるすべてのS3バケットに対して暗号化の要件があります。顧客管理キーを用いてルートバケットを暗号化することはできますか？","response":"はい、可能です。言っていただいて助かりました。デプロイメントの一部として、あるいは後で追加する設定として、マネージドストレージやワークスペースストレージに対して顧客管理キーを追加することができます。\n ・ワークスペースストレージ: 上述したルートバケット、EBSボリューム、ジョブの実行結果、Delta Live Tablesなどを格納します。\n ・マネージドストレージ: ノートブックソースやメタデータ、パーソナルアクセストークン、Databricks SQLのクエリーを格納します。"}
{"category":"closed_qa","context":"","instruction":"AWSにおける標準的なDatabricksデプロイメントを自動化することは可能ですか？","response":"可能です。DatabricksにはTerraformプロバイダーがあります。また、POCで使用でき、Terraformスクリプトと連携できるCloudFormationテンプレートであるAWS QuickStartもあります。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogをセットアップするためのステップバイステップの手順を知りたいです。","response":"Unity Catalogを使用するためにどのようにDatabricksアカウントをセットアップするのか、どの様に最初のテーブルを作成するのかに関して、ハイレベルの概要を説明します。Unity Catalogを使えるようにDatabricksアカウントを有効化するには、以下の手順を踏んでください。\n 1. Unity CatalogがあなたのAWSアカウントにデータを格納、アクセスできるようにS3バケットとIAMロールを設定します。\n 2. 皆様の企業が利用しているリージョンのそれぞれにメタストアを作成します。このメタストアは、Unity Catalogのすべてのデータに対するトップレベルのコンテナとして動作します。メタストアの作成者として、あなたはオーナーでありメタストアの管理者となります。\n 3. メタストアにワークスペースをアタッチします。それぞれのワークスペースでは、Unity Catalogで管理するデータに対して同じビューを持つことになります。\n 4. Databricksアカウントにユーザー、グループ、サービスプリンシパルを追加します。既存のDatabricksアカウントにおいては、これらのIDは既に表示されます。\n 5. (オプション)メタストア管理者のロールをグループに移譲します。\n ユーザーに対するデータアクセスをセットアップするには、以下の手順を踏みます。\n 1.ワークスペースで少なくとも1つの計算資源、クラスターかSQLウェアハウスを作成します。Unity Catalogによって保護されるデータオブジェクトに対するgrant文を含み、クエリーやコマンドの実行にこの計算資源を使用します。\n  2. 少なくとも1つのカタログを作成します。カタログはあなたが操作するテーブルを保持するスキーマ(データベース)を保持します。\n 3. 少なくとも1つのスキーマを作成します。\n 4. テーブルを作成します。\n データ階層のそれぞれのレベル(カタログ、スキーマ、テーブル)において、ユーザー、グループ、サービスプリンシパルに対して権限を付与します。また、dynamic viewsを用いて行・列レベルのアクセス権を付与することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksで提供してるコンピュートタイプの機能比較はありますか？","response":"はい、あります。Databricksでは、それぞれが異なるタイプのワークロードに適した5つのコンピュートタイプを提供しています。\n ・ジョブコンピュート: スケーラブルかつ高信頼性の計算資源でデータエンジニアリングやMLワークロードを実行します。プロダクションワークロードで使用することを強くお勧めします。\n ・汎用コンピュート: インタラクティブなデータサイエンス、分析ワークロード向けに設計されています。\n ・SQL Pro: 高い同時実行性、低レーテンシーワークロード向けに設計された計算資源でBIや分析ワークロードを実行します。\n ・サーバレスSQL: SQL Proの全機能に加え、Databricksによって管理される最適化計算資源フリートを通じて提供される高速な起動時間とオートスケールを実現します。\n ・SQL Classic: お使いのデータレイクからタイムリーに洞察を得るために、BIレポート、分析、ビジュアライゼーションのためにSQLクエリーを実行します。"}
{"category":"closed_qa","context":"","instruction":"従来のデータウェアハウスにはないデータレイクハウスの利点は何ですか？","response":"Databricksのレイクハウスプラットフォームでは従来のDWHのようなバッチ処理に加えてストリーミングも一つのプラットフォーム上で処理できます。"}
{"category":"closed_qa","context":"","instruction":"Databricksが管理するクラスタのドライバノードとワーカーノードをホストする場所はどこ？","response":"データプレーン。コンピューティングリソースであるSparkクラスタのドライバーノードとワーカーノードはそれぞれデータプレーン（各クラウドプロバイダー内）に存在します。"}
{"category":"closed_qa","context":"","instruction":"Databricks ReposがDatabricks Lakehouse Platform上でCI/CDワークフローを促進する方法は何ですか？","response":"Databricks Repos はCI/CD プロセスを起動するために、コードの変更をコミットまたはプッシュすることができます。"}
{"category":"closed_qa","context":"","instruction":"Delta lake (デルタレイク)とは？","response":"Delta Lakeは、信頼性、セキュリティ、パフォーマンスを提供するオープンフォーマットのストレージレイヤーです。"}
{"category":"closed_qa","context":"","instruction":"SQLキーワードのうち、既存のDeltaテーブルに新しい行を追加するために使用するのは？","response":"「INSERT INTO」です。"}
{"category":"closed_qa","context":"","instruction":"なぜデータガバナンスが重要なのか？","response":"データガバナンスはデータが確実に価値をもたらし、皆様のビジネス戦略をサポートするための監視機能です。データガバナンスは、組織内のデータ資産をセキュアに管理するために実装されるポリシーとプラクティスをカプセル化します。データの量と複雑性は増加し続けており、コアとなるビジネス成果を確実なものとするために、より多くの企業がデータガバナンスを必要としています。\n 分析や機械学習の基礎としての一貫性があり高いデータ品質。\n 洞察に至る時間の短縮。\n 組織の誰もがデータドリブンの意思決定を行えるようにするデータの民主化。\n HIPAA、FedRAMP、GDPR、CCPAのような業界規制に対するリスクとコンプライアンスのサポート。\nユーザーが大規模なクラスターを起動することを防ぎ、高価なGPUインスタンスを使用する際のガードレールを作成すること等によるコストの最適化。"}
{"category":"closed_qa","context":"","instruction":"優れたデータガバナンスソリューションとはどのようなものか？","response":"データドリブンの企業は多くの場合、レイクハウス上に分析のためのデータアーキテクチャを構築しています。データレイクハウスは、データレイクに格納されている膨大な量のデータに対して直接、効率的かつセキュアなデータエンジニアリング、機械学習、データウェアハウス、ビジネスインテリジェンスを可能とするアーキテクチャです。データレイクハスのデータガバナンスは、以下のような主要な機能を提供します: \n 統合されたカタログ: 統合されたカタログは、それぞれのデータオブジェクトのメタデータに加えて、すべてのデータ、MLモデル、分析のアーティファクトを格納します。また、統合されたカタログは既存のHiveメタストアのような他のカタログのデータも組み合わせます。\n 統合されたデータアクセスコントロール: すべてのデータ資産、すべてのクラウドに対して単一かつ統合された権限モデル。これには、個人識別情報(PII)に対する属性ベースのアクセスコントロール(ABAC)も含まれます。\n データ監査: 説明可能性を高めるために、アラートとモニタリング機能を用いて集中的にデータアクセスが監査されます。\n データ品質管理: 後段のBI、分析、機械学習ワークロードで高精度かつ有用なデータが活用されるように、ビルトインの品質管理、テスト、モニタリング、強制を用いた堅牢なデータ品質管理。\n データリネージ: ソースから利用に至るレイクハウスにおけるデータフローに対するエンドツーエンドの可視性を提供するデータリネージ。\n データディスカバリー: データサイエンティスト、データアナリスト、データエンジニアが適切なデータをクイックに発見、参照し、価値創出に至る時間を短縮できるようにする簡単なデータ発見。\n データ共有: クラウド、プラットフォーム横断でデータを共有。"}
{"category":"closed_qa","context":"","instruction":"Databricksで提供しているデータガバナンス機能を教えてください。","response":"Databricksでは、Unity CatalogとDelta Sharingを用いて、データとIAに対する集中管理されたガバナンスを提供します。\n Unity Catalogは、DatabricksレイクハウスにおけるデータとAIに対する高精細なガバナンスソリューションです。データアクセスを管理し、監査する集中管理の場所を提供すすることで、データに対するセキュリティとガバナンスをシンプルにする役に立ちます。\n Delta Sharingは、使用している計算プラットフォームに関係なく、別の組織や組織内の別チームとセキュアにデータ共有するために、Databricksによって開発されたオープンプロトコルです。Unity CatalogとDelta Sharingを導入する際のベストプラクティスに関しては、Databricks Unity Catalogのベストプラクティスをご覧ください。\n すべての優れたデータガバナンスの物語は、強力なアイデンティティ基盤からスタートします。Databricksにおけるアイデンティティ設定のベストプラクティスについては、Databricksにおけるアイデンティティ管理のベストプラクティスをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogにおけるデータのクエリー方法を教えてください。","response":"Unity Catalogには3レベルの名前空間記述を採用しています。\nUnity Catalogにおいて、テーブルやビューは親のカタログやスキーマに格納されます。2つの異なる記述スタイルでテーブルやビューを参照することができます。カタログやスキーマを指定するためにUSE CATALOG文とUSE文を使用することができます。\n あるいは、3レベル名前空間記述を使うことができます。\n 3レベルの名前空間記述を用いることで、複数のカタログやスキーマのデータに対するクエリーをシンプルなものにします。\n また、<catalog_name>をhive_metastoreに設定することで、Hiveメタストアのデータに対する3レベル名前空間記述を使用することができます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのWorkspace Filesとは","response":"DatabricksワークスペースとGitリポジトリを同期できるDatabricks Repos(Repositoryの複数形を略してReposです)という機能があるのですが、このRepo(リポジトリ)配下にはノートブック以外にもPythonスクリプトやCSVファイルを格納することができます。これらのファイルをワークスペースファイル(Workspace File)と呼びます。リポジトリに格納されているノートブックからこれらのファイルを操作することができます。"}
{"category":"closed_qa","context":"","instruction":"プログラムでどうやってDatabricksワークスペースのファイルを操作するか？","response":"Databricks Reposに格納されている任意のファイルをプログラムで操作することができます。これによって、以下のようなタスクを行えるようになります。\n ノートブックやコードと一緒に小規模なデータファイルを格納する。\n Gitと同期されるディレクトリにログファイルを書き込む。\n 相対パスを用いてモジュールをインポートする。\n 環境設定ファイルを作成、修正する。\n ノートブックから出力を書き込む。\n Tensorboardのようなライブラリの実行結果を書き込む。"}
{"category":"closed_qa","context":"","instruction":"オブジェクトストレージとは？","response":"クラウドコンピューティングにおいて、オブジェクトストレージやblobストレージはデータをオブジェクトとして保持するストレージコンテナのことを指し、それぞれのオブジェクトはデータ、メタデータ、グローバルにユニークな識別子(URI)から構成されます。オブジェクトストレージにおけるデータ操作のオペレーションは多くの場合、REST APIインタフェースを通じた作成、読み込み、更新、削除(CRUD)に限定されます。いくつかのオブジェクトストレージは、バージョン管理やライフサイクル管理のような機能を提供しています。オブジェクトストレージには以下のメリットがあります: \n 1. 高い可用性、堅牢性、信頼性。\n 2. 他の多くのストレージオプションと比較して低いストレージコスト。\n 3. 無限にスケール(クラウドの特定のリージョンで利用できるストレージの総量に制限されます)。\n 多くのクラウドベースのデータレイクは、クラウドオブジェクトストレージにおけるオー文ソースデータフォーマットを基盤として構築されます。"}
{"category":"closed_qa","context":"","instruction":"Databricksではオブジェクトストレージをどのように使うのか？","response":"オブジェクトストレージは、Databricksのほとんどのオペレーションで使用される主要なストレージとなります。Databricksファイルシステム(DBFS)によって、Databricksユーザーは他のファイルシステムと同じように、オブジェクトストレージのオブジェクトのファイルを操作することができます。外部のデータシステムに対するテーブルを設定しない限り、Databricksで作成されるすべてのテーブルのデータはクラウドオブジェクトストレージに格納されます。\n クラウドオブジェクトストレージに格納されるDelta Lakeのファイルは、Databricksレイクハウスのデータ基盤となります。"}
{"category":"closed_qa","context":"","instruction":"Databricksではどのようにクラウドオブジェクトストレージを設定するのか？","response":"Databricksでは、データファイルとテーブルを格納するためにクラウドオブジェクトストレージを活用します。ワークスペースをデプロイする際、DatabricksはDBFSルートと呼ばれるクラウドオブジェクトストレージロケーションを設定します。お使いのアカウントの他のクラウドオブジェクトストレージロケーションへの接続を設定することができます。\n ほぼすべてのケースにおいて、DatabricksでApache Sparkを用いて操作するデータファイルはクラウドオブジェクトストレージに格納されます。"}
{"category":"closed_qa","context":"","instruction":"ブロックストレージとは？","response":"クラウドコンピューティングにおいて、ブロックストレージやディスクストレージは、単に「ハードドライブ」とも呼ばれる従来型のハードディスクドライブ(HDD)やソリッドステートドライブ(SSD)を指します。クラウドコンピューティング環境にブロックストレージをデプロイする際、通常は1つ以上の物理ドライブの論理的なパーティションがデプロイさmれます。製品やクラウドベンダーによって実装が若干異なりますが、多くの場合、以下のような特性が共通しています。\n すべての仮想マシン(VM)はアタッチされたブロックストレージボリュームを必要とします。\n ブロックストレージボリュームが永続化されている限り、ブロックストレージボリュームにインストールされたファイルやプログラムは永続化されます。\n ブロックストレージボリュームは多くの場合、一時的なデータストレージとして使用されます。\n VMにアタッチされたブロックストレージボリュームは通常VMとともに削除されます。"}
{"category":"closed_qa","context":"","instruction":"Databricksではブロックストレージをどのように使うのか？","response":"計算リソースを起動する際、DatabricksはVMをデプロイし、ブロックストレージボリュームをアタッチします。このブロックストレージは、計算資源の寿命における一時的データファイルの格納に用いられます。これらのファイルには、オペレーティングシステム、インストールされたライブラリ、ディスクキャッシュで用いられるデータが含まれます。Apache Sparkは効率的な並列処理やデータロードのためにバックグラウンドでブロックストレージを使用しますが、Databricksで実行されるコードのほとんどはブロックストレージに対して直接データの読み書きをしません。\n お使いのドライバーノードにアタッチされたブロックストレージを使用するPythonやBashコマンドのような任意のコードを実行することができます。ドライバーのファイルシステムのファイルにアクセスするをご覧ください。\n ワークスペースファイルが有効化されたワークスペースでは、Pythonユーザーはドライバーのブロックストレージを操作することなしに、ノートブックと一緒に格納されているファイルやデータを読み書きすることができますプログラムでDatabricksワークスペースのファイルを操作するをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでどのようにビューを作成するか?","response":"ビューはメタストアにある1つい上のテーブルやビューから構成される読み取り専用オブジェクトです。Unity Catalogの3レベル名前空間の3層目に存在します。ビューは複数のスキーマ、カタログに存在するテーブルやビューから作成することができます。\n ダイナミックビューは、行列のアクセス制御、データマスキングを提供するために使用されます。\n ビュー作成構文のサンプルは以下のようになります。\n '''CREATE VIEW main.default.experienced_employee　\n (id COMMENT 'Unique identification number', Name) \n COMMENT 'View for experienced employees' \n AS SELECT id, name \n  FROM all_employee \n  WHERE working_years > 5;'''"}
{"category":"closed_qa","context":"","instruction":"Databricks extension for Visual Studio Codeでどんな事ができるのか？","response":"Visual Studio Codeで開発したローカルコードをリモートワークスペースのコードと同期させることができます。\n Visual Studio Codeで開発したローカルのPythonコードファイルを、リモートワークスペースのDatabricksクラスタ上で実行します。\n Visual Studio Code からローカルの Python コードファイル (.py) および Python ノートブック (.py と .ipynb) を、リモートワークスペース内の自動化されたDatabricks ジョブとして実行します。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeとは？","response":"Feature Store(特徴量ストア)は、データサイエンティストが特徴量を格納、検索、共有できるようにする集中管理されたデータリポジトリです。Feature Storeは、モデルトレーニングと推論で特徴量を計算するために同じコードが使用されることを確実にします。これによって、モデル作成者がアクセスできる整理されたデータセットが生成され、彼らはトレーニングの際にも、モデルデプロイの際にも活用できることを知りながらそのデータセットを活用できるようになります。多くの企業は、Feature Storeを活用することで、実験やデプロイメントを劇的に加速したことを報告しています。例えば、AnheuserのDirector of Data EngineeringであるBusch InBevは「特徴量エンジニアリングやデータ変換の共通ソースを用いることで、我々のデータサイエンスの能力をクイックにスケールさせ、データエンジニアやアナリストたちを統合する際に、Feature Storeは大きな助けとなっています」と述べています。\n Feature Storeを使い始めるのは簡単で、主キーとタイムスタンプを持つすべてのDeltaテーブルは簡単にFeature Storeで活用することができます。Databricks Feature Storeの詳細に関してはドキュメントをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricks Feature StoreがAutoMLに組み込まれましたか？","response":"はい、モデルの成果を改善するために、クイックかつ容易にFeature Storeのデータを活用するAutoMLの新機能が発表されました。AutoMLのユーザーはモデル品質を改善するために、シンプルにAutoMLデータセットとFeature Storeテーブルをjoinできるようになります。機械学習(ML)がより高速、より簡単になると、お客様は増加する様々なユースケースに対してこの変換技術を適用できるようになります。これによって、お客様はMLを用いて収益を増やし、コストを削減するさらに多くの手段を発見できるようになります。すでに我々は、多くのお客様が重要な問題を解決するためにAutoMLを活用しているのを目撃しています。いくつかのお客様は、自身の成果を加速するためにAutoMLを活用しつつも、自身のML専門性を拡張するためにAutoMLを活用しています。AutoMLはDatabricks Feature Storeと完全にインテグレーションされます。"}
{"category":"closed_qa","context":"","instruction":"AutoMLとDatabricks Feature StoreとのインテグレーションはどのようにMLの成果を加速するのか？","response":"Databricks AutoML(AWS、Azure、GCP)は、すべてのレベルの技術専門家によるMLモデルを構築、トレーニングをサポートするために開発されました。AutoMLは高品質の候補モデルを提供するだけではなく、お客様にすべてのモデルコードをノートブックで提供しますので、お客様はモデルのパフォーマンスをさらにチューニングすることができます。\n これまでは、トレーニングセットとしてテーブルを用いてモデルをトレーニングすることができました。今では、お客様は自身のFeature Storeのデータを用いて、AutoMLトレーニングデータを拡張することで、モデルの品質を改善することができます。これによって、さらに正確なモデルを簡単にトレーニングできるようになります。Feature Storeインテグレーションを用いたAutoMLモデルは自動で特徴量のリネージをキャプチャし、新規モデルをエンドツーエンドのリネージトラッキングに追加します。このリネージによって、開発を加速し、あなたのMLOpsとコンプライアンスの要件を満たすサポートを行うツールを提供します。"}
{"category":"closed_qa","context":"","instruction":"Databricks Feature StoreをAutoMLでどう使いますか？","response":"AutoMLエクスペリメントページで、Databricksランタイム11.3 LTS ML以降を選択します。問題タイプ、データセット、予測ターゲットを選択した後、画面の左下にボタンが表示されます。このボタンを選択することで、お使いのデータセットとjoinする特徴量テーブルを選択し、joinに使用する検索キーを選択する画面が表示されます。joinしたいテーブルと検索キーが指定したら、シンプルにStart AutoMLボタンをクリックし、入力したデータと特徴量テーブルから追加されたデータの両方を用いてモデル生成がスタートされます。このサンプルでは、NYCイエロータクシー料金データを特徴量テーブルで拡張することで、モデルのフィットが21%改善されています(RMSEが3.991から3.142に減少)。AutoMLのUIにおけるこのインテグレーションだけではなく、AutoML APIでもプログラムから特徴量テーブルによるトレーニングデータの拡張がサポートされています。APIに関してはこちらを参照ください(AWS、Azure、GCP)。MLをより高速かつシンプルにすることに対する投資を継続しているので、お客様が自身のワークフローを改善している様子を見ることができて嬉しく思っていますし、チームの皆様が自身のML目標を達成するためのより多くの手段を発見することを楽しみにしています。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでどのようにクロスアカウントの外部ロケーションを作成するか？","response":"外部ロケーションとはUnity Catalogでクラウドストレージにアクセスする際に使用するオブジェクトです。Databricksワークスペースとアクセス先のS3が同じAWSアカウントであればhttps://qiita.com/taka_yayoi/items/18fee92365eee58e0b94の手順でOKなのですが、クロスアカウントの際は以下の流れです。\n 1. S3があるAWSアカウントでS3にアクセスするためのIAMロールの信頼ポリシーにUnity Catalogの信頼関係を追加。\n 2. 上記S3にアクセスするためのIAMロールのARNをコピー。\n 3. Databricksワークスペースで上記ARNを用いてストレージ資格情報を作成。上記ストレージ資格情報を用いて外部ロケーションを作成。\n という流れでした。すなわち、DatabricksがデプロイされているAWSアカウントではAWSの設定変更は不要ということです。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでどのようにマネージドテーブルを作成するか？","response":"マネージドテーブルは、Unity Catalogでテーブルを作成するデフォルトの手法となります。これらは、Deltaテーブルフォーマットを使用します。デフォルトでは、メタストアを作成する際に設定したルートストレージロケーションにマネージドテーブルは格納されます。しかし、カタログやスキーマレベルで別のストレージロケーションを指定することができます。マネージドテーブルのデータは、ロケーションが指定された階層構造の最下層のレベルに格納されます。例えば、スキーマのロケーションが指定されるとそれが使用されます。もし、スキーマロケーションが指定され無い場合、カタログのロケーションが使用され、カタログのロケーションが指定されない場合には、デフォルトメタストアのルートロケーションが使用されます。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでどのように外部テーブルを作成するか？","response":"外部テーブルは、データがルートストレージロケーションの外に格納されるテーブルです。DatabricksクラスターやDatabricks SQLウェアハウスの外のデータに直接アクセスする必要がある場合にのみ外部テーブルを使用してください。外部テーブルにDROP TABLEを実行すると、Unity Catalogは背後のデータは削除しません。マネージドテーブルと同じように、外部テーブルの権限を管理し、クエリーでこれらを使用することができます。SQLで外部テーブルを作成するには、CREATE TABLE文でLOCATIONパスを指定します。外部テーブルでは、以下のファイルフォーマットを使用することができます: DELTA, CSV, JSON, AVRO, PARQUET, ORC, TEXT。外部テーブルの背後にあるクラウドストレージへのアクセスを管理するには、ストレージ資格情報と外部ロケーションをセットアップしなくてはなりません。"}
{"category":"closed_qa","context":"","instruction":"マネージドテーブルと外部テーブル (External Table)の違いは何ですか？","response":"マネージドテーブルは、Sparkがメタデータとデータの両方を管理するSpark SQLテーブルです。マネージドテーブルの場合、DatabricksはメタデータとデータをあなたのアカウントのDBFSに格納します。Spark SQLがテーブルを管理するので、DROP TABLE example_dataはメタデータとデータ両方を削除します。外部テーブルは、データがルートストレージロケーションの外に格納されるテーブルです。Spark SQLはメタデータを管理しますので、DROP TABLE example_dataを実行した際には、Sparkはメタデータのみを削除します。データ自身は削除しません。このため、あなたが指定した場所にデータは残り続けます。"}
{"category":"closed_qa","context":"","instruction":"グローバルテーブルとローカルテーブルの違いは何ですか？","response":"グローバルテーブルは全てのクラスターからアクセスすることができます。DatabricksはグローバルテーブルをDatabricksのHiveメタストア、あるいは外部のHiveメタストア(英語)に登録します。Hiveサポートの詳細はApache Hive compatibility(英語)を参照ください。ローカルテーブルはHiveメタストアに登録されず、他のクラスターからはアクセスできません。これは一時ビューとも呼ばれます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでMETASTORE_DOES_NOT_EXISTエラーが出る際はどうしたらいいですか？","response":"Unity Catalogをセットアップすればいいのですが、もう一つの方法として「Unity Catalogを参照しに行かないクラスター」を作成するというものがあります。これは現時点ではレガシーとなりますが、上述のアクセスモードを設定しないクラスターを作成することになります。\n 1. コンピューティングを作成をクリックします。\n 2. 画面上のUIプレビューにマウスカーソルを移動します。トグルスイッチが表示されるので、これをオフにします。レガシーなクラスター作成画面に変更されます。\n 3. クラスターモードをシングルノードなどにして、クラスターを作成します。必要に応じて、インスタンスプロファイルやSpark設定を行い、S3やGlueにアクセスできるようにしておきます。\n これでMETASTORE_DOES_NOT_EXISTエラーを解消できるはずです。"}
{"category":"closed_qa","context":"","instruction":"なぜスタートアップはDatabricksで開発するのか？","response":"Abnormal Security、People.ai、YipitDataのように成功している数多くのスタートアップは、Databricksレイクハウスで製品を開発することで偉大な成功を収めています。自身のデータプラットフォームの意思決定をドライブしたいくつかの要因があります。\n 1. 妥協なしのデータ: お使いのすべてのデータ、分析、AI要件の実行. \n 2. 製品化のスピード: 皆様のチームがコアアプリケーションにフォーカスさせましょう. \n 3. 成長に備える: いかなる規模でもコスト効率の高いパフォーマンス. \n 4. オープンソースとマルチクラウドでインフラストラクチャの柔軟性を維持する. 皆様のスタートアップ製品をDatabricksレイクハウスで開発しましょう."}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでデータを読み込む際にどうやってCopy Intoを活用するか？","response":"COPY INTOでサポートされているソースやファイルフォーマットからUnity Catalogのマネージドテーブル、外部テーブルにデータを取り込む際にCOPY INTOを使用することができます。3レベルの識別子(<catalog_name>.<database_name>.<table_name>)を用いてUnity Catalogのテーブルを指定することができます。現在のクエリーやノートブックにおけるデフォルトのカタログやデータベースを設定するために、USE CATALOG <catalog_name>やUSE <database_name>を使うことができます。既存のテーブルにCOPY INTOを行いたい場合には、テーブルを格納するカタログとデータベースに対するUSAGE権限と、当該テーブルに対するMODIFY権限が必要になります。一時資格情報が設定されたクラウドオブジェクトストレージロケーションを含む、あなたがアクセスできるソースロケーションからCOPY INTOを実行することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksでどのように外部データを取り扱いますか？","response":"Databricksでは、数多くのデータロードとデータ取り込みの最適化を提供しています。また、DatabricksではSQLとデータフレームユーザーに対してクエリーフェデレーションをサポートしています。Databricksのクエリーフェデレーションとは？をご覧ください。これまでにDatabricksデータの読み書きを行ったことがないのであれば、PythonやScalaのデータフレームチュートリアルを試すことを検討ください。Apache Sparkに慣れ親しんでいるユーザーにとっても、このチュートリアルはクラウドにあるデータへのアクセスに関する新た課題に取り組むことができます。Partner Connectは数多くのエンタープライズソリューションに対して、最適化され設定が容易なインテグレーションを提供します。What is Databricks Partner Connect?をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksで使えるデータフォーマットは？","response":"Databricksには、Apache Sparkによってネイティブにサポートされているすべてのデータフォーマットに対してビルトインのキーワードバインディングがあります。Apache SparkがParquetを使うのと同じように、Databricksはデータとテーブルの読み書きにおけるデフォルトのプロトコルとしてDelta Lakeを使います。以下のデータフォーマットのすべては、Apache SparkのデータフレームとSQLにおいてビルトインのキーワード設定を有しています：Delta Lake、Parquet、ORC、JSON、CSV、Avro、Text、Binary。また、DatabricksではMLflowエクスペリメントをロードするカスタムキーワードを提供しています。"}
{"category":"closed_qa","context":"","instruction":"Databricksで画像データを処理できますか？","response":"はい、処理できます。画像はbinaryデータとしてロードすることをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Databricksで使用するクラウドストレージをどのように設定するのか？","response":"Databricksでは、データファイルとテーブルを格納するためにクラウドオブジェクトストレージを使用します。ワークスペースをデプロイメントする際、DatabricksはDBFSルートと呼ばれるクラウドオブジェクトストレージロケーションを設定します。アカウントの他のクラウドオブジェクトストレージロケーションとの接続を設定することもできます。ほぼ全てのケースにおいて、DatabricksでApache Sparkを用いて操作するデータファイルはクラウドオブジェクトストレージに格納されます。接続を設定する際のガイドとして、以下のドキュメントをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"JDBCでDatabricksに接続するデータソースは？","response":"数多くのデータソースに接続するためにJDBCを使うことができます。Databricksランタイムには数多くのJDBCデータベースのドライバーが含まれていますが、お使いのデータベースに接続するためにはドライバーや別バージョンのドライバーをインストールする必要があるかもしれません。サポートされているデータベースには以下のものが含まれます:　Query PostgreSQL with Databricks、Query MySQL with Databricks、Query MariaDB with Databricks、Query SQL Server with Databricks"}
{"category":"closed_qa","context":"","instruction":"Databricksと連携できるデータサービスは？","response":"以下のデータサービスでは、接続設定、セキュリティ資格情報、ネットワーク設定が必要となります。AWSアカウントやDatabricksにおける管理者、パワーユーザー権限が必要になるかもしれません。また、いくつかにおいてはDatabricksライブラリを作成し、クラスターにインストールする必要があります。Query Amazon Redshift with Databricks、Google BigQuery、MongoDB、Cassandra、Couchbase、ElasticSearch、Neo4j、Redis、Read and write data from Snowflake、Azure Cosmos DB、Query data in Azure Synapse Analytics"}
{"category":"closed_qa","context":"","instruction":"Databricksにどのようにデータを追加するのですか？","response":"データ追加UIを用いたデータのロードには複数の方法が存在ます。1. CSVファイルからDelta Lakeテーブルにロードするには、Upload dataを選択し、データアップロードUIにアクセスします。2. レガシーDBFSファイルアップロードを使うにはDBFSを選択します。3. 様々なデータソースへの接続を設定するためのサンプルノートブックを起動するためにその他のアイコンをクリックします。データソースの完全な一覧に関しては、Databricksにおける外部データの取り扱いをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderとは？","response":"Auto Loaderは、追加のセットアップなしにクラウドストレージにデータファイルが新たに到着すると、インクリメンタルかつ効率的に処理を行います。Auto Loaderは、cloudFilesという構造かストリーミングソースを提供しています。"}
{"category":"closed_qa","context":"","instruction":"ETLの自動化をどうすれば良いですか？","response":"Auto LoaderとDelta Live Tablesを用いることで、スケーラブル、インクリメンタルなデータ取り込みのインフラストラクチャのデプロイメントをシンプルにすることができます。Delta Live Tablesではノートブックのような標準的なインタラクティブな処理の実行ではなく、プロダクションに使えるインフラストラクチャのデプロイメントにフォーカスしています。"}
{"category":"closed_qa","context":"","instruction":"いつCOPY INTOを使い、いつAuto Loaderを使うのか","response":"以下にAuto LoaderとCOPY INTOを選択する際に検討すべきいくつかの事柄を示します:　数千規模のファイルを取り込もうとしているのであればCOPY INTOを使うことができます。一定期間で数百万以上のファイルを予想しているのであれば、Auto Loaderを使ってください。Auto Loaderは、COPY INTOと比較してファイルを検知するオペレーションの総数が少なくて済み、処理を複数のバッチに分割することができ、Auto Loaderは低コストで規模に対して効率的であることを意味します。データスキーマが頻繁に進化するのであれば、Auto Loaderはスキーマ推定、進化に関するより良いプリミティブを提供しています。Auto Loaderにおけるスキーマ推定とスキーマ進化の設定をご覧ください。再度アップロードされたファイルのサブセットのロードにおいては、COPY INTOの方が管理が若干簡単です。Auto Loaderを用いる場合、ファイルのサブセットのSELECTを再処理することがより困難となります。しかし、Auto Loaderストリームを同時に実行しながらも、ファイルのサブセットを再ロードするためにCOPY INTOを使うことができます。"}
{"category":"closed_qa","context":"","instruction":"私のアプリケーションはどのように稼働している？","response":"アプリケーションのモニタリングは以下の点に応じて異なるレベルで実行されることになります: 1. お使いのアプリケーションで収集されるメトリクス(バッチの処理時間/レーテンシー/スループットなど), 2. どこからアプリケーションをモニタリングしたいのか。最もシンプルなレベルでは、様々なシチュエーションで活用できるSpark UIに直接ビルトインされているストリーミングダッシュボード(新しい構造化ストリーミングUI)があります。ストリーミングワークロードを実行するジョブの失敗時にアラートを発生させることもできます。より詳細なメトリクスが必要、あるいは、お使いのコードベースの一部としてこれらのメトリクスに基づくカスタムアクションを作成したい場合には、StreamingQueryListenerがあなたが探しているものに適しています。(ドライバーやワーカーのマシンレベルのトレースを含む)Sparkのメトリクスをレポートさせたいのであれば、プラットフォームのメトリクスシンクを使うべきです。検討すべきもう一つの事柄は、観測のためにこれらのメトリクスをどこに表示させたいのかということです。クラスターレベルでのGangliaダッシュボード、ストリーミングワークロードのモニタリングのためのDatadogのようなパートナーアプリケーションのインテグレーション、PrometheusやGrafanaのようなツールを用いたよりオープンソースなオプションがあります。それぞれには、コスト、パフォーマンス、メンテナンス要件に関する利点、欠点があります。UIによるインタラクションで十分な低ボリュームのストリーミングワークロードを使用していようが、より堅牢なモニタリングプラットフォームに投資することを決断しようが、皆様のプロダクションストリーミングワークロードの観測方法を理解する必要があります。このシリーズで今後ポストされる「モニタリングとアラート」では、より詳細な議論が含まれることになります。特に、ストリーミングアプリケーションをモニタリングするための様々な計測方法や観測性を活用するためのいくつかのツールを詳細に見ていきます。"}
{"category":"closed_qa","context":"","instruction":"自分のアプリケーションはリソースを効果的に活用しているのか？","response":"開発者として、我々は作業中のコードと適切に記述されたコードの違いを理解しています(あるいはすぐに学びます)。コードの実行形態を改善することは非常に楽しいものですが、最終的に問題になるのは全体的な実行コストです。構造化ストリーミングアプリケーションにおけるコストの検討は、他のSparkアプリケーションと概ね同じようなものとなります。特筆すべき違いは、これらのワークロードは多くの場合「常時稼働」のアプリケーションなので、プロダクションワークロードの最適化に失敗すると、非常に高コストになり、無駄な出費がすぐに累積していくという点です。コスト最適化の支援は頻繁にリクエストされるので、これに関する別の記事をポストする予定です。フォーカスするキーポイントは使用の効率性とサイジングです。クラスターサイズを適切に設定することは、ストリーミングアプリケーションにおいて効率性と無駄の違いを生み出す最も重要な違いとなります。時にはプロダクションにおけるアプリケーションの完全な負荷条件をデプロイする前に推定することが困難であるため、これは特にトリッキーになります。別のケースでは、日、週、月を通じて取り扱われるボリュームの変動性によって困難となる場合があります。最初にデプロイする際は、パフォーマンスのボトルネックを避けるために追加の費用を投じて若干サイズを大きめにすることをお勧めします。適切なクラスター使用率を確認するために数週間クラスターが稼働した後に、お使いのモニタリングを活用してください。例えば、負荷のピークの際にCPUやメモリーの使用率は高いレベルにあるのか、あるいは、定常的に負荷が低くてクラスターのサイズを引き下げて良いのか？これに対する定期的なモニタリングを継続し、データボリュームの時系列変化を注視します。いずれかが発生している場合、コスト効率の高いオペレーションを維持するためにクラスターのサイズ変更が必要になるかもしれません。一般的なガイドラインとして、過度なシャッフルオペレーションやjoin、極端なウォーターマークの閾値(ご自身の要件を超えないでください)はアプリケーションを実行するために必要なリソースを増加させるので、これらを避けるべきです。大きなウォーターマークの閾値は、構造化ストリーミングにバッチ間の状態ストアに多くのデータを保持させることになり、クラスターのメモリー要件を引き上げることになります。また、設定するVMのタイプにも注意してください - メモリーを大量に必要とするストリームにメモリー最適化VMを使っていますか？計算が膨大に発生するストリームに計算最適化VMを使っていますか？そうで無い場合には、それぞれの使用率を確認し、より適したマシンタイプを試すことを検討してください。クラウドプロバイダーから提供される新たなサーバーファミリーでは、より最適化されたCPUが搭載されており、多くの場合において処理実行の高速化につながり、これはSLAを達成するために少ない台数で済むことを意味します。"}
{"category":"closed_qa","context":"","instruction":"デプロイメント後発生する問題にどのように対応すべきか？","response":"コスト最適化と同じように、Sparkにおけるストリーミングアプリケーションのトラブルシュートは内部の機構はほとんど変わらないので、他のSparkアプリケーションと多く変わりません。ストリーミングアプリケーションにおいては、問題は通常、障害シナリオとレーテンシーシナリオという2つのカテゴリーに属することになります。"}
{"category":"closed_qa","context":"","instruction":"なぜネイティブSQL？","response":"理由が３つあります。1. 複雑な変換処理による柔軟性を提供します。ネイティブSQLはユーザーに対して、特定の変換に対して理想的なツールを選択する柔軟性を提供します。DAXは表現豊富でパワフルですが、より複雑なSQLクエリーやミサポートのオペレーションに対しては、ネイティブなSQLクエリーによってよりシンプルなワークフローを実現することができます。また、これはカスタムカラム定義を折りたたんだり、プッシュダウンでいないケースにおいては、単にネイティブSQLを構築することで大きなメリットを得ることができます。2. Databricks SQLの計算資源のさらなる活用が可能になります。3. 他のPower Queryソースからの移行をシンプルにできます。Power BIで他のデータソースとカスタム/ネイティブSQLを使用しているお客様においては、Databricks SQLへの移行は面倒なものでした。これまでのソリューションは、これをDatabricksのビューやDAXのロジックにマッピングしようとしていました。今では、これはDatabricks Power BIコネクターで適切なネイティブSQLを指定するだけで済むシンプルなものとなっています。"}
{"category":"closed_qa","context":"","instruction":"Databricksの課金データをどのようにクイックに集計するか？","response":"以下の流れでクイックに集計ができます。1. アカウントコンソールにログインし、ログデータのダウンロードを行う。2. CSVデータの集計。使用量ログのデリバリーを設定し、集計処理をジョブ化するなどして全てを自動化することもできます"}
{"category":"closed_qa","context":"","instruction":"JEDAIとは？","response":"Japan Enduser Group | Databricks Innovationの略であり、データブリックス・ジャパン株式会社が運営するユーザーコミュニティのコミュニティです。"}
{"category":"closed_qa","context":"","instruction":"Databricks Account Consoleで何が出来るか？","response":"1. SKU/Workspace/タグ　ベースで表示ができる 2.フィルタリングも可能, 3.表示期間を指定出来る 4. 結果をCSVにダウンロード出来る 5. DBUもしくは $USD で表示できる"}
{"category":"closed_qa","context":"","instruction":"Z-orderingとは？","response":"Z-orderingは、関連する情報を同じファイルセットに配置するテクニックです。この局所性は、自動でDatabricksのDelta Lakeによるデータスキッピングアルゴリズムによって使用されます。この挙動によって、DatabricksのDelta Lakeの読み込むデータ量を劇的に削減します。データをZ-orderするには、ZORDER BY句で並び替えるカラムを指定します。あるカラムが頻繁にクエリー条件に指定され、そのカラムが高いカーディナリティを持っている(すなわち、大量の固有値を持つ)のであればZORDER BYを使いましょう。カンマ区切りで複数のカラムをZORDER BYに指定することができます。しかし、カラムを追加するごとに局所性の効果は失われます。統計情報が収集されていないカラムに対するZ-orderingは効果がなく、リソースの無駄となります。これは、データスキッピングでは、min、max、countのようなカラムローカルの統計情報が必要となるためです。スキーマでカラムを再度並び替える、あるいは統計情報を収集するカラムの数を増やすことで、特定のカラムに対して統計情報を収集するように設定することができます。"}
{"category":"closed_qa","context":"","instruction":"Z-orderingの注意事項は何か？","response":"Z-orderingの注意事項に2点あります。1. Z-orderingは冪等性がありませんが、インクリメントなオペレーションで使用することができます。複数回実行してもZ-orderingに要する時間が削減することは保証されません。しかし、Z-orderされたばかりのパーティションに新規データが追加されない場合、そのパーティションを再度Z-orderingしても効果はありません。2. Z-orderingはタプルの数が均等にバランスされたデータファイルを生成しようとしますが、ディスク上のデータサイズが均等になるとは限りません。これら2つの指標は多くの場合相関しますが、そうではない場合もあり最適化タスクの処理時間に偏りが生じることがあります。例えば、ZORDER BY dateを行い、最新のレコードがすべて前回のZORDERのものよりも幅が広い(長い配列や文字列など)場合、OPTIMIZEジョブのタスク時間や結果ファイルのサイズに偏りが生じることになります。しかし、これはOPTIMIZEコマンド自身のみの問題です。以降のクエリーにネガティブなインパクトを起こすことはありません。"}
{"category":"closed_qa","context":"","instruction":"Databricks Certified Machine Learning Professionalの対策は？","response":"1. Scalable Machine Learning with Apache Spark Japaneseのノートブック(日本語)を読み込むことをやりました. Scalable Machine Learning with Apache Spark Japaneseには解説のスライドが付属しています。スライドではデータサイエンスの基礎から解説されているので、初学者でも学び始めることができます。2. ML in production JapaneseのDatabricksにおける機械学習モデルの運用に焦点を当てて解説されているNotebookで練習しました。実験管理やモデル管理はScalable Machine Learning with Apache Spark Japaneseとオーバーラップする内容が多いです。新しい内容としてはRESTエンドポイントへのデプロイ、テスト自動化、モニタリングなどが解説されています。"}
{"category":"closed_qa","context":"","instruction":"ノートブック間でどのようにコードを共有するか？","response":"Databricksでは、ノートブック間でコードを共有するための方法をいくつかサポートしています。それぞれの方法においては、コードのモジュール化を実現し、ライブラリのようにノートブックでコードを共有することができます。また、Databricksでは依存関係を持つパイプラインや戻り値に基づくif-then-elseワークフローのような複雑なワークフローにノートブックを組み込むこともできます。方法１：ノートブックをインポートするために%runを使う。方法２：ソースコードファイルを参照するためにDatabricks Reposを使用する。"}
{"category":"closed_qa","context":"","instruction":"ノートブックのテストの方法を教えてください。","response":"1. 多くのユニットテストのライブラリはノートブック内で直接動作します。例えば、ノートブックのコードをテストするためにビルトインのPythonの`unittest`パッケージを使用することができます。セルの出力エリアにテスト失敗が表示されます。単一のノートブックにおいてテスト用の呼び出しなのか、通常の呼び出しなのかを区別するためにウィジェットを使うことができます。2. ノートブックからテストコードを分離する。%runを使った例です。"}
{"category":"closed_qa","context":"","instruction":"DatabricksジョブのUIでどのように失敗したタスクを特定するか？","response":"DatabricksジョブのUIで失敗したタスクを特定するには: 1. サイドバーのワークフローをクリックします。2. 名前の列でジョブ名をクリックします。ジョブの実行タブにはアクティブなジョブ実行、失敗したものを含む完了したジョブ実行が表示されます。3. マトリックススビューに切り替えるには、マトリックスをクリックします。魔トリックビューには、それzれおのジョブタスクの成功、失敗を含むジョブの実行履歴が表示されます。マトリックスビューを用いることで、ジョブ実行で失敗したタスクをクイックに特定することができます。4. 関連メタデータを確認するために失敗したタスクの上にマウスカーソルを移動します。メタデータには、開始日、終了日、期間、クラスター詳細、エラーメッセージが含まれています。5. 失敗の原因の特定を行うために、失敗したタスクをクリックします。タスク実行の詳細ページが開き、タスクの出力、エラーメッセージ、メタデータが表示されます。"}
{"category":"closed_qa","context":"","instruction":"ジョブが失敗したら最初から再実行しなければなりませんか？","response":"いいえ。Databricksジョブのリペア、リラン機能によって、失敗したタスクやスキップされたすべての後段のタスクを再実行することができます。"}
{"category":"closed_qa","context":"","instruction":"マネージドテーブルとアンマネージドテーブルと外部テーブルがどう違うか？","response":"マネージドテーブル は、ユーザーがテーブル名だけを使ってデータ(Sparkデータフレーム)を永続化する方法で、データの保存先(ストレージとパス)はSparkシステム側で自動的にアサインされます。つまり、ユーザーは保存先などストレージ側を意識する必要がありません。データを削除する場合も、DROP TABLE文を実行するだけで済み、テーブルのメタデータの削除と、データ実体の削除が同時に実行されます。データの保存先が自動管理されるので 「マネージド」テーブル と呼びます。一方、アンマネージドテーブルは、ユーザーがデータの保存先パスを指定して永続化し、後からテーブル名とデータ保存先をリンクします。この場合、データの削除には、DROP TABLE文でのテーブル(のメタデータ)削除に加えて、データ保存先のデータ実体を削除する必要があります。つまり、ユーザーはテーブル(メタデータ)とデータ実体の保存先の両方を意識する必要があり、よって、「アンマネージド」テーブルと呼ばれています。この方法だと、Sparkシステム管理のストレージ外上に置かれているデータに対してもテーブル名とリンクができるため、この意味で、アンマネージドテーブルは「外部テーブル(External Table)」 とも呼ばれます。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるユーザー定義関数(UDF)とは何か？","response":"ユーザー定義関数(user-defined-function:UDF)はユーザーによって定義される関数であり、ユーザー環境でカスタムロジックを再利用できるようになります。拡張可能なロジックを分散処理できるようにするために、Databricksでは様々な種類のUDFをサポートしています。"}
{"category":"closed_qa","context":"","instruction":"カスタムロジックはどのような場合にUDFではないのか？","response":"厳密にはすべてのカスタム関数がUDFという訳ではありません。SQLやSparkデータフレームを用いて一連のSparkビルトインメソッドを安全に定義することができ、完全に最適された挙動をさせることができます。例えば、以下のSQL、Python関数は再利用可能な関数として単位変換を定義するためにSparkのビルトインメソッドを組み合わせています。"}
{"category":"closed_qa","context":"","instruction":"どのUDFが最も効率的なのか？","response":"UDFはコード実行において多大なる処理ボトルネックを持ち込む場合があります。Databricksでは、Apache Spark、SQL、Delta Lakeの構文を用いて記述されたコードに対して自動で様々なオプティマイザを適用します。UDFによってカスタムロジックが導入されると、これらのオプティマイザはこのカスタムロジックに対してタスクを効率的に計画する能力を発揮することができません。さらに、JVM外で実行されるロジックにおいては、データのシリアライズにおける追加のコストが発生します。いくつかのUDFは他のものより効率的です。パフォーマンスの観点では:　1. Databricksのオプティマイザのおかげでビルトイン関数が最も高速です。2. JVMで実行されるコード(Scala、Java、Hive UDF)はPythonのUDFよりも高速です。3. Pandas UDFはPython UDFに関連づけられるシリアライズ残すとを削減するためにArrowを活用します。4. 通常はPython UDFは避けるべきですが、パフォーマンス劣化なしに、コードを結びつけるためにPythonを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"いつUDFを使うべきか？","response":"UDFの主要なメリットは、慣れ親しんだ言語でロジックを表現し、コードのリファクタリングに伴う人間のコストを削減できるというものです。アドホックなクエリー、主導のデータクレンジング、探索的データ分析、小中規模のデータセットに対する大部分のオペレーションにおいては、UDFによるレーテンシーのオーバーヘッドがコードのリファクタリングに関連するコストを上回ることはほとんどありません。ETLジョブ、ストリーミングオペレーション、超大規模なデータセットに対するオペレーション、その他の定常的、連続的に実行されるワークロードにおいては、ネイティブなApache Sparkのメソッドを用いるためにロジックをリファクタリングするコストはすぐに取り戻すことができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksにデータベースはあるのか？","response":"コンピューティングにおいて、データベースは、電子的に保存され、アクセスできる組織化されたデータの集合である。この定義に沿って考えれば、答えはYESです。しかし、OracleやSQL Server、MySQL、Snowflakeのようなデータベース管理システム(DBMS)がインストールされているのか？と聞かれると答えはNOとなります。これらのDBMSやデータウェアハウスとDatabricksにおけるデータベースは実装が異なります。"}
{"category":"closed_qa","context":"","instruction":"なぜDatabricksにおけるデータベースの実装が他のデータベース管理システムやデータウェアハウスと異なるのでしょうか。","response":"まず、データ管理プラットフォームとしてデータウェアハウス(個人的にはデータベースと同義だと思っています)が1990年台に台頭し、多くの企業で導入されました。ビジネス上の意思決定を行うためにデータを蓄積し、BIなどを通じて意思決定にデータが活用されるようになりました。データウェアハウスの特徴をざっくり列挙します。1. 行と列で構成される構造化データを蓄積, 2. SQLによる問い合わせ、加工が可能 3. (比較的)大量のデータを管理可能。これらの特徴は現在においても多くの企業のデータ活用に役立っており、ある意味成熟したテクノロジーと言えます。しかし、ここ数年、このデータウェアハウスでは応えられない要件が出現しています。そう、機械学習やAIの活用です。データレイクの台頭。BIやSQLを用いた分析など、過去の構造化データを蓄積し、活用するというシーンにおいてはデータウェアハウスは非常に有用です。一方、機械学習モデルをトレーニングし、将来の予測に活用する、画像やテキストを分析するという要件にデータウェアハウスは応えることができませんでした。そのため、機械学習・AIのニーズに応えることのできる新たなデータ管理プラットフォームが求められました。これによって、2010年頃にデータレイク誕生しました。データレイクの特徴をざっくり列挙します。1. クラウドサービスのオブジェクトストレージにデータを格納, 2. 構造化データ、非構造化データに対応 3. 理論上、容量は無制限 4. 機械学習のワークロードとの親和性が高い。これによって、データレイクを活用し始めた人々は機械学習・AIによる明るい未来を夢想しました。しかし、世の中はそんなに甘くありません。上述の特徴は確かに機械学習の取り組みにおいては非常に有用なものでありましたが、企業の取り組みにおける重要な観点が欠落していました。パフォーマンスとガバナンスです。オブジェクトストレージに無尽蔵にデータを格納できたとしても、検索・更新に非常に時間を要してしまい、さらには、管理がなされないためゴミだらけでのデータレイクになってしまうということが頻発しました。結果として、データレイク(湖)ならぬデータスワンプ(沼)とまで揶揄されるようになってしまいました。レイクハウスの誕生。今の機械学習・AIのニーズに応えきれないから(データ)レイクハウスが開発されたということです。レイクハウスではデータはオープンなフォーマットでクラウドストレージに格納します。これによって、ベンダーロックインの回避、無限のストレージ、機械学習ワークロードとの親和性といったことを実現します。さらに、データウェアハウスで培われたパフォーマンスやガバナンスをも実現しているのです。"}
{"category":"closed_qa","context":"","instruction":"DBMSやデータウェアハウスとDatabricksにおけるデータベースの違いは何か？","response":"まず、データ管理プラットフォームとしてデータウェアハウス(個人的にはデータベースと同義だと思っています)が1990年台に台頭し、多くの企業で導入されました。ビジネス上の意思決定を行うためにデータを蓄積し、BIなどを通じて意思決定にデータが活用されるようになりました。データウェアハウスの特徴をざっくり列挙します。1. 行と列で構成される構造化データを蓄積, 2. SQLによる問い合わせ、加工が可能 3. (比較的)大量のデータを管理可能。これらの特徴は現在においても多くの企業のデータ活用に役立っており、ある意味成熟したテクノロジーと言えます。しかし、ここ数年、このデータウェアハウスでは応えられない要件が出現しています。そう、機械学習やAIの活用です。データレイクの台頭。BIやSQLを用いた分析など、過去の構造化データを蓄積し、活用するというシーンにおいてはデータウェアハウスは非常に有用です。一方、機械学習モデルをトレーニングし、将来の予測に活用する、画像やテキストを分析するという要件にデータウェアハウスは応えることができませんでした。そのため、機械学習・AIのニーズに応えることのできる新たなデータ管理プラットフォームが求められました。これによって、2010年頃にデータレイク誕生しました。データレイクの特徴をざっくり列挙します。1. クラウドサービスのオブジェクトストレージにデータを格納, 2. 構造化データ、非構造化データに対応 3. 理論上、容量は無制限 4. 機械学習のワークロードとの親和性が高い。これによって、データレイクを活用し始めた人々は機械学習・AIによる明るい未来を夢想しました。しかし、世の中はそんなに甘くありません。上述の特徴は確かに機械学習の取り組みにおいては非常に有用なものでありましたが、企業の取り組みにおける重要な観点が欠落していました。パフォーマンスとガバナンスです。オブジェクトストレージに無尽蔵にデータを格納できたとしても、検索・更新に非常に時間を要してしまい、さらには、管理がなされないためゴミだらけでのデータレイクになってしまうということが頻発しました。結果として、データレイク(湖)ならぬデータスワンプ(沼)とまで揶揄されるようになってしまいました。レイクハウスの誕生。今の機械学習・AIのニーズに応えきれないから(データ)レイクハウスが開発されたということです。レイクハウスではデータはオープンなフォーマットでクラウドストレージに格納します。これによって、ベンダーロックインの回避、無限のストレージ、機械学習ワークロードとの親和性といったことを実現します。さらに、データウェアハウスで培われたパフォーマンスやガバナンスをも実現しているのです。"}
{"category":"closed_qa","context":"","instruction":"Pysparkでどのように増分データ内の重複を削除するか？","response":"PySparkにもPandasと同じく、dropDuplicates関数があります（drop以降をアンスコで繋いでいないという差はある）。ただし、keep引数がない！よって、重複があるデータのうちどれを残すべきか？PySparkでは、この関数だけでは指定できないことになります。やっていることは、Pandasではkeep引数でやってくれていたことを、PySparkではorderBy,coalease関数を組み合わせて代用するというもの。データの新旧を、レコードが持つ更新日付カラムを降順に並び替えることで判断し、coalesce関数で一番上（つまり、最新データ）だけ残す。あとはdropDuplicate関数で、最新データ以外を削除する。"}
{"category":"closed_qa","context":"","instruction":"ExcelからどうやってDelta Sharing経由でデータにアクセスするか？","response":"まずはExcelアドインのインストールが必要です。そして、Delta Sharingの設定を行います。Excelアドオンで操作します。"}
{"category":"closed_qa","context":"","instruction":"SparkでどのようにExcelファイルを扱うか？","response":"SparkでExcelファイルを扱うためにspark-excelライブラリを利用します。ExcelファイルをSparkデータフレームとして読み込んだり、また逆に出力したり、さらには既存のExcelファイルの特定の部分にSparkデータフレームのデータを上書きして保存するということもできます。Spark自体がビッグデータを扱うものなので、可視化という意味ではBIツールを使うことがほとんどで、Excelを使うことは多くはないと思いますが、どうしても使わなければいけない場合はこのライブラリを使うことで簡単に実現できます。"}
{"category":"closed_qa","context":"","instruction":"SparkデータフレームをどうやってExcelファイルとして出力するか？","response":"データフレームを以下のようにして、save()で指定したファイルパスにExcelファイルを出力します。format()で com.crealytics.spark.excelを指定します。( df_output.write.format( 'com.crealytics.spark.excel' ).option( 'header', 'true' ).mode( 'overwrite' ).save( xlsx_file_path ) )"}
{"category":"closed_qa","context":"","instruction":"ExcelファイルをどのようにSparkデータフレームとして読み込むか？","response":"spark.read.format()でcom.crealytics.spark.excelを指定してデータフレームに読み込みます。"}
{"category":"closed_qa","context":"","instruction":"なぜcsvデータをDelta形式に変換するか?","response":"Delta形式とはDelta lakeというDWHとData lakeの両方の特徴を機能として併せ持つオープンソースのストレージサービスを利用するためのデータ形式になります。Delta形式に変換することによって、以下のような代表的なメリットがあります。\n・大規模データの高速処理が可能 \n ・データの信頼性(例えばデータの破損や複数人による同時操作による影響を防ぐなど)を保つ \n・タイムトラベル機能により加工前のテーブルに戻ることができる \n・ストリーミングやバッチ処理に応用できる"}
{"category":"closed_qa","context":"","instruction":"Pysparkのwindow 関数とは？","response":"window 関数はデータを集計して計算する際に、計算する範囲を区切ることができる関数です。例えば平均気温を求めたいとなったとき、すべての地域をまとめた平均気温を求めるのではなく、地域ごとに平均を求めたいときなどに window 関数は使用されます。"}
{"category":"closed_qa","context":"","instruction":"Bamboolibとは？","response":"Bamboolibは、皆さんが知っていて大好きな標準的Pythonデータサイエンスライブラリであるpandasの機能に対してグラフィカルユーザーインタフェースを提供するローコードツールです。Bamboolibを用いることで、コード自身を記述することなしに簡単にコードファーストのデータサイエンスのすべてのパワーを手に入れることができます。これは、以下のことが可能になることを意味します:　1. データベースのテーブルやCSVファイルにアクセスしてデータをロードし、2. 生データから調査に適したクリーンかつ整理されたデータに加工、変換し、3. 皆様のビジネスに多大なインパクトをもたらすキーとなる洞察を明らかにするためにデータを探索、可視化、分析することができます。\n Bamboolibはローコード分析にガラスボックスアプローチを用いることでこれを実現しています: これらの分析オペレーションに対するpandasコードを生成し、UIを用いて実施したすべての分析結果を再現できるようにしています。そして、活用したいテクニックがネイティブでBamboolibで利用できない場合には、Bamboolibのプラグインフレームワークを用いることで容易に機能を拡張することができます。"}
{"category":"closed_qa","context":"","instruction":"Bamboolibは何ですか？","response":"Bamboolibはすべてのバックグラウンドを持つDatabricksユーザーに対して、コードファーストのデータサイエンスで利用できるすべてのディープな機能と柔軟性へのゲートウェイを提供します。実践者は以下のことをこなえるようになります。1. 生産性が向上します: 通常の定型文を記述することなしにpandasデータフレームを準備、分析、可視化できるので、手元の作業にフォーカスできます。2. すぐにpandasに慣れることができます: Excel、MATLAB、SASのような他のツールをよく使っていた場合には、何をしたいのかを知っていたとしても、pandasやPythonでそれをどのように行うのかがわからないかと思います。Bamboolibは、行いたいオペレーションの自然言語の記述をPythonコードに変換することを助けてくれます。3. 分析とコードの結果に自信を持てるようになります: 作業の過程でBamboolibが生成するコードを容易に確認、評価、エクスポートできるので、行っているすべての作業が自分自身、あるいは分析結果を共有する同僚によって再現が可能です。"}
{"category":"closed_qa","context":"","instruction":"Bamboolibのメリットは何か？","response":"Bamboolibのメリットは、自分達の実践者のスキルセットを拡大し、彼らのインパクトを向上させたいと考えているデータ分析のリーダーや組織にも適用されます。1. オンボーディングをシンプルにしセルフサービス分析を実現します: シチズンデータサイエンティスト、ドメイン専門家、その他の従業員が、最小限の技術的オーバーヘッドでDatabricksノートブックでインパクトがあり再現可能な結果を達成することを可能にします。2. 学習とスキル開発の機会を提供します: BamboolibはPythonのデータサイエンスの世界のシンプルなエントリーポイントとなります。ガラスボックスアプローチによって、ユーザーは分析を実現するコードにアクセスでき、このコードを用いることで彼らはpandasライブラリとモダンなデータサイエンスのコアな手法を学ぶことができます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのクラスターとSQLウェアハウスの違いは何ですか？ ","response":"DatabricksクラスターとSQLウェアハウスの違いをクイックにまとめます。SQLウェアハウスは、クエリー、ダッシュボード、ビジュアライゼーションのようなDatabricks SQLのワークロードの実行に使用されます。お使いのワークスペースがUnity Catalogのメタストアにアタッチされているのであれば、SQLウェアハウスを用いることで、デフォルトでUnity Catalogのデータにアクセスでき、Unity Catalog固有のコマンドを実行することができます。クラスターは、Data Science & EngineeringやDatabricks Machine Learningのペルソナベースの環境におけるワークロードの実行に使用されます。Unity Catalogにアクセスできるクラスターを作成するためには、クラスターを作成するワークスペースがUnity Catalogメタストアにアタッチされ、Unity Catalogを使用できるアクセスモード(共有あるいはシングルユーザー)を使用しなくてはなりません。利用している環境に応じて、これらの計算資源を用いてUnity Catalogのデータを操作することができます: Databricks SQLではSQLウェアハウス、Data Science & EngineeringやDatabricks Machine Learning環境ではクラスターとなります。さらに、1. SQLウェアハウス(エンドポイント)はSQLコマンドを実行するために開発されており、Scala/R/PythonやSQLコマンドを実行するために開発されています。2. SQLウェアハウス(エンドポイント)ではJAR、PIP、WHLのようなライブラリのオーバヘッドがなく、クラスターではライブラリによるオーバヘッドが生じることがあります。3. SQLウェアハウス(エンドポイント)はSQLウェアハウスの管理を簡素化しており、起動時間を加速します。クラスターの設定は初めての方にとっては複雑なものになる場合があります。4. SQLウェアハウス(エンドポイント)はクラスターとしてスケールアップ/スケールダウンします。クラスターはノードごとにスケーリングし、最大範囲までスケールアップします。5. SQLウェアハウス(エンドポイント)には、起動時間を劇的に削減するサーバレスの機能(プライベートプレビュー)がありますが、クラスターではその機能はまだありません。ただし、SQLウェアハウス(エンドポイント)とクラスターの両方はTableauのようなBIツールからの接続に使用でき、自動起動の機能を有しています。"}
{"category":"closed_qa","context":"","instruction":"クラスターアクセスモードとは？","response":"Databricksでクラスターを作成する際、クラスターを使用しようとしているワークロードのタイプ固有のアクセスモードを選択しなくてはなりません。Unity Catalogは特定のクラスターアクセスモードを用いたセキュリティを強制します。クラスターがUnity Catalogを使用できるアクセスモード(共有あるいはシングルユーザー)のいずれかを設定されていない場合、クラスターはUnity Catalogのデータにアクセスすることはできません。クラスターアクセスモードをシングルユーザーや共有に設定することで、Unity Catalogの要件を満たすように既存クラスターをアップグレードすることができます。"}
{"category":"closed_qa","context":"","instruction":"Dataframe in, Dataframe outはどういうこと？","response":"Spark のデータエンジニアリングを実施する際には、Dataframe in, Dataframe out、つまり、データフレームを引数としてデータフレームをリターンとするメソッドを組み合わせて開発を行うことが推奨されている。その理由の１つは、テストのやりやすさがある。"}
{"category":"closed_qa","context":"","instruction":"Databricksでサポートしているアイデンティティは何ですか？","response":"Databricksのアイデンティティには3つのタイプが存在します。1. ユーザー: Databricksによって識別されるユーザーアイデンティティでありメールアドレスで表現されます。2. サービスプリンシパル: ジョブ、自動化ツール、そして、スクリプト、アプリ、CI/CDプラットフォームのようなシステムで使用するアイデンティティです。3. グループ: ワークスペース、データ、そのほかのセキュリティ保護可能オブジェクトへのグループのアクセスを管理するために、管理者によって使用されるアイデンティティのコレクションです。"}
{"category":"closed_qa","context":"","instruction":"Databricksで定義されるロールは何ですか？","response":"Databricksで定義されるロールが5つ存在します。1. アカウント管理者は、ワークスペース、Unity Catalogメタストアの作成、課金、クラウドリソースを含むDatabricksのアカウントレベルの設定を管理することができます。アカウント管理者はアカウントにユーザーを追加し、管理者ロールを割り当てることができます。また、ワークスペースでアイデンティティフェデレーションを使用している場合、ユーザーにワークスペースへのアクセス権を与えることができます。2. ワークスペース管理者は、Databricksワークスペースにユーザーを追加し、ワークスペース管理者ロールや、クラスターの作成や特定のペルソナベースの環境へのアクセス権などの能力を割り当てることができます。3. メタストア管理者は誰がカタログを作成でき、テーブルをクエリーできるのかと言ったUnity Catalogメタストアの全てのセキュリティ保護可能オブジェクトの権限を管理することができます。4. アカウントユーザーはワークスペースを参照し、ワークスペースに接続するためにアカウントコンソールを使用することができます。アカウント管理者とワークスペース管理者はアカウントにユーザーを追加することができます。5. ワークスペースユーザーはワークスペースでデータサイエンス、データエンジニアリング、データ分析タスクを実行します。アカウント管理者とワークスペース管理者は、ワークスペースでアイデンティティフェデレーションを使用している場合、アカウントユーザーにワークスペースへのアクセス権を与えることができます。"}
{"category":"closed_qa","context":"","instruction":"誰がDatabricksでアイデンティティを管理できるのか？","response":"Databricksでアイデンティティを管理するには、アカウント管理者かワークスペース管理者である必要があります。以下の表では、ユーザー管理のアクションに必要な特定のアクセス権の詳細を示しています。アカウントには合わせて最大10,000ユーザーとサービスプリンシパルを持つことができ、最大5000グループを持つことができます。それぞれのワークスペースでは、最大10,000ユーザーとサービスプリンシパル、最大5000グループを持つことができます。"}
{"category":"closed_qa","context":"","instruction":"管理者はどのようにユーザーをワークスペースに割り当てるのか？","response":"Databricksワークスペースで作業するために、ユーザー、サービスプリンシパル、グループを有効化するには、アカウント管理者あるいはワークスペース管理者がそれらをワークスペースに割り当てる必要があります。アカウント管理者は、ワークスペースでアイデンティティフェデレーションが有効化されている場合、ワークスペースに対してアカウントに存在するユーザー、サービスプリンシパル、グループにアクセスを割り当てることができます。また、ワークスペース管理者はアイデンティティフェデレーションが有効化されている自分のワークスペースにユーザー、サービスプリンシパル、グループを割り当てることができます。逆に、新規ユーザーや新規サービスプリンシパルをワークスペースに直接追加すると、それらのユーザー、サービスプリンシパルは自動でアカウントに追加され、ワークスペースに割り当てられます。この上流のフローはお勧めしません。アカウントにユーザーを追加し、ワークスペースに割り当てる方が分かりやすいです。Databricksはどのようにワークスペースとアカウント間でアイデンティティを同期するのか？をご覧ください。ワークスペースローカルグループとして知られる、ワークスペースで直接作成されたグループは自動でアカウントに追加されません。アカウントのグループとして手動でワークスペースローカルグループを再作成することができます。グループに対する特別な検討をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"管理者はどのようにワークスペースのアイデンティティフェデレーションを有効化するのか？","response":"ワークスペースでアイデンティティフェデレーションを有効化するには、ワークスペースをUnity Catalogメタストアに割り当てることで、ワークスペースでUnity Catalogを有効化する必要があります。Enable a workspace for Unity Catalogをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksはどのようにワークスペースとアカウント間でアイデンティティを同期するのか？","response":"2022年8月にすべての既存のワークスペースユーザーとサービスプリンシパルは自動でアカウントレベルのユーザー、アカウントレベルのサービスプリンシパルに同期されました。Databricksはワークスペースにユーザーが追加された際には、常にユーザーとサービスプリンシパルをアカウントに同期し続けます。ワークスペースユーザーがアカウントレベルのユーザーや既に存在している管理者とユーザー名(メールアドレス)を共有している場合にはマージされます。ワークスペースローカルグループはアカウントレベルに同期されません。ワークスペースローカルグループは、ワークスペース管理者のコンソールと(当該ワークスペースでアイデンティティフェデレーションが有効化されている場合)アカウントコンソールのワークスペースアクセス権タブでワークスペースローカルと識別されます。詳細はグループに対する特別な検討をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"AutoMLのパフォーマンスは？","response":"今回、どのくらいの精度がでるかというところで、SIGNATE様が出されているTest用のCompetitionデータを使ってAutoMLを回した結果です。最終的に最もよい精度(accuracy)は0.934を記録しているので、精度としてはいい値かと思います。コンペの上位の方達も9.7近くの値をだされているので、baselineの作成としてはAutoMLを活用いただけるのでは！"}
{"category":"closed_qa","context":"","instruction":"Databricks にて CAST できない場合はどうしたらいいですか？","response":"一つのやり方としては、Databricks にて CAST できない場合にゼロに置換する Python関数を共有します。"}
{"category":"closed_qa","context":"","instruction":"Databricksノートブックのアドバイザーとは？","response":"Databricksアドバイザーは実行されるコマンドを自動的に分析し、ノートブック上に適切なアドバイスを表示します。追知されるアドバイスは、作業効率の改善、コスト削減、一般的なミスの回避に役立つ情報を提供します。"}
{"category":"closed_qa","context":"","instruction":"アドバイスをどうやって設定しますか？","response":"ユーザー設定に移動するか、をクリックするか、展開されたアドバイスボックス内の歯車アイコンをクリックすることでノートブックの設定ページに移動します。Turn on Databricks Advisorを切り替えることで、アドバイスを有効化・無効化できます。ある種のアドバイスを非表示にした場合、Reset hidden adviceリンクが表示されます。これらのアドバイスを再度表示させたい場合にはこのリンクをクリックします。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesのチェンジデータキャプチャ(CDC)をどのように活用するか？","response":"データレイクにおける新規レコードを特定し、データへの変更を補足するために、どのようにDelta Live Tablesパイプラインのチェンジデータキャプチャを活用できます。CDCデータが外部システムから到着する一般的なCDCユースケースにおけるDelta Live TablesパイプラインにおけるAPPLY CHANGES INTOコマンドを使用します。"}
{"category":"closed_qa","context":"","instruction":"チェンジデータキャプチャ(CDC)の重要性を教えてください。","response":"チェンジデータキャプチャ(CDC)はデータベースやデータウェハウスのようなデータストレージにおけるレコードの変更をキャプチャするプロセスです。これらの変更は通常、データの削除、追加、更新のようなオペレーションとみなされます。データベースをエクスポートするデータベースのダンプを取得し、レイクハウス/データウェアハウス/データレイクにインポートするデータレプリケーションはシンプルな方法ですが、これはスケーラブルなアプローチとは言えません。データベースでなされた変更のみをキャプチャし、これらの変更をターゲットデータベースに適用するのがチェンジデータキャプチャです。CDCはオーバーヘッドを削減し、リアルタイム分析をサポートします。バルクロードによる更新をすることなしに、インクリメンタルなロードを実現します。"}
{"category":"closed_qa","context":"","instruction":"お使いのSQLデータベースをどのようにレイクハウスに同期するのか？","response":"CDCツール、Auto Loader、DLTパイプラインを用いたCDCフロー: 1. CDCツールがデータベースログを読み込み、変更を含むJSONメッセージを生成し、Kafkaに対して変更説明を伴うレコードをストリーミング, 2. KafkaがINSERT, UPDATE, DELETEオペレーションを含むメッセージをストリーミングし、クラウドオブジェクトストレージ(S3、ADLSなど)に格納, 3. Auto Loaderを用いてクラウドオブジェクトストレージからメッセージをインクリメンタルにロードし、生のメッセージとして保存するためにブロンズテーブルに格納, 4. 次に、クレンジングされたブロンズレイヤーテーブルに APPLY CHANGES INTO を実行し、後段のシルバーテーブルに最新の更新データを伝搬。外部データベースからCDCデータを処理するための実装を以下に示します。入力はKafkaのようなメッセージキューを含む任意のフォーマットになり得ることに注意してください。"}
{"category":"closed_qa","context":"","instruction":"DebeziumのようなCDCツールの出力はどのようなものか？","response":"変更データを表現するJSONメッセージは、以下の一覧と同じような興味深いフィールドを持っています。operation: オペレーションのコード(DELETE, APPEND, UPDATE, CREATE)。operation_date: それぞれのオペレーションのアクションがあった日付、タイムスタンプ Debeziumの出力には以下のようなフィールドが含まれます(このデモには含めていません)。before: 変更前の行。after: 変更後の行"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesではどうやってデータ品質を追跡するか？","response":"Delta Live Tablesはエクスペクテーションを通じてデータ品質を追跡します。これらのエクスペクテーションはDLTのログイベントとともに技術的なテーブルとして格納されます。この情報を分析するために、シンプルにビューを作成することができます。"}
{"category":"closed_qa","context":"","instruction":"Nixtlaとは？","response":"Nixtlaは、一貫性があり使いやすい方法で広範な予測の能力を活用できるようにすることを目的とした一連のライブラリです。散在しているソフトウェアパッケージから必要な機能を探し出すことにフラストレーションを感じている実践者によって開発されており、Nixtlaでは標準化とパフォーマンスに重きを置いており、企業は断続的需要の予測を含む現実世界の予測の問題を比較的簡単に解くことが可能となります。多くの企業がクラウドを活用してタイムリーな方法で膨大な数の予測を生成しようとしていることを知り、NixtlaはFugueエンジンとして知られる高性能、自動スケールアウトエンジンをライブラリに組み込みました。Databricksと組み合わせることで、企業は高速かつコスト効率が高い方法で膨大なボリュームの予測結果を生成できるようになります。"}
{"category":"closed_qa","context":"","instruction":"Data ExplorerとREST APIを用いたリネージの可視化はどうやって実現するか？","response":"Databricksで実行されたクエリーに対する実行時データリネージをキャプチャするためにUnity Catalogを活用することができます。リネージはすべての言語でサポートされており、カラムレベルまで捕捉されます。リネージデータには、クエリーに関連するノートブック、ワークフロー、ダッシュボードが含まれます。リネージは、ニアリアルタイムでData Explorerに表示され、Databricks REST APIで取得することができます。リネージはUnity Catalogメタストアにアタッチされているすべてのワークスペースから集約されます。これは、あるワークスペースでキャプチャされたリネージは、メタストアを共有する他のワークスペースで参照できることを意味します。リネージデータを参照するには、ユーザーは適切なアクセス権を持っている必要があります。リネージデータは30日間保持されます。"}
{"category":"closed_qa","context":"","instruction":"Delta 形式テーブルmerge とは何か","response":"merge は直訳すると融合するというような意味を持ちます。Databricks では Delta 形式のテーブルに対してのみ、merge を使用したテーブルの削除及び upsert(更新と追加の総称)が可能になります。もちろん merge を使用せずとも insert や delete などを直接叩いてテーブルを変更させることも可能です。ですが、merge を使用することによって以下のようなメリットが挙げられます。テーブル読み込みの効率化。ほとんどのユースケースではテーブルの一部分のみに変更を加えると思います。その一部分の変更のためだけにテーブルの全体を読み込んだり再書き込みを行ってしまうと、変更したい部分以外にも無駄な工数が発生することになります。merge では条件式を加えることにより、その条件に合った部分、またその条件外の部分にのみ変更を加えるといった操作が行えます。つまり、merge の条件式に関係のない部分はデータの検索を行わない(データスキッピングと呼ばれる)のでクエリ処理を高速化させます。条件式によって変更部分の指定方法に自由度が増し、さらにクエリ処理を効率的に行えるのがメリットとなります。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesとは","response":"Delta Live Tablesは、信頼性が高く、維持が容易でテスト可能なデータ処理パイプラインを構築するためのフレームワークです。データに対する変換処理を定義すると、Delta Live Tablesはタスクのオーケストレーション、クラスター管理、モニタリング、データ品質、エラー処理を行います。そして、Delta Live Tablesはそれぞれの処理ステップであなたが定義したターゲットスキーマに基づき、どのようにデータが変換されるのかを管理します。また、Delta Live Tablesのエクスペクテーションを活用してデータ品質を維持することもできます。エクスペクテーションによって、期待されるデータ品質を定義し、エクスペクテーションに沿わないデータをどのように取り扱うのかを定義することができます。"}
{"category":"closed_qa","context":"","instruction":"MLOpsとは？","response":"MLOpsとは、DevOps+DataOps+ModelOpsによって構成され、組織がコード、データ、およびモデルを管理するのに役立つ一連のプロセスと自動化のことを指します。"}
{"category":"closed_qa","context":"","instruction":"なぜMLOpsは必要なのか？","response":"機械学習 (ML) システムでは、モデル、コード、データはすべて時間の経過とともに進化し、更新スケジュールと MLパイプラインに摩擦が生じる可能性があります。一方、モデルの再トレーニングが必要になるデータドリフトにより、モデルのパフォーマンスは時間の経過とともに低下する可能性があります。さらに、データ サイエンティストがデータセットを調査し続け、モデルを改善するためにさまざまなアプローチを適用すると、より適切に機能する新しい特徴量や他のモデルファミリーが見つかる可能性があります。そして、コードを更新して再デプロイします。MLOps のベストプラクティスにより、データ サイエンティストは、特徴量エンジニアリング、モデル アーキテクチャ、ハイパーパラメータに関する新しいアイデアを迅速に調査して実装し、新しいパイプラインを自動的に構築、テストして、運用環境にデプロイできます。堅牢で自動化された MLOps システムは、アイデアからビジネス価値の向上まで、組織の AI イニシアチブを強化し、データと ML への投資収益を生み出します。"}
{"category":"closed_qa","context":"","instruction":"チェンジデータキャプチャ(CDC)は何？","response":"チェンジデータキャプチャ(CDC)は、ニアリアルタイムのデータアプリケーションにおける顧客の追跡、注文・商品のステータスの追跡の様に、データベースにおけるインクリメンタルな変更(データの削除、追加、更新)を特定、補足するプロセスです。CDCでは、新規イベントが生じるのに合わせて継続的かつインクリメンタルな方法でデータを処理することで、リアルタイムのデータ進化を実現しています。2025年までには企業の80%がマルチクラウドの戦略を実施しようとしていることから、複数の環境におけるETLパイプラインにおけるすべてのデータの変更をリアルタイムかつシームレスに集中管理することが重要となります。"}
{"category":"closed_qa","context":"","instruction":"Databricksでファイルを読み書きする際にパスをどう指定すればいいだろう？","response":"大きく分けて、分散ファイルシステム向けコマンドとローカルファイルシステム向けコマンドに分けて考えるといいかと思います。名前の通り、それぞれのファイルシステムがデフォルトになります。そして、Databricksでは上述のコマンドのいずれにおいても、DBFS、ローカルファイルシステムにアクセスすることができるのですが、明示的に指示をしない場合、デフォルトのファイルシステムにアクセスします。指示する方法は以下の通りです。1. 分散ファイルシステム向けコマンドでローカルファイルシステムにアクセスする場合: パスの先頭にfile:/を追加2. ローカルファイルシステム向けコマンドでDBFSにアクセスする場合: パスの先頭に/dbfsを追加"}
{"category":"closed_qa","context":"","instruction":"Databricksでファイルを読み書きするのになぜ難しいと感じるのか？","response":"大きく2点があると思います。1. ファイルシステムが2種類(Databricksファイルシステム / ドライバーノードのローカルファイルシステム)あることがわかりにくい。2. 使っているAPIに応じてパスの書き方を変える必要があることがわかりにくい"}
{"category":"closed_qa","context":"","instruction":"Databricksにあるファイルシステムは何種類があるか？","response":"ファイルシステムが2種類あります。一つ目はDatabricksファイルシステム。Databricksでファイルの永続化に使用するDBFS(Databricksファイルシステム)が存在します。これは、お客さまのクラウドアカウントに構築されるデータプレーン上のオブジェクトストレージ(S3やADLS)を用いて構成される、仮想的なファイルシステムです。２つ目はSparkクラスターのドライバーノードのローカルストレージ。Databricksで計算資源を利用する際は、お客さまクラウドアカウント上に構築されるデータプレーン上にDatabricksクラスター(Apache Sparkクラスター)を構成し、これにノートブックをアタッチして処理を実行します。クラスターの実態はAWSであればEC2、AzureであればVMになります。当然これらにはローカルのストレージがアタッチされています。また、Sparkクラスターはドライバーノードとワーカーノードから構成されていますが、ユーザーは通常ドライバーノードにSparkジョブを投入して結果を受け取るというように、ドライバーノードとやり取りを行います。これは、Databricksでも同様です。ノートブックをDatabricksにアタッチして処理を実行する際はドライバーノードとやり取りを行い、Sparkの並列分散処理はドライバーノードからワーカーノードに指示を行うことで実行され、結果がドライバーノードに戻され、その結果がノートブックに表示されます。この処理の過程でローカルストレージにファイルを保存することが可能です。"}
{"category":"closed_qa","context":"","instruction":"2つのファイルシステム（Databricksファイルシステム / ドライバーノードのローカルファイルシステム)あることがわかりにくい）の違いは何？","response":"ファイルの永続化に使えるのは①のDBFSだけです。クラスターのドライバーノードのローカルストレージは揮発性であり、クラスターを停止すると全てが失われます。このことを理解しておかないと、せっかく準備したデータがなくなってしまった！ということになりかねません。ですので、計算処理過程での中間ファイルなどはローカルストレージに保存し、処理結果のファイルの永続化はDBFSで行うということを意識することが重要となります。もう一つ重要な違いとして、DBFSはオブジェクトストレージで動作しているので、ランダムアクセスはサポートされていません。DBFSにおいてランダムアクセスが発生するような書き込み(例：zip)を行おうとするとエラーになります。この場合、こちらで説明している様にローカルファイルシステム上でzipファイルを作成してから、DBFSにコピーしてください。"}
{"category":"closed_qa","context":"","instruction":"なぜ、使っているAPIに応じてパスの記述方法を変えなくてはいけないのか？","response":"アクセスする先が違うので、そのことをDatabricksに教えてあげないといけないためです。"}
{"category":"closed_qa","context":"","instruction":"どの程度の頻度でVACUUMを実行すべきですか？","response":"過度なクラウドデータストレージのコストを削減するために、すべてのテーブルに対してVACUUMを定期的に実行することをお勧めします。vacuumのデフォルト保持期間は7日間です。閾値を高く設定すると、より長いテーブル履歴にアクセスすることができますが、格納されるデータファイルの数が増加し、クラウドプロバイダーのストレージコストを増加させることになります。"}
{"category":"closed_qa","context":"","instruction":"低い保持期間の閾値でDeltaテーブルをVACUUMできないのはなぜですか？","response":"Delta Lakeには、危険なVACUUMコマンドの実行を防ぐ安全チェック機能があります。指定しようとしている保持期間よりも長い時間実行されるオペレーションが存在しないことが確実である場合には、Spark設定spark.databricks.delta.retentionDurationCheck.enabledをfalseに設定することで、安全チェック機能をオフにすることができます。警告！テーブルに対するリーダー、ライターの同時実行によって、古いスナップショットやコミットされていないファイルが使用されている場合があるので、保持期間の最小値は7日間にすることをお勧めします。VACUUMがアクティブなファイルをクリーンアップすると、同時実行されているリーダーが失敗することがあり、さらに悪いケースでは、VACUUMがコミットされていないファイルを削除するとテーブルが破損する場合があります。最長の同時実行トランザクション期間や、テーブルに対する最新の更新に対してすべてのストリームが遅延する最長期間よりも長い期間を選択すべきです。"}
{"category":"closed_qa","context":"","instruction":"お使いのアイデンティティプロバイダーからユーザー、グループをどのようにDatabricksアカウントに同期するか？","response":"お使いのアイデンティティプロバイダーからご自身のDatabricksアカウントにユーザーやグループが自動で同期される様に、SCIMプロビジョニングを使用することをお勧めします。SCIMはDatabricksにユーザーやグループを作成するためにアイデンティティプロバイダーを使用するので、新たな従業員やチームのオンボーディングを容易にし、適切なアクセスレベルを付与します。ユーザーが組織を去ったり、Databricksにアクセスする必要がなくなった場合には、管理者はアイデンティティプロバイダーでユーザーを無効化すると、そのユーザーアカウントはDatabricksからも削除されます。これによって、一貫性のあるオフボーディングを確実なものにし、許可されないユーザーがセンシティブなデータにアクセスすることを防ぐことができます。Databricksを使用しようとしているすべてのユーザーとグループのすべてを、個々のワークスペースではなくアカウントコンソールに同期する様にすべきです。これによって、アカウント内にあるすべてのワークスペースにおいて、すべてのアイデンティティの一貫性のある状態を保つために、一つのSCIMプロビジョニングアプリケーションを設定するだけで済む様になります。ワークスペースですでにワークスペースレベルのSCIMプロビジョニングをセットアップしているのであれば、アカウントレベルのSCIMプロビジョニングを有効化し、ワークスペースレベルのSCIMプロビジョンをオフにすべきです。アイデンティティフェデレーションへのアップグレードをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"シングルサインオン(SSO)をどのように設定するか？","response":"シングルサインオン(SSO)を用いることで、組織のアイデンティティプロバイダーを用いてユーザーを認証できる様になります。セキュリティ強化とユーザビリティの改善のためにSSOを設定することをお勧めします。アカウントとそれぞれのワークスペースでSSOを設定する必要があります。アカウントにおいて、アカウントとすべてのワークスペースで同じアイデンティティプロバイダーのSSOを設定する必要があります。望ましくは、アカウントレベルのSSO設定にOICDを用いて、認証機能のサポートを確実なものとします。ワークスペースでSSOを設定したら、パスワードアクセスコントロールを設定すべきです。パスワードアクセスコントロールによって、REST APIの認証でユーザー名、パスワードを使用できなくすることができます。代わりに、ユーザーはパーソナルアクセストークンを用いてREST APIの認証を行うべきです。すべてのワークスペースユーザーに対してパスワードに対するCan Use権限を許可しないことをお勧めします。パーソナルアクセストークンの詳細については、Manage personal access tokensをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"アイデンティティフェデレーションをどのように有効化するか","response":"アイデンティティフェデレーションによって、アカウントコンソールでユーザー、サービスプリンシパル、グループを設定することができ、これらのアイデンティティを特定のワークスペースに割り当てることができる様になります。これによって、Databricksにおける管理とデータガバナンスをシンプルなものにします。アイデンティティフェデレーションを用いることで、それぞれのワークスペースで設定を繰り返すことなしに、アカウントコンソールでユーザー、サービスプリンシパル、グループを設定できます。これによって、新たなチームのDatabricksへのオンボーディングにおける摩擦を削減し、ワークスペースごとのSCIMプロビジョニングアプリケーションを別々に管理するのではなく、Databricksアカウントに対してアイデンティティプロバイダーを用いた一つのSCIMプロビジョニングアプリケーションを管理するだけで済む様になります。アカウントにユーザー、サービスプリンシパル、グループが追加されると、それらの権限をワークスペースに割り当てることができます。アイデンティティフェデレーションが有効化されたワークスペースに対してのみ、アカウントレベルのアイデンティティのアクセスを割り当てることができます。ワークスペースでアイデンティティフェデレーションを有効化するには、管理者はどのようにワークスペースのアイデンティティフェデレーションを有効化するのか？をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"グループにワークスペース権限をどのように割り当てるか？","response":"ワークスペースでアイデンティティフェデレーションが有効化されたので、当該ワークスペースでアカウントアクセス権限をユーザー、サービスプリンシパル、グループを割り当てることができます。ワークスペースの権限をユーザー個人ではなく、ワークスペースにはグループの権限を割り当てることをお勧めします。すべてのDatabricksアイデンティティはグループのメンバーとして割り当てることができ、メンバーはグループに割り当てら得れた権限を継承します。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesのテーブルとビューの選択基準は何？","response":"パイプラインが効率的かつメンテナンスしやすくなる様に、パイプラインのクエリーを実装する際に、テーブルかビューかベストなデータセットタイプを選択する様にしましょう。以下のケースではビューの使用を検討してください: 1. 管理しやすいクエリーに分割したいと考える大規模かつ複雑なクエリーである場合。2. エクスペクテーションを用いて中間結果を検証したい場合。3. ストレージコスト、計算コストを削減したいと考えており、クエリー結果のマテリアライゼーションが不要な場合。テーブルはマテリアライゼーションされるので、計算コストとストレージリソースを必要とします。以下のケースではテーブルの使用を検討してください: 1. 複数の後段のクエリーがテーブルを使用する場合。ビューはオンデマンドで計算されるので、クエリーされるたびにビューは再計算されます。2. 開発中にクエリーの結果を参照したい場合。テーブルはマテリアライズされるので、パイプラインの外で参照、クエリーすることができ、開発中にテーブルを用いることは、計算処理の適切性の検証の役に立ちます。検証後に、マテリアライゼーションが不要なクエリーをビューに変換します。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesのパイプラインをどの様に分割するか?","response":"パイプラインをどの様に分割するのかを決定する際、以下の点を検討してください。以下のケースでは大規模パイプラインにしてください:　1. クラスターリソースをより効率的に活用したい。2. ワークスペースのパイプライン数を削減したい。3. ワークフローのオーケストレーションの複雑性を削減したい。以下のケースでは複数のパイプラインにしてください。1. チーム境界で機能を分割したい場合。例えば、あなたのデータチームではデータを変換するためのパイプラインを維持管理し、データアナリストは変換データを分析するパイプラインを維持管理する場合。2. 結合度を削減し、共通機能の再利用を促進するために、アプリケーション固有の境界で機能を分割する場合。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesのパイプラインにおけるクラスターの使用率をどうやって最適化するか？","response":"パイプラインにおけるクラスターの使用率を最適化するために、強化オートスケーリングを使いましょう。強化オートスケーリングは、リソースの追加によってパイプラインの処理スピードを改善するとシステムが判断した時のみリソースを追加します。不要になるとすぐにこれらのリソースは解放され、すべてのパイプラインのアップデートが完了するとクラスターはシャットダウンします。プロダクションパイプラインで強化オートスケーリングを設定する際には、以下のガイドラインを使用してください。1. Min workersの設定はデフォルトのままにしてください。2. 予算とパイプラインの優先度に基づいて、Max workersを設定してください。3. Delta Live Tablesランタイムが、ワークロードにベストなインスタンスタイプを選択できる様に、インスタンスタイプは未設定の状態にしてください。"}
{"category":"closed_qa","context":"","instruction":"RからMLflowを活用することが可能でしょうか？","response":"はい、DatabricksではPythonだけではなく、Rを活用して機械学習モデルをトレーニングして、MLflowを用いてトラッキングすることもできます。Rを得意とされる方も是非試してみてください！"}
{"category":"closed_qa","context":"","instruction":"Delta Sharingを使ってローカルPCからDatabricksのデータにアクセスする方法は？","response":"以下の流れでアクセスします。1. DeltaSharing受信者の作成, 2. アクティベーションリンク発行 ＆ クレデンシャル情報, 3. 共有オブジェクトの作成, 4. ローカルPCからアクセス"}
{"category":"closed_qa","context":"","instruction":"Azure Databricks と AWS Databricks 間でデータ共有はどうすればいいか？","response":"Delta Sharingを利用するとDatabricks to Databricksもしくは Databricks to Open(DeltaSharing Client)間でデータ共有が行えるようになります。共有といってもReadOnlyなので一方通行ですが、他のデータブリックス環境で作成されたデータを、どのデータブリックス環境（もちろん許可された）からアクセス出来るようになったら、データ活用の幅が広がる気がします。"}
{"category":"closed_qa","context":"","instruction":"Databricksでpandasは使えるのか？","response":"Databricksランタイムには、標準的なPythonパッケージの一つとしてpandasが含まれており、Databricksのノートブックやジョブでpandasデータフレームを作成、活用することができます。Databricksランタイム10.0以降では、Pandas API on SparkがPySparkデータフレーム上における馴染み深いpandasコマンドを提供しています。また、pandasデータフレームとPySparkデータフレームの間で変換を行うことができます。Apache Sparkには、PySparkデータフレームに直接pandasの変換処理を適用できるpandas function APIの形でPythonロジックを実行するArrow最適化処理が含まれています。また、Apache Sparkでは、Pythonで定義された任意のユーザー関数に対して同様のArrow最適化を活用するpandas UDFもサポートしています。"}
{"category":"closed_qa","context":"","instruction":"pandasはDatabricksのどこにデータを格納するのか？","response":"Databricksのさまざまな場所にデータを格納するためにpandasを使用することができます。どこにデータを格納し、どこからデータをロードできるのかは、ワークスペース管理者による設定に依存します。注意！クラウドオブジェクトストレージにプロダクションのデータを格納することをお勧めします。Working with data in Amazon S3をご覧ください。Unity Catalogが有効化されたワークスペースをお使いの際には、外部ロケーションのクラウドストレージにアクセスすることができます。Unity Catalogにおける外部ロケーションとストレージ認証情報の管理をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksではpandasを用いてどの様にデータをロードするのか？","response":"Databricksでは、探索のためにワークスペースにデータをアップロードするために、数多くの選択肢を提供しています。pandaによるデータロードで望ましい方法は、データをどのようにワークスペースにロードするのかによって変化します。ローカルマシン上にノートブックと一緒に小規模なデータファイルを格納していたのであれば、データとコードを一緒にReposにアップロードすることができます。データファイルをロードするために相対パスを使うことができます。Databricksではデータロードに対する数多くのUIベースのオプションを提供しています。これらのオプションの多くは、あなたのデータをDetlaテーブルとして保存します。そして、SparkデータフレームにDeltaテーブルを読み込み、それをpandasデータフレームに変換することができます。DBFSや相対パスを用いてデータファイルを保存している場合には、これらのデータファイルを再ロードするためにDBFSや相対パスを使用することができます。以下のコードではサンプルを示しています。"}
{"category":"closed_qa","context":"","instruction":"pandasを用いてS3から直接データをロードすることが可能でしょうか？","response":"はい、可能です。pandasと完全修飾のURLを用いることで、S3から直接データをロードすることができます。クラウドのデータにアクセスするためにクラウドの認証情報を指定する必要があります。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogメタストアの設定のベストプラクティスは何？","response":"Unity Catalogは、DatabricksレイクハウスにおけるデータとAIに対するきめ細かいガバナンスソリューションです。データアクセスを集中的に管理、監査できる場所を提供することで、データに対するセキュリティ、ガバナンスをシンプルにする助けとなります。以下の図では、Unity Catalogにおける主要なセキュリティ保護可能オブジェクトを示しています。メタストアはUnity Catalogにおけるトップレベルのコンテナです。データ資産(テーブルやビュー)とそれらに対するアクセスを管理する権限を格納します。Databricksのアカウント管理者はメタストアを作成し、どのワークロードがどのメタストアを使用するのかをコントロールするためにDatabricksワークスペースに割り当てることができます。オペレーションを行うそれぞれのリージョンに一つのメタストアを作成し、当該リージョンのすべてのワークスペースにリンクさせます。このため、Databricksが複数のリージョンで稼働している場合、複数のメタストアを持つことになります。メタストア間でデータを共有するには、Delta Sharingを使用します。それぞれのメタストアには、マネージドテーブルで使用されるルートストレージロケーションが設定されます。このストレージロケーションにいかなるユーザーも直接のアクセス権を持たない様にする必要があります。ストレージロケーションへのアクセスを許可すると、ユーザーはUnity Catalogメタストアによるアクセスコントロールを迂回し、監査可能性を阻害することになります。これらの理由から、現在DBFSのルートファイルシステムに使用しているバケットや、Unity Catalogメタストアのルートストレージロケーションとして使用していたDBFSルートファイルシステムのバケットを再利用すべきではありません。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでデータの整理のベストプラクティスは何？","response":"皆様の組織の情報アーキテクチャにおける区域を提供するためにカタログを使用することをお勧めします。多くの場合、これはカタログがソフトウェア開発環境のスコープ、チーム、ビジネスユニットに対応することを意味します。スキーマ(データベースよも呼ばれます)は、Unity Catalogの3レベル名前空間の2番目のレイヤーであり、テーブルやビューを整理します。テーブルはマネージドあるいは外部テーブルとなります。Unity Catalogにおいてテーブルを作成する際、マネージドテーブルがデフォルトとなります。これらのテーブルはメタストアを作成した際に指定したUnity Catalogのルートストレージロケーションに格納されます。Unity Catalogの機能サポートを確実にするために、可能な限りマネージドテーブルを使用することをお勧めします。すべてのマネージドテーブルはDelta Lakeを使用します。外部テーブルは、マネージドのストレージロケーションの外のストレージロケーションにデータが格納されるテーブルです。これらはUnity Catalogによって完全には管理されません。外部テーブルでは、Delta Lakeや、Parquet、JSON、CSVを含む数多くのデータフォーマットをサポートしています。生のデータに対する直接アクセスを提供する際、外部テーブルは好適なオプションと言えます。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogで外部ロケーションと外部テーブルの管理のベストプラクティスは何？","response":"4つの外部ロケーションが作成され、それらすべてによって使用される1つのストレージ認証情報があります。ユーザーやグループには、Unity Catalogメタストア内の別々のストレージロケーションに対してアクセスを許可することができます。これによって、特定のグループに対してクラウドストレージバケットの別の部分へのアクセスを許可することができます。Unity Catalogメタストアのストレージロケーションを用いて外部テーブルをs買う制することができます。これらの外部テーブルは独立にセキュリティ保護することができます。1つのスキーマ内の1つのストレージロケーションから外部テーブルを作成することをお勧めします。一貫性に関する問題のリスクから、1つ以上のメタストアの外部テーブルとして共通のテーブルを登録することは強くお勧めしません。例えば、1つのメタストアにおけるスキーマへの変更は、2番目のメタストアに登録されません。メタストア間でデータを共有するには、Delta Sharingを使用してください。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでアクセスコントロールの設定のベストプラクティスは何？","response":"Unity Catalogにおける個々のセキュリティ保護可能オブジェクトにはオーナーが存在します。オブジェクトを作成したプリンシパルが最初のオーナーとなります。オブジェクトのオーナーは、テーブルのSELECTやMODIFY、他のプリンシパルにセキュリティ保護可能オブジェクトの権限を付与する権利など、オブジェクトに対するすべての権限を有します。このため、オブジェクトに対する権限付与に責任を持つグループに、すべてのオブジェクトに対するオーナーシップを設定することがベストプラクティスとなります。オーナーとメタストア管理者の両方が、セキュリティ保護可能オブジェクトのオーナーをグループに移譲することができます。さらに、(テーブルやビューなど)オブジェクトがカタログに含まれている場合、カタログとスキーマのオーナーがオブジェクトのオーナーシップを変更することができます。Unity Catalogにおけるセキュリティ保護可能オブジェクトは階層的であり、権限は下方に継承されます。これは、カタログやスキーマに対して付与した権限が、カタログやスキーマに存在する、あるいは今後作成されるオブジェクトすべてに自動で権限が付与されることを意味します。詳細に関しては、Inheritance modelをご覧ください。テーブルやビューからデータを読み込むには、ユーザーは以下の権限を有している必要があります:　\n 1. テーブルやビューに対するSELECT \n 2. テーブルを保持するスキーマに対するUSE SCHEMA \n 3. スキーマを保持するカタログに対するUSE CATALOG \n 4. USE CATALOGによって、許可されたユーザーが子供のオブジェクトにアクセするためにカタログを移動でき、USE SCHEMAによって、許可されたユーザーが子供のオブジェクトにアクセスするためにスキーマを移動できます。例えば、テーブルからデータを取得するには、ユーザーはテーブルに対するSELECT権限を有している必要があり、親のカタログに対するUSE CATALOG権限と、親のスキーマに対するUSE SCHEMA権限が必要となります。このため、特定のグループに対してご自身のデータ名前空間のセクションへのアクセスを制限するために、この権限を活用することができます。一般的なシナリオは、チームごとにスキーマをセットアップし、当該チームのみがそのスキーマに対するUSE SCHEMAとCREATE権限を持つというものです。これは、チームメンバーによって作成されたすべてのテーブルはチーム内でのみ共有されるということを意味します。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogでクラスター設定の管理のベストプラクティスは何？","response":"一連のルールに基づいてクラスターを設定する能力を制限するために、クラスターポリシーを用いることをお勧めします。クラスターポリシーによって、Unity Catalogが有効化されたクラスターのみを作成できる様に制限をかけることができます。クラスターポリシーを用いることで選択肢を削減し、ユーザーのクラスター作成プロセスをシンプルにし、シームレスにデータにアクセスできる様になります。また、クラスターポリシーで、クラスタあたりの最大コストを制限することでコストをコントロールすることができます。アクセスコントロールの一貫性を保証し、強力なアイソレーション保証を強制するために、Unity Catalogは計算資源に対するセキュリティ要件を課します。このため、Unity Catalogではクラスターのアクセスモードというコンセプトを導入しています。Unity Catalogはデフォルトで保護されています。クラスターに適切なアクセスモードが設定されていない場合、クラスターはUnity Catalogのデータにアクセスすることができません。Unity Catalogにおけるクラスターアクセスモードをご覧ください。クラスターを共有する際にはユーザー分離アクセスモードを使用し、自動化ジョブや機械学習ワークロードにはシングルユーザーアクセスモードを使うことをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalogで監査アクセスのベストプラクティスは何？","response":"完全なデータガバナンスソリューションには、アラート、モニタリング能力を提供するデータアクセスの監査が必要となります。Unity Catalogはメタストアに対するアクションの監査ログを取得し、Databricks監査ログの一部としてデリバリーされます。監査ログの設定を確認してください。これには、あなたが指定するS3バケットにDatabricksが監査ログをデリバリーできる様にするための適切なアクセスポリシーの設定が含まれています。通常、監査ログは15分以内に記録されます。アカウント内のすべてのワークスペースで同じ監査ログ設定を使用する様にしてください。お使いのDatabricksレイクハウスプラットフォームに関連する重要なイベントに対する完全なる可視性をどの様に得るのかに関しては、監査ログによるDatabricksレイクハウスプラットフォームのモニタリングをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Unity CatalogでDelta Sharingのベストプラクティスは何？","response":"Delta Sharingは、使用している計算プラットフォームに関係なく企業内の別部門、あるいは別組織とセキュアにデータを共有するためにDatabricksによって開発されたオープンプロトコルです。メタストアでDelta Sharingが有効化されると、Unity CatalogはDelta Sharingサーバーを稼働させます。メタストア間でデータを共有するには、Databricks-to-Databricks Delta Sharingを活用することができます。これによって、別のリージョンのメタストアからテーブルを登録することができます。これらのテーブルは利用するメタストアで読み取り専用オブジェクトとして表示されます。これらのテーブルは他のUnity Catalogのオブジェクトと同じようにアクセス権を設定することができます。メタストア間でDatabricks-to-Databricks Delta Sharingを用いる際、アクセスコントロールは一つのメタストアに限定されることに注意してください。テーブルの様なセキュリティ保護可能オブジェクトにアクセス権が設定されており、そのリソースがイントラアカウントのメタストアに共有された場合、ソースからの権限が対象の共有には適用されません。対象の共有においては自身で権限を付与する必要があります。"}
{"category":"closed_qa","context":"","instruction":"地理空間情報分析とは？","response":"地理空間情報分析とは「特定エリアに関する位置情報とそれに紐づく事象のデータから有益な知見や発見を得ること」を指します。代表的なデータには衛星画像や船舶・飛行機・車載機器・携帯電話などから取得できる位置情報データが存在し、それらを使ってデータを可視化・分析して地球上で何が起きているのか理解できます。また世界で大きなポテンシャルを秘めている領域と考えられており、米国の市場調査会社のひとつは、世界の高精度な地理空間情報ビジネスの市場規模は、2020 年時点で 568.8 億ドル（約 7兆 6,788 億円）、その後 2020 年から年 12.8％のペースで成長し、2028 年には 1,475.8 億ドル（約 19 兆 9,233 億円）に達すると予測しています。日本でも衛星画像分析用のプラットフォーム Tellus や位置情報を使った人口の統計情報 モバイル空間統計 をNTTドコモ社が運営されていたりと、多くの企業・団体がサービスを展開して盛り上がっている市場です。"}
{"category":"closed_qa","context":"","instruction":"TBLPROPERTIES とは？","response":"TBLPROPERTIES テーブルに設定するプロパティです。Delta Lake 形式のテーブルでは delta. ではじまる TBLPROPERTIES の設定値に応じて処理を実行します。Delta Lake 形式のテーブルの場合には、メタストアに設定値が保持されるのではなく、ディレクトリに設定値が保持されるようです。Delta Lake のバージョンアップに伴い、設定できる値が増えていくため、継続的に設定方針を検討する必要があります。"}
{"category":"closed_qa","context":"","instruction":"MLflow Recipesとは？","response":"MLflow Recipesはデータサイエンティストが高品質のモデルを迅速に開発し、本番環境にデプロイするためのフレームワークです。詳細は、以下のビデオとドキュメントをご覧ください。https://mlflow.org/docs/latest/recipes.html"}
{"category":"closed_qa","context":"","instruction":"MLflow RecipesにサポートしているML問題の種類は？","response":"回帰問題と分類問題の二種類です。"}
{"category":"closed_qa","context":"","instruction":"Unity Catalog のカタログIsolation機能は何か魅力的か？","response":"従来はUnity CatalogのManaged Tableとして利用する場合、DeltaLakeの保存先はUnity Catalogセットアップ時に作成するメタストアのストレージのみでした。そのため複数のワークスペースでUnity Catalogを共有してもストレージは同じになってしまうため、管理や課金を分割出来ないという問題がありました。（従来は外部テーブルしか利用できなかった）今回の機能によって、カタログやスキーマ単位でストレージロケーションを分けられるため上記のような問題が解消されます。これは管理者からするとかなり嬉しい機能なのです。"}
{"category":"closed_qa","context":"","instruction":"Online Feature Storeとは？","response":"Feature Storeとはデータサイエンスチームが特徴量を共有するためのリポジトリです。DatabricksにおけるFeature Storeの説明はこちらをご参照ください。日次のバッチ推論のようなジョブであれば単純にDatabricksのメタストアからテーブルを呼び出せば良いのですが、REST APIによるリアルタイム推論のように高速なレスポンスが求められるケースがあります。Databricks on AWSではFeature Storeに登録したテーブルをAmazon DynamoDB、Amazon Aurora(MySQL互換), Amazon RDS(MySQL)のテーブルとして発行することができ、これををOnline Feature Storeと呼んでいます。Online Feature Storeによって、低レイテンシで推論用の特徴量を供給することが可能になります。"}
{"category":"closed_qa","context":"","instruction":"ノートブックのオートコンプリート(auto-complete)機能がありますか？","response":"ノートブックがクラスターに接続されると、あなたが入力しているセルに、VS Code IntelliSenseによるオートコンプリートの提案が自動で表示されます。提案を選択するために上、下のキーやマウスを使い、選択内容をセルに入力するにはTabやEnterを使います。"}
{"category":"closed_qa","context":"","instruction":"PySparkでサポートされているUDFは何？","response":"PySparkでサポートされているUDFには大きく2つのカテゴリーが存在します: Python UDFとPandas UDFです。Python UDFは、Pickleによってシリアライズ/デシリアライズされるPythonオブジェクトを受け取り/返却するユーザー定義スカラー関数であり、一度に一行を処理します。Pandas UDF(ベクトライズドUDF)は、Apache Arrowによってシリアライズ/デシリアライズされるシリーズ、データフレームを受け取り/返却するUDFであり、ブロックごとに処理します。Pandas UDFは用途や入出力のタイプに応じてカテゴリ分けされるバリエーションが存在します: Series to Series, Series to Scalar, Iterator to Iteratorです。Pandas UDF実装をベースとしたPandas Functions APIが存在します: Map (mapInPandasなど)、(Co)Grouped Map (applyInPandasなど)、そして、Arrow Function APIのmapInArrowもあります。関数が入出力のイテレーターを受け取らない限り、上述のすべてのUDFタイプにメモリープロファイラーは適用されます。"}
{"category":"closed_qa","context":"","instruction":"Python UDFとPandas UDFの違いは何か？","response":"Python UDFは、Pickleによってシリアライズ/デシリアライズされるPythonオブジェクトを受け取り/返却するユーザー定義スカラー関数であり、一度に一行を処理します。Pandas UDF(ベクトライズドUDF)は、Apache Arrowによってシリアライズ/デシリアライズされるシリーズ、データフレームを受け取り/返却するUDFであり、ブロックごとに処理します。Pandas UDFは用途や入出力のタイプに応じてカテゴリ分けされるバリエーションが存在します: Series to Series, Series to Scalar, Iterator to Iteratorです。"}
{"category":"closed_qa","context":"","instruction":"なぜ不正検知が重要なのか？","response":"小売、金融、サイバーセキュリティなどいかなる業界においても、不正な挙動が発生したらすぐに検知することは間違いなく優先度が高いと言えます。この様な能力の欠如は、利益の損失、規制機関からの罰金、顧客プライバシーの侵害、サイバーセキュリティの場合にはセキュリティ侵害によって信頼が損なわれます。このため、このような比較的不自然な大量のクレジットカードのトランザクションの発見、怪しいユーザーの行動の特定、Webサービスにおける不自然なリクエストボリュームのパターンの識別によって、日々の業務を素晴らしいものにするのか、あるいはひどい災害になるのかを二分します。"}
{"category":"closed_qa","context":"","instruction":"不正検知における課題は何か？","response":"不正検知はいくつかの課題を突きつけます。1つ目は、「不正」とはどの様なものかというデータサイエンスの課題です。幸運なことに、機械学習がデータから正常なものから不正なパターンをどの様に区別するのかを学習するパワフルなツールを備えています。不正検知の場合、全ての不正がどの様なものであるのかを知ることは不可能なので、このためのリソースが利用できる場合でも機械学習モデルのトレーニングのためのデータセットをラベル付けすることは不可能です。このため、不正を検知するには、ラベル無しのデータからパターンを学習する教師無し学習が用いられてきました。完璧な教師なし不正検知機械学習モデルが分かったとしても、多くの場合、現実的な問題がスタートします。ソースシステムからデータが到着するとすぐに新たな観測事項が投入され、変換処理を経てモデルによって最終的なスコアが計算される様なプロダクション環境にモデルを投入するベストな方法とはどのようなものでしょうか？そして、5-10分のように短い周期あるいはニアリアルタイムで処理を行うにはどうしたらいいのでしょうか？これには、洗練されたETLパイプラインの構築、不正レコードを適切に識別することができる教師無し機械学習モデルのインテグレーションが含まれます。また、このエンドツーエンドのパイプラインは、プロダクションレベルである必要があり、データ取り込みからモデル推論に至るデータ品質を保証し、背後のインフラストラクチャを維持管理しなくてはなりません。"}
{"category":"closed_qa","context":"","instruction":"モデルトレーニングとトラッキングにおいてDatabricksはどの様に役に立つのか？","response":"Databricksで機械学習に関わることを行う際には、機械学習(ML)ランタイムのクラスターが必須となります。MLランタイムでは、データサイエンスや機械学習に関連するタスクで一般的に使用される数多くのオープンソースライブラリがデフォルトで利用できます。scikit-learnはこれらのライブラリの一つであり、アイソレーションフォレストアルゴリズムの素晴らしい実装を提供しています。このランタイムによって、機械学習エクスペリメントのトラッキング、モデルのステージング、デプロイメントのためのノートブック環境とMLflowの密なインテグレーションが実現されます。MLクラスターにアタッチされたノートブック環境で行われる全てのモデルトレーニンやハイパーパラメーターの最適化処理は、デフォルトで有効化されているMLflowのオートロギングによって自動で記録されます。モデルが記録されると、さまざまな方法でMLflowのモデルを登録、デプロイすることが可能になります。特に、Apache Spark™による分散ストリーム、バッチ推論におけるベクトル化ユーザー定義関数(UDF)としてこのモデルをデプロイするために、以下の画像のように、MLflowはUDFを作成、登録するためのコードをユーザーインタフェース(UI)上に生成します。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのモデルサービングでLLMをサポートしていますか？","response":"はい、サポートできるようになりました。MLflowでLLMを記録できるということは、MLflowのモデルサービングを使ってLLMをサービングできるのでは、そして、streamlitと組み合わせたらチャットボットを動かせるのではと思い立ちました。結論、いけました。"}
{"category":"closed_qa","context":"","instruction":"AI Functionsとは？","response":"AI FunctionsはビルトインのDB SQLの関数であり、SQLから直接の大規模言語モデル(LLM)へのアクセスを提供します。このローンチによって、馴染みのあるSQLインタフェースから皆様の企業のデータに対するLLMの実験をクイックに行えるようになります。適切なLLMプロンプトを開発したら、Delta Live Tablesやスケジュールされたジョブのような既存のDatabricksツールを活用したプロダクションパイプラインにクイックに転用することができます。これによって、LLMの開発、本格運用のワークフローの両方をシンプルにします。AI FunctionsはLLM呼び出しにおける技術的な複雑性を抽象化し、アナリストやデータサイエンティストは、背後のインフラストラクチャを気にすることなしに、これらのモデルを使い始めることができるようになります。"}
{"category":"closed_qa","context":"","instruction":"MLflowではHugging Faceトランスフォーマーをサポートしていますか？","response":"はい。大規模言語モデル(LLM)を管理、デプロイできる能力を拡張する革新的な機能が搭載された、このオープンソース機械学習プラットフォームの最新のアップデートであるMLflow 2.3が公開されました。この強化されたLLMサポートは以下を通じて提供されます:1. 3つの新たなモデルフレーバー: Hugging Face Transformers、OpenAI関数、LangChain, 2. モデルファイルのマルチパートのダウンロード、アップロードを通じたクラウドサービスとのモデルのダウンロード、アップロード速度の劇的な改善"}
{"category":"closed_qa","context":"","instruction":"MLflowでOpenAI APIをサポートしていますか？","response":"はい、MLflow2.3で関数ベースのフレーバーによるOpenAI APIがサポートされるようになりました。OpenAI Pythonライブラリによって、Pythong言語で記述されたアプリケーションからOpenAI APIに簡単にアクセスできるようになります。これには、OpenAI APIリソースへのマッピングを行う一連の事前定義積みクラスが含まれています。これら提供されるクラスを用いることで、さまざまなバージョンのモデルやOpenAI APIエンドポイントに対して、動的に接続を初期化し、データを引き渡し、レスポンスを取得します。MLflow OpenAIフレーバーは以下をサポートしています: 1. 自動シグネチャスキーマ検知 2. 推論を加速するための並列APIリクエスト 3. レートリミットエラーのような一時的なエラーにおける自動APIリクエストリトライ"}
{"category":"closed_qa","context":"","instruction":"MLflowでLangChainをサポートしていますか？","response":"はい、MLflow2.3でサポートしています。MLflowのLangChainフレーバーは、質問応答システムやチャットボットのようなLLMベースのアプリケーションの構築やデプロイのプロセスをシンプルにします。LangChainの高度な機能と、MLflowの効率化された開発、デプロイサポートを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"PyTorch MLモデルの分散トレーニングはサポートしていますか？","response":"はい、Apache Spark 3.4では、SparkクラスターでのPyTorchの分散トレーニングを行えるように、TorchDistributorモジュールが追加されます。内部では、環境とワーカー間のコミュニケーションチャネルを初期化し、ワーカーノード横断で分散トレーニングを実行するために、CLIコマンドtorch.distributed.runを活用します。このモジュールは、シングルノードマルチGPUとマルチノードGPUクラスターの両方における分散トレーニングジョブをサポートしています。"}
{"category":"closed_qa","context":"","instruction":"databricks-dolly-15kとは？","response":"databricks-dolly-15kは、ChatGPTの魔法のようなインタラクティブ性を大規模言語モデルが示せるようにするために、数千人のDatabricks従業員によって生成された15,000以上のレコードを含むコーパスです。Databricks従業員は、InstructGPTの論文で説明されている7つのカテゴリー、オープンエンドの自由記述カテゴリーを含む、8つの異なる指示カテゴリーのそれぞれでプロンプト/レスポンスを作成する取り組みに招待されました。コントリビューターは、Wikipedia以外のウェブのソースからの情報を使わないように指示され、指示やレスポンスを構成する際に生成型AIを使わないように明示的に指示されました。それぞれのカテゴリーに適切な質問と指示のタイプの動機づけを行うために、それぞれの挙動のサンプルが提供されました。データ生成プロセスの過程で、コントリビューターは他のコントリビューターによる質問に回答するオプションが提供されました。オリジナルの質問を言い換え、適切に回答できるだろうと予測できる質問のみを選択することが求められました。特定のカテゴリーでは、コントリビューターはWikipediaからコピーされた参照テキストを提供することが求められました。参照テキスト(実際のデータセットではcontextフィールドに示されています)には、括弧付きのWikipediaの引用番号(例 [42])が含まれることがあり、後段のアプリケーションではユーザーが削除することが推奨されます。"}
{"category":"closed_qa","context":"","instruction":"databricks-dolly-15kの指示カテゴリーの種類は？","response":"人間生成データ: Databricks従業員は8つの異なる指示カテゴリーのそれぞれでプロンプト/レスポンスのペアを作成する取り組みに容態されました。Wikipedia: アノテーターが参照テキストを必要とする指示カテゴリー(情報抽出、クローズドのQA、要約)においては、コントリビューターは特定の指示カテゴリーのサブセットにおいて、Wikipediaの文を選択しました。どのようにターゲットの文を選択するのかに関するガイドはアノテーターには与えられませんでした。"}
{"category":"closed_qa","context":"","instruction":"Dolly 2.0とは？","response":"Dolly 2.0はEleutherAIのpythiaモデルファミリーをベースとした12Bのパラメーターを持つ言語モデルであり、Databricks従業員によってクラウドソースされた人間の手で生成された新たな高品質な指示追従データセットのみを用いてファインチューニングされています。我々は、すべて商用利用できるトレーニングコード、データセット、モデルの重みを含むDolly 2.0のすべてをオープンソース化します。これは、すべての組織がサードパーティにデータを共有したり、APIアクセスに支払いをすることなしに、人々と会話できるLLMを作成、所有、カスタマイズできることを意味します。"}
{"category":"closed_qa","context":"","instruction":"Dollyになぜ新たなデータセットを作成したのか？","response":"Dolly 1.0をリリースするやいなや、試したい方々からのリクエストが殺到しました。ナンバーワンの質問は「これを商用利用できますか？」でした。Dolly 1.0やその他の指示追従LLMを作成する重要なステップは、指示とレスポンスのペアを用いてモデルをトレーニングすることです。Dolly 1.0はスタンフォードのAlpacaチームがOpenAI APIを用いて作成したデータセットを用いて$30でトレーニングされました。このデータセットにはChatGPTのアウトプットが含まれており、スタンフォードチームが指摘しているように、利用条項はOpenAIと競合するモデルの作成を許可していません。このため、残念ですが、共通した質問に対する回答は「おそらくダメです！」でした。我々の知る限り、既存のよく知られた指示追従モデル(Alpaca、Koala、GPT4All、Vicuna)は、商用利用を禁ずるこの制限に苦しんでいます。この難問に対処するために、我々は商用利用できるような、まだ「トレーニング」されていない新たなデータセットを作成する方法を模索し始めました。"}
{"category":"closed_qa","context":"","instruction":"どのように作成したのか？","response":"我々は、OpenAIの研究論文から、InstructGPTモデルは13,000個の指示追従の挙動を示す内容から構成されるデータセットでトレーニングされたことを理解しています。これからインスピレーションを得て、Databricks従業員が先頭に立つことで同様の成果を達成できないかを検討することにしました。結果として、13kの質問と回答を生成することは見かけよりも困難であることがわかりました。すべての回答はオリジナルではなくてはなりません。ChatGPTやウェブのどこかからコピーしてはならず、そうでないと我々のデータセットが汚染されることになります。うんざりするような話でしたが、DatabricksにはLLMに非常に興味を持っている5,000人以上の従業員がいます。そして、OpenAIの40人のラベル付け担当者よりも、さらに高品質なデータセットを作成するために、彼らの間でクラウドソースすることを検討しました。しかし、彼らはすべて多忙であり、山盛りの仕事を抱えていることを知っていましたので、これを行うためのインセンティブの方法が必要でした。我々は、トップ20のラベラーがすごい賞を受け取れるコンテストをセットアップしました。また、7つの固有のタスクを説明したました: 1. オープンなQ&A: 例えば、「なぜ人々はコメディ映画を好むのか？」や「フランスの首都は？」あるケースにおいては、これには明確な回答はなく、他のケースにおいては、図示や広範な世界に対する知識を必要とします。2. クローズドなQ&A: これは参照テキストの文に含まれる情報のみを用いて回答できる質問です。例えば、原子に関するWikipediaの段落に関して、「原子核における陽子と中性子の比率は？」と尋ねるというものです。3. Wikipediaからの情報抽出: ここでは、アノテーターはWikipediaから段落をコピーし、文からエンティティやその他の重みや測定結果のような事実情報を抽出します。4. Wikipediaからの情報の要約: ここでは、アノテーターにWikipediaの文が渡され、短い要約にまとめることが求められます。5. ブレーンストーミング: このタスクでは、終わりのないアイディエーションと可能性のあるオプションとの関連付けが求められます。例えば、「今週末に友達とできる面白いアクティビティは？」と言ったものです。6. 分類: このタスクでは、アノテーターはクラスのメンバーシップ(例: このアイテムは動物リスト、鉱物リスト、野菜リストに含まれますか)や、感情や映画レビューのような短い文のプロパティに対する判断を求められます。7. クリエイティブな記述: このタスクには、詩やラブレターの記述のようなものが含まれます。"}
{"category":"closed_qa","context":"","instruction":"Dolly 2.0をどうすれば使い始められますか？","response":"Dolly 2.0の重みをダウンロードするには、シンプルにDatabricks Hugging Faceのページを訪れ、databricks-dolly-15k datasetをダウンロードするにはDolly repo on databricks-labsにアクセスしてください。そして、ご自身の組織でLLMを活用する方法を知るには、我々のウェビナーにご参加ください。"}
{"category":"closed_qa","context":"","instruction":"Oracle Database プログラムの移行方法は何か？","response":"データの取り込み、データの提供（クエリ実行）を Databricks へ移行し、Oracle Database のストアードプロシージャ等のプログラムを Azure SQL Database へ移行する。Azure SQL Database へ移行を行う際には、Microsoft 社が提供している移行ツールである SQL Server Migration Assistant for Oracle(SSMA) を利用する。1. プロビジョニング済みとサーバーレスの切り替えが可能であり、コスト最適化の実施が可能であること, 2. SMP型DWHとして利用されている実績があるデータベースエンジンであること, 3. 移行化ツールが Microsoft 社から提供されており、ツール利用に関する Azure サポートを受けられること。PL/SQL のプログラムにてロジックが少なく SELECT 文をベースにした処理である場合には、SQL Server Migration Assistant for Oracle(SSMA)の移行先としてサポートしている Azure Synapse Analytics 専用 SQL プールへの移行も検討の余地がある。ただし、Microsoft 社のドキュメントにて次のような記載がある通り、行ごとの処理が多数ある場合には不向きである。"}
{"category":"closed_qa","context":"","instruction":"どうしてDatabricksクラスターポリシー？","response":"Databricksクラスターポリシーを用いることで、管理者は: 1. エンドユーザーのクラスター設定を制御することでコストを抑えることができます。2. エンドユーザーがクラスターを作成しやすくなります。3. コスト管理のためにワークスペース横断でタグを強制できます。また、3つの主要なユースケース向けに事前定義されたクラスターポリシーを提供します: パーソナルユース、共有利用、ジョブです。また、管理者は自分のポリシーをカスタマイズしたり、事前定義済みのポリシーを編集することができます。"}
{"category":"closed_qa","context":"","instruction":"Apache Spark構造化ストリーミングにおけるウォーターマーキングとは？","response":"一般的に、リアルタイムのストリーミングデータを取り扱う際、データの取り込み方法やアプリケーション全体がダウンタイムのような問題を体験するかどうかによって、イベント時間と処理時間の間には遅れが生じます。これらの潜在的に変動する遅延によって、このデータの処理に使用するエンジンには、いつ集計ウィンドウをクローズし、集計結果を生成するのかを決定するメカニズムが必要となります。生成したい集計結果を適切なものにするために、Sparkが集計ウィンドウをいつクローズし、適切な集計結果を生成するのかを理解できるようにウォーターマークを定義する必要があります。構造化ストリーミングアプリケーションにおいては、ウォーターマーキングと呼ばれる機能を用いることで、計算したい集計結果に適したすべてのデータが収集されるようにすることができます。基本的には、ウォーターマークを定義することで、Spark構造化ストリーミングは(一連の予想遅延期間に基づく)ある時点Tまでのすべてデータがいつ取り込まれたのかを認識することができるので、タイムスタンプTまでのウィンドウ集計をクローズし、結果を生成することができます。"}
{"category":"closed_qa","context":"","instruction":"レイクハウスとは何ですか？","response":"Databricksが提供するプラットフォームの名称です。Databricksが作った造語であり、由来は従来型データプラットフォームであるデータウェアハウスとデータレイクであり、両者の長所を兼ね備えたプラットフォームであるという意味を込めてレイクハウスとしています。"}
{"category":"closed_qa","context":"","instruction":"Databricksの想定されるエンドユーザーは誰ですか？","response":"データエンジニア、データサイエンティスト、SQLアナリスト、MLエンジニアなど機械学習/AIプロジェクトに関わる方すべてです。なお、SQL/Python/Rの知識を有している方を主たるユーザーとしていますが、最近ではシチズンデータサイエンティストの方向けの機能を提供するように機能を拡張しています。"}
{"category":"closed_qa","context":"","instruction":"日本語のドキュメントはありますか？","response":"鋭意準備中です。以下のドキュメントもご活用ください。https://qiita.com/taka_yayoi/items/125231c126a602693610、https://learn.microsoft.com/ja-jp/azure/databricks/"}
{"category":"closed_qa","context":"","instruction":"Databricksプラットフォームはどのようなアーキテクチャですか？","response":"Databricksはコントロールプレーンとデータプレーンから構成されています。コントロールプレーンには、DatabricksのAWSアカウント上でDatabricksが管理するバックエンドサービスが含まれます。あなたが実行する全てのコマンド(ノートブックのセル)は、完全に暗号化された状態でコントロールプレーンに存在することになります。保存されたコマンドはデータプレーンに格納されます。\n データプレーンはあなたのAWSアカウントで管理され、あなたのデータが格納される場所になります。また、ここでデータが処理されます。この図では、既にデータがDatabricksに登録されていることを仮定していますが、イベントデータ、ストリーミングデータ、IoTデータなど外部のデータソースからデータを取り込むことが可能です。Databricksコネクターを用いることで、あなたのAWSアカウント外の外部データソースにも接続することができます。\n あなたのデータは常に、データプレーンのあなたのAWSアカウントに存在します。このため、あなたのデータがロックインされることなく、常にフルコントロール、オーナーシップを維持することができます。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeとは何ですか？","response":"レイクハウスの根幹とも呼べるコンポーネントがDelta Lakeです。Delta Lakeはデータレイクの抱える課題を解決するために、データウェアハウスで培われた技術をデータレイクに提供します。\n データウェアハウスでは当たり前のACIDトランザクションをデータレイクで実現することで、データが不完全な状態になることを回避します。\n データベースで当たり前のスキーマ適用を行うことで、スキーマに合致しないデータの流入を防ぎます。\n データベースで培われたインデックス、パーティショニングをデータレイク上で実現することで、大規模データに対する高速な処理を実現します。"}
{"category":"closed_qa","context":"","instruction":"Sparkとは何ですか？","response":"Databricksの創業者が開発した分散処理フレームワークです。Databricksにおける処理エンジンとなっています。Sparkを活用することで、大量データに対するETL処理、機械学習、BIを高速に実施することが可能となります。"}
{"category":"closed_qa","context":"","instruction":"MLflowとは何ですか？","response":"エンドツーエンドの機械学習モデル管理をサポートするソフトウェアです。トレーニングされたモデルを自動でトラッキングし、本格運用としてモデルを簡単にデプロイすることが可能となります。"}
{"category":"closed_qa","context":"","instruction":"Databricksでファイルはどの様に管理されますか？","response":"Databricksで取り扱うファイルは、Databricksのワークスペースを構築する際に指定した(1)S3バケット(DBFSルートバケットと呼ばれます)に格納されます。加えて、稼働しているクラスターの(2)ローカルストレージにファイルを保存することができますが、こちらはクラスターが停止すると全て失われます。"}
{"category":"closed_qa","context":"","instruction":"データベースの様な使い方はできますか？","response":"できます。Hiveメタストアにデータを登録することでSQLベースでのデータ操作が可能となります。"}
{"category":"closed_qa","context":"","instruction":"matplotlib/seabornなどを用いて可視化を行うことができますか？","response":"できます。さらにはビルトインの可視化機能を用いてお手軽にグラフを作成することができます。"}
{"category":"closed_qa","context":"","instruction":"画像データを取り扱うことはできますか？","response":"できます。専用のデータソースを活用することで、画像サムネイルを簡単に参照することもできます。"}
{"category":"closed_qa","context":"","instruction":"テキストデータを取り扱うことはできますか？","response":"できます。Sparkを活用することで、大量テキストデータを高速に処理することも可能です。"}
{"category":"closed_qa","context":"","instruction":"Githubと連携できますか？","response":"できます。Reposをご活用ください。"}
{"category":"closed_qa","context":"","instruction":"VSCodeなどのIDEをローカルで使いたいのですが","response":"VScodeをサポートしています。Databricks Connectを使うことで他のIDEからDatabricksクラスターを利用いただけます。"}
{"category":"closed_qa","context":"","instruction":"AWSでDatabricksを導入したいのですがどうしたら良いですか？","response":"以下の手順に従って環境を導入ください。複雑な構成が必要な場合にはDatabricksまでご連絡ください。導入をサポートします。https://qiita.com/taka_yayoi/items/fb4f57c069e1f272e88a"}
{"category":"closed_qa","context":"","instruction":"AzureでDatabricksを導入したいのですがどうしたら良いですか？","response":"マーケットプレースから導入してください。https://learn.microsoft.com/ja-jp/azure/databricks/getting-started/?tabs=azure-portal"}
{"category":"closed_qa","context":"","instruction":"GCPででDatabricksを導入したいのですがどうしたら良いですか？","response":"マーケットプレースから導入してください。https://qiita.com/taka_yayoi/items/cff8460e3864121a0e4a"}
{"category":"closed_qa","context":"","instruction":"ソリューションアクセラレータとは何ですか？","response":"Databricksがお客様との共創を通じて生み出したソリューションを他の会社の方でも利用できる様にインターネットに公開しているサンプルノートブックと説明資料(ブログ記事)です。"}
{"category":"closed_qa","context":"","instruction":"ソリューションアクセラレータは有償ですか？","response":"いいえ、すべて無料です。"}
{"category":"closed_qa","context":"","instruction":"Databricksでユーザーはどの様に管理されますか？","response":"ユーザー、グループ単位で管理されます。アイデンティティプロバイダーと連携したシングルサインオンもサポートしています。"}
{"category":"closed_qa","context":"","instruction":"Databricks上のオブジェクト(ノートブック、フォルダなど)のセキュリティはどうなっていますか？","response":"ロールベースのアクセス権管理が可能です。"}
{"category":"closed_qa","context":"","instruction":"閉域網で環境を構築したいのですがどうしたら良いですか？","response":"AWSであればPrivateLinkを活用して閉域網で環境を構築することができます。"}
{"category":"closed_qa","context":"","instruction":"データベースのテーブルの行列レベルでアクセス制御を行うことはできますか？","response":"できます。テーブルアクセスコントロールを有効化したクラスターを使用してください。"}
{"category":"closed_qa","context":"","instruction":"クラウドストレージ上のファイルに対するアクセス制御を行うことはできますか？","response":"できます。インスタンスプロファイル、あるいはIAMクレディンシャルパススルーをクラスターに設定してください。"}
{"category":"closed_qa","context":"","instruction":"ユーザーの操作履歴など監査ログを記録することはできますか？","response":"はい、監査ログの設定をしていただくことでDatabricks上のユーザーの操作を記録することができます。"}
{"category":"closed_qa","context":"","instruction":"アクセストークン、パスワードなど機密性の高い情報をノートブックに記載したくないのですがどうしたら良いですか？","response":"シークレットに格納してください。"}
{"category":"closed_qa","context":"","instruction":"価格体系はどうなっていますか？","response":"従量課金と年額事前契約があります。前者はクレジットカードの登録が必要となります。後者はプロフェッショナルサービスによるサポート、ディスカウントのメリットがあります。"}
{"category":"closed_qa","context":"","instruction":"どの様なケースでDatabricksの課金が発生しますか？","response":"クラスターが起動している期間のみDBU(Databricks Unit)に基づく課金が発生します。クラスターを起動しなければ、Databricksからの課金は発生しません。なお、インスタンス、ネットワーク、ストレージなどクラウドプロバイダーの課金が別途発生します。"}
{"category":"closed_qa","context":"","instruction":"無償で機能を試すことはできますか？","response":"できます。機能制限のあるDatabricks Community Editionあるいは、2週間のフリートライアル期間でフル機能を試していただくことが可能です。"}
{"category":"closed_qa","context":"","instruction":"毎回クラスターを停止する必要がありますか？","response":"いいえ、自動停止(Auto Termination)の設定をしていただければ、クラスターが利用されていない状態が一定期間継続した場合に自動で停止されます。"}
{"category":"closed_qa","context":"","instruction":"どのようにクラスターを設定したら良いのでしょうか？","response":"ベストプラクティスを参照してください。https://qiita.com/taka_yayoi/items/ef3dc37143e7b77b50ad"}
{"category":"closed_qa","context":"","instruction":"定期的に処理を実行したいのですが。","response":"ジョブを活用ください。"}
{"category":"closed_qa","context":"","instruction":"リモートワーク主体になっているのですがDatabricksは使えますか？","response":"使えます。むしろ、各種コラボレーション機能はリモートワークのシーンにおいて威力を発揮します。"}
{"category":"closed_qa","context":"","instruction":"人的リソースが少ないのですがDatabricksは使えますか？","response":"SaaSで提供されるDatabricksでは、インフラ管理の工数を劇的に引き下げることができます。1名の管理者で複数部門にサービスを提供した事例もあります。また、分析者はコラボレーション機能、プロジェクト管理機能を活用することで、少ない分析者リソースであっても複数のAIプロジェクトを回すことができます。"}
{"category":"closed_qa","context":"","instruction":"Jupyter Notebookを使っているのにDatabricksを利用するメリットはありますか？","response":"Jupyter notebookと比較したデータブリックスのメリットをご説明します。\n 1. Jupyter notebookは主にEDAと機械学習モデル構築に焦点を置いていますが、データブリックスでは分析の前段のデータ加工からデータパイプラインの構築、モデルの構築からデプロイ、ひいては経営層向けに提示するBIレポート作成までもカバーしています。\n 2. クリックだけの操作でSparkクラスターを起動することができます。フルバージョンのデータブリックスでは、任意のスペックのクラスターを構成でき、オートスケーリングを活用することができます。\n 3. データベースのインストールが不要です。データブリックスでは最初からデータベースとしてHiveメタストアを利用できます。以下ではSQLを用いてデータベースを操作してみます。%sqlに関しては後ほどご説明します。\n 4. リモートワークでのコラボレーションが簡単です。遠隔地であってもリアルタイム、時間差でのコミュニケーションを容易に行うことができます。\n 5. データブリックスはノートブックに対する基本的なバージョン管理機能を提供します。画面右上のRevision historyから過去のノートブックを参照できます。それぞれのバージョンに対して、コメントの追加、復旧、バージョンの削除、バージョン履歴の削除を行うことができます。Githubのリポジトリと連携することもできます。\n 6. プログラミング言語を柔軟に切り替えることができます。\n 7. データを簡単に可視化できます。\n 7. 大量データを高速に処理できます。"}
{"category":"closed_qa","context":"","instruction":"コミュニティエディションの制限は何ですか？","response":"フルバージョンのデータブリックスと比較してコミュニティエディションには以下の制限があります。1. 作成できるクラスターは15GB RAM、2 Core CPUのシングルノードのみ, 2. ワークスペースに追加できるユーザー数は最大3名まで, 3. クラスターのリージョンはus-westのみ, 4. 使用できない機能, 5. ジョブのスケジュール機能, 6. クラスターのオートスケーリング機能, 7. Git連携, 8. MLflowの一部モデル管理機能(レジストリ、RESTサービング), 9. REST APIによるワークスペースの制御, 10. セキュリティ、ロールベースのアクセス制御、監査、シングルサインオン, 11. BIツール連携のサポート"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeは他のトランザクションストレージレイヤーと何が違うのですか？","response":"Delta Lakeの他のトランザクションストレージレイヤーソフトウェアも類似の課題を解決するためのものですが、Delta Lakeはデータエコシステムにおけるより広いユースケースをカバーしています。データレイクに、信頼性だけでなく、高性能、セキュリティを追加し、バッチ処理、ストリーミング処理を統合するフレームワークを提供します。また、データ変換パイプラインの効率を改善するだけではなく、BI、データサイエンス、機械学習と言った下流のアクティビティの効率も改善できます。DatabricksのDelta Lakeを活用することで、Deltaエンジンによるよる良いパフォーマンス、きめ細かいアクセスコントロールによる高セキュリティ・ガバナンス、多くのBIツールに対する高性能ネイティブコネクターによるエコシステムの拡大と言った様々な恩恵を享受することができます。最後に、Delta Lakeは世界中の数千のお客様によって、3年以上の実運用に耐え続けている実績を持っています。毎日、少なくとも3ペタバイトのデータがDelta Lakeに投入されています。"}
{"category":"closed_qa","context":"","instruction":"どのようにDelta Lakeにデータを追加するのですか？","response":"Delta Lakeへのデータ追加は非常に簡単です。DatabricksオートローダーあるいはSQLのCOPY INTOコマンドを使って、既存のデータレイク(S3やADLS)にデータを追加するたびに自動でDelta Lakeにデータが追加されます。また、Apache Spark™を用いて、バッチでデータを読み取り、変換処理を行い、結果をDeltaフォーマットで書き込むということも可能です。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのDelta LakeはBIやレポート作成のユースケースに適していますか？","response":"はい、Delta LakeはBIやレポート作成にも活用できます。特にこのデータ分析ユースケースに対応するために、我々は最近SQL Analytics(現在パブリックプレビュー)のリリースを発表しました。SQL Analyticsはデータレイクに直接アクセスして分析を行うBIユースケースに特化して設計されています。もし、データレイクに対してクエリを実行する多くのユーザーをお持ちなのであれば、SQL Analyticsに目を通して見ることをお勧めします。ビルトインされているクエリ、ダッシュボード機能を利用することもできますし、最適化されたネイティブコネクターを介して、既にお使いのBIツールを利用することも可能です。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeはデータエンジニアリング以外に、機械学習の作業を支援しますか？","response":"はい、Delta Lakeは、あなたがお使いのデータセットに対するバージョン管理機能を提供します。これは、再現性の観点から非常に重要な機能です。あなたのモデルを特定のバージョンのデータセットに紐付けできることは非常に重要です。これによって、他のメンバーが検証目的でトレーニングを再現し、同じ結果が得られることを確認できます。つまり、全く同一のデータ、あなたがトレーニングに使用したのと全く同じバージョンのデータでトレーニングを再現できるようになります。Databricksにおけるデータサイエンス、機械学習に関してはこちらを参照ください。"}
{"category":"closed_qa","context":"","instruction":"コンプライアンスの文脈でDelta Lakeを活用できますか？Delta Lakeでは、GDPRやCCPAに準拠するために、どのように以前のバージョンのデータを削除できますか？","response":"Delta Lakeはデータレイクにあるファイルにおける特定のレコードを削除する機能を持っており、これはCCPAやGDPRのような規制に対して大きな意味を持ちます。対象を削除する際、ビジネスサイドは多くのケースでクラウドのオブジェクトストレージ上のデータまでも削除することを要求します。Databricksによってデータが管理されるDelta Lake管理テーブルを活用することで、削除処理はあなたのクラウドオブジェクトストレージまで伝播します。"}
{"category":"closed_qa","context":"","instruction":"セキュリティ、ガバナンスの目的でアクセスコントロールを課することはできますか？","response":"はい、DatabricksのDelta Lakeに加えて、ワークスペースオブジェクト(フォルダー、ノートブック、エクスペリメント、モデル)、クラスター、プール、ジョブ、データスキーマ、テーブル、ビューなどに対して、アクセス権限を管理するアクセスコントロールリスト(ACL)を活用できます。管理者、あるいは権限が移譲されたユーザーがアクセスコントロールを制御できます。Databricksにおけるデータガバナンスのベストプラクティスに関しては、こちらを参照ください。https://docs.databricks.com/data-governance/best-practices.html"}
{"category":"closed_qa","context":"","instruction":"どのようにストリーミング、バッチ処理に対応できますか？","response":"Delta Lakeにおいては、バッチ、ストリーミング処理の両方を実行することができ、複雑性、冗長性、運用の課題を回避してアーキテクチャをシンプルにできます。Delta Lakeのテーブルはバッチ処理におけるテーブル、ストリーミング処理のソースとシンクになることができます。ストリーミングによるデータの投入、バッチによる履歴データの補完、インタラクティブなクエリ全てがDeltaテーブルで動作し、直接Sparkの構造化ストリーミングと統合できます。\n 以上がイベントにおいてお客様からいただいた質問のほんの一部です。ライブイベントに参加された方には、スケーラブルかつコスト効率の高いレイクハウスの基盤であるDelta Lakeを学ぶ時間を割いていただいたことを感謝します。参加されなかった皆様においては、レコーディングを参照いただければと思います。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeとは何で、レイクハウスにおいてどのような役割ですか？","response":"Delta Lakeは、お使いのデータレイクに信頼性、セキュリティ、パフォーマンスをもたらすオープンフォーマットのストレージレイヤーであり、ストリーミング処理、バッチ処理の両方に対応しています。Delta Lakeは、分析をシンプルにし、企業全体でデータにアクセスできるようにするために、構造化、準構造化、非構造化データに対する単一の場所を提供することでデータのサイロ化を排除します。最終的には、Delta Lakeはコスト効率の良い、高度にスケーラブルなレイクハウスアーキテクチャの基盤かつイネーブラとなります。"}
{"category":"closed_qa","context":"","instruction":"なぜDelta Lakeと呼ばれるのですか？","response":"Delta Lakeと名付けた理由は二つあります。最初の理由は、Delta Lakeはデータに対する、変更あるいはdelta(差分)を追跡し続けます。第二の理由は、Delta Lakeはお使いのデータレイクに流れ込むデータをフィルタリングする「デルタ(三角州)」として動作するからです。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでDelta Lakeテーブルを作成して、オープンソースのSparkでアクセスできますか？","response":"はい、それを行うためには、オープンソースのSparkとDelta Lakeをインストールします。DatabricksのDelta Lakeは、Databricksでのみ利用できるDeltaエンジンによって、オープンソースのDelta Lakeより高速なものとなっています。詳細はこちらの記事を参照ください。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeのファイルフォーマットは何ですか？","response":"Delta Lakeで使用されるファイルフォーマットは、parquetとJSONの組み合わせであるdeltaと呼ばれるものです。Delta Lakeのテーブルは、テーブルのコンテンツとトランザクションログ(およびチェックポイント)のデータオブジェクトを保持するクラウドオブジェクトストア、あるいはファイルシステム上のディレクトリです。"}
{"category":"closed_qa","context":"","instruction":"SQL Analytics(現Databricks SQL)とは何ですか？","response":"Databricks SQLは、データレイク上のDelta Lakeテーブルにクエリーを実行できる、馴染みのあるSQLベースの環境を使用する、データアナリスト専用の新たなワークスペースです。Databricks SQLは完全に独立したワークスペースなので、データアナリストはノートブックベースのデータサイエンスツール(実際、データサイエンティストはSQLエディタで作業することも好みますが)に悩まされることなしに、Databricksのプラットフォームで直接作業することができます。しかし、データアナリストとデータサイエンティストは同じデータソースで作業をするので、全体的なインフラストラクチャは非常に簡素化され、信頼できる唯一の情報源を維持することができます。\n Databricks SQLで以下のことが可能となります。\n 1. データレイク上の完全かつ最新なデータに対してクエリーが行えるように、お使いのTableau、Microsoft Power BIのようなBIツールとの統合。\n 2. Databricksでデータアナリスト、データサイエンティストが直接データレイクにクエリーできるようにするための、既存のBIツールを補完するSQLネイティブのインタフェース。\n 3. データにおける重要な変化に関するアラート、豊富なビジュアライゼーションとドラッグ&ドロップによるダッシュボードを通じたクエリーの共有。\n 4. 最新かつ完全なデータを用いた従来の分析ワークロードをサポートするために、お使いのデータレイクに高信頼、高品質、スケーラビリティ、セキュリティ、パフォーマンスを提供"}
{"category":"closed_qa","context":"","instruction":"Delta LakeにおけるSQLパフォーマンスの詳細はどこで学べますか？","response":"Databricks SQL、Delta、そして(パフォーマンスを含む)レイクハウスアーキテクチャを学ぶためには、こちらの無料のトレーニングをチェックしてください。トレーニングでは、データ管理の進化とレイクハウスを学びます。このモデルを用いることで、どのようにして、信頼できる唯一の情報源に支えられた、高性能のストリーミング、データサイエンス、機械学習、BIが統合されたシステムをデータチームが活用できるのかを説明します。\n このセッションのハンズオンでは、統合されたSQLエディタ、ダッシュボードツールであるDatabricks SQLを活用できるのかを学びます。お客様の会社のデータに対してどれだけ容易にクエリーを実行できるのか、ダッシュボードを構築できるのかを探索します。そして、Databricks SQLが、レイクハウスインフラストラクチャにおいて、どのようにデータが使用され、アクセスされたのかに関して、きめ細かい可視性を提供するのかを理解することができます。"}
{"category":"closed_qa","context":"","instruction":"クラウドベースのHadoopをDatabricksレイクハウスプラットフォームに移行する理由は何ですか？","response":"クラウドベースのHadoopプラットフォームをDatabricksレイクハウスプラットフォームに移行する理由は７つあります。\n\n以下に、\n\nレイクハウスプラットフォームでご自身のアーキテクチャをシンプルに\n集中管理されるデータガバナンスとセキュリティ\nすべてのデータワークロードにおける最高のパフォーマンス\n生産性の改善とビジネス価値の向上\nデータとAIによるイノベーションの促進\nクラウドに依存しないプラットフォーム\nDatabricksパートナーエコシステムのメリット\n"}
{"category":"closed_qa","context":"","instruction":"MLflowでLangChainのサポートをサポートしていますか？","response":"はい、MLflow 2.3からLangChainのサポートができるようになりました。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのWebターミナルは？","response":"DatabricksのWebターミナルは、Sparkドライバーノードでシェルコマンドを実行したり、VimやEmacsのようなエディタを使用できる便利で高度にインタラクティブな手段を提供します。SSHの使用とは異なり、Webターミナルは、一つのクラスターで多数のユーザーによって使用でき、キーのセットアップは不要です。Webターミナルの使用例には、リソース使用量の監視やLinuxパッケージのインストールが含まれます。"}
{"category":"closed_qa","context":"","instruction":"Databricksで不正レコード、不正ファイルをどのように取り扱うか？","response":"Databricksでは、Sparkジョブを阻害することなしに不正なレコードやファイルを取り扱うための統合インタフェースを提供しています。データソースのオプションbadRecordsPathを指定することで、例外のログから例外レコード/ファイルや理由を取得することができます。badRecordsPathでは、CSVやJSONソースにおける不正レコードに関する情報やすべてのファイルベースのビルトインソース(Parquetなど)の不正ファイルを記録する例外ファイルのパスを指定します。\n\nさらに、ファイルを読み込む際、ネットワーク接続例外、IO例外などの一時的なエラーが起きる場合があります。これらのエラーは無視されますが、badRecordsPathで記録され、Sparkはタスクの実行を継続します。\n\n注意\nファイルベースのデータソースにおけるbadRecordsPathオプションの使用においては、いくつか重要な制限が存在します。\n\n非トランザクションであり、一貫性の無い結果につながる場合があります。\n一時的なエラーは失敗として取り扱われます。"}
{"category":"closed_qa","context":"","instruction":"MLflowエクスペリメントへどうやってアクセスするか？","response":"MLflowエクスペリメントデータソースは、MLflowエクスペリメントのランデータをロードするための標準APIを提供します。ノートブックエクスペリメントから、あるいはエクスペリメント名、エクスペリメントIDを用いてデータをロードすることができます。"}
{"category":"closed_qa","context":"","instruction":"ノートブックエクスペリメントはどの場合に作成されますか？","response":"ノートブック上でmlflow.start_run()コマンドを実行した場合、ランはアクティブなエクスペリメントにメトリクスやパラメーターを記録します。アクティブなエクスペリメントが存在しない場合、Databricksはノートブックエクスペリメントを作成します。ノートブックエクスペリメントは対応するノートブックと同じ名前とIDを持ちます。ノートブックのURL(英語)の最後にある数字のIDがノートブックIDです。"}
{"category":"closed_qa","context":"","instruction":"MLflowのエクスペリメントには何種類がありますか？","response":"エクスペリメントには2種類あります。ワークスペースエクスペリメントとノートブックエクスペリメントです。\n\nMLflow APIかワークスペースUIからワークスペースエクスペリメントを作成できます。ワークスペースエクスペリメントはノートブックに紐付けられておらず、いかなるノートブックでもエクスペリメントIDかエクスペリメント名を指定することでランをこれらのエクスペリメントに記録することができます。\nノートブックエクスペリメントは特定のノートブックに紐付けられています。mlflow.start_run()を実行した際にアクティブなエクスペリメントが存在しない場合には、Databricksは自動でノートブックエクスペリメントを作成します。"}
{"category":"closed_qa","context":"","instruction":"SQLで何ができる？","response":"データベースにあるデータに対するすべての操作が行えると言っても過言ではありません。できることが多岐に渡るため、ある程度の分類がされています。\n\nDDL(Data Definition Language)\nデータベース、テーブルなどを作成、削除します\nCREATE/DROP/ALTER\nDML(Data Manipulation Language)\nテーブルを検索したり、変更します\nSELECT/INSERT/UPDATE/DELETE\nDCL(Data Control Language)\nテーブルに対する権限設定などを行います\nGRANT/REVOKE\n"}
{"category":"closed_qa","context":"","instruction":"テーブルの更新に使うSQLを教えてください。","response":"以下のSQLを実行してテーブルを更新していきます。\n\nINSERT\nUPDATE\nDELETE\nSELECT AS OF"}
{"category":"closed_qa","context":"","instruction":"SQLでテーブルの更新処理には注意事項は何でしょうか？","response":"SQLでの更新処理には以下の注意事項があります。\n誤ってデータを消してしまった、上書きしてしまったということを避けることは正直困難です。\nそんな場合に頼りになる機能があります。Delta Lakeのタイムトラベルです。\nここまで操作してきたテーブルの実態はDelta Lakeというソフトウェアによって実装されているものです。\nDelta Lakeはトランザクション保証、インデックスなどの機能がサポートされていますが、上の様なケースで役立つのがデータのバージョン管理であるタイムトラベルです。\nDelta Lakeのテーブルに実行される更新処理はすべて記録され、任意のタイミングにロールバックすることができます。"}
{"category":"closed_qa","context":"","instruction":"これまでのテーブル更新処理をみたい場合に使うSQLは何？","response":"テーブルの更新履歴を確認する時に、「DESCRIBE HISTORY」を使います。"}
{"category":"closed_qa","context":"","instruction":"SQLの読み方を教えてください。","response":"英語の文法としてSQLを読むと理解しやすいです。\n\nSQLの柔軟性は素晴らしいものですが、その代償としていくらでも複雑になり得ます。\n複雑なSQLを理解するには、\nフォーマットして可読性を上げ、\n英語の文を読む語順 (SELECT -> FROM -> WHERE -> GROUP BY -> HAVING -> ORDER BY) でブロックを把握し、\nFROMに記述されているテーブルから、\nWHEREに記述されている抽出条件に基づいてレコードを抽出し、\nGROUP BYに指定されている集約キーでグルーピングし、\nHAVINGに記述されている条件に基づいてグループを絞り込み、\nORDER BYに指定されているソートキーで並び替えて、\nSELECTに指定されている列\nを取得する。"}
{"category":"closed_qa","context":"","instruction":"SQLをDatabricksで使用するメリットは何？","response":"Databricksでは、以下のメリットを享受いただきながらSQLを活用いただくことができます。\nDatabricks SQL\nノートブックでのSQLサポート\nSQLの結果をダッシュボード、機械学習などとシームレスに連携\n並列分散処理による高速化\n高度な要件に答えるSQL拡張"}
{"category":"closed_qa","context":"","instruction":"MLflowとは？","response":"MLflowは機械学習モデルのライフサイクル管理のためのフレームワークを提供するソフトウェアです。機械学習のトラッキング、集中管理のためのモデルレジストリといった機能を提供します。Databricksでは、マネージドサービスとしてMLflowを利用できる様になっていますので、Databricksノートブック上でトレーニングした機械学習は自動でトラッキングされます。"}
{"category":"closed_qa","context":"","instruction":"Apache Sparkとは？","response":"Apache Sparkは、大規模なデータの高速リアルタイム処理を実現するオープンソースのクラスタコンピューティングフレームワークです。大量なデータを並列で処理することで、非常に高いパフォーマンスを発揮することができます。データ加工だけでなく、機械学習モデルのトレーニングやハイパーパラメーターチューニングを並列処理することが可能です。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeとは？","response":"https://qiita.com/taka_yayoi/items/5a804e53c60a55fb7340"}
{"category":"closed_qa","context":"","instruction":"機械学習(ML)プロジェクトとは？","response":"MLプロジェクトは機械学習(ML)モデルを構築することが目的ではなく、ビジネス課題を解決するために立ち上げるのが一般的です。"}
{"category":"closed_qa","context":"","instruction":"MLflowによる機械学習モデルのライフサイクル管理はどの範囲でしょうか？","response":"MLflowを活用することで、データサイエンティストやMLエンジニアによる機械学習モデルの実験段階から本格運用までをサポートすることができます。"}
{"category":"closed_qa","context":"","instruction":"機械学習プロジェクトにおけるMLflow/Spark/Delta Lakeの価値は何？","response":"MLflow、Spark、Delta Lake、これらすべては、データサイエンティスト、MLエンジニアの皆様が、生産性高く機械学習の取り組みを進められる様にするためのものです。"}
{"category":"closed_qa","context":"","instruction":"ParquetやIcebergテーブルのDelta Lakeテーブルへの変換処理をどのように実行しますか？","response":"SQLコマンド「CONVERT TO DELTA」を利用して、ParquetやIcebergテーブルのDelta Lakeテーブルへの変換処理を実行します。ParquetやIcebergテーブルのDelta Lakeへのインクリメンタルな変換に関しては、ParquetやIcebergからDelta Lakeへのインクリメンタルなクローンをご覧ください。https://qiita.com/taka_yayoi/items/bc3edb691c6f1fc7c79a"}
{"category":"closed_qa","context":"","instruction":"ParquetやIcebergデータのインクリメンタルな取り込みのためにいつクローンを使うべきか","response":"以下のシチュエーションにおいてParquetやIcebergを取り込む際にクローンを使用することをお勧めします。\nParquetやIcebergからDelta Lakeへのマイグレーションを実施しているが、ソーステーブルを使い続ける必要がある。\n追加、更新、削除がなされるプロダクションソーステーブルとターゲットテーブル間の取り込みのみ動機を維持する必要がある。\nレポーティング、機械学習、バッチETLにおけるソースデータのACID準拠のスナップショットを作成したいと考えている。\n注意\nソーステーブルという用語は、クローンされるテーブルとデータファイルを指し、ターゲットテーブルはオペレーションによって作成、更新されるDeltaテーブルを指します。"}
{"category":"closed_qa","context":"","instruction":"クローンの構文はどのようなものか？","response":"ParquetやIcebergに対するクローンは、Deltaテーブルのクローンに使うものと同じ基本構文を使用し、シャロークローンとディープクローンをサポートしています。\n 多くのワークロードにおいてインクリメンタルなクローンを用いることをお勧めします。ParquetやIcebergに対するクローンではSQL構文を使用します。\n注意\nParquetやIcebergに対するクローンでは、Deltaのクローンや変換とは異なる要件、保証となります。ParquetやIcebergテーブルのクローンの要件と制限をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"ParquetやIcebergテーブルのクローンの要件と制限について教えてください。","response":"ディープクローンにしてもシャロークローンにしても、クローンが実行された後にターゲットテーブルに適用された変更は、ソーステーブルに同期されません。クローンによるインクリメンタルな同期は一方向であり、ソーステーブルに対する変更は自動でターゲットのDeltaテーブルに適用されます。\n\nParquetやIcebergテーブルに対するクローンを使用する際には、以下の制限も適用されます。\n\nこのオペレーションでは、ファイルレベルの統計情報は収集しません。このため、ターゲットテーブルではDelta Lakeのデータスキッピングのメリットを享受することができません。\nクローンする前に、パーティションを持つParquetテーブルをHiveメタストアのようなカタログに登録し、ソーステーブルと同じテーブル名を使用しなくてはなりません。パーティションを持つParquetテーブルに対してパスベースのクローン構文を使用することはできません。\nパーティション進化をおこなったIcebergテーブルをクローンすることはできません。\nIcebergテーブルにはパスのみを用いてアクセスすることができます。テーブル名はサポートされていません。\nUnity Catalogではシャロークローンをサポートしていません。"}
{"category":"closed_qa","context":"","instruction":"外部ロケーションにあるParquetやIcebergファイルのディレクトリをDelta Lakeテーブルに変換できますか？","response":"はい、ストレージロケーションに対する書き込み権限を持っているのであれば、ParquetデータファイルのディレクトリをDelta Lakeテーブルに変換することができます。"}
{"category":"closed_qa","context":"","instruction":"マネージドテーブル、外部テーブルをUnity CatalogのDelta Lakeに変換できるか？","response":"Unity Catalogでは、外部テーブルで多くのフォーマットをサポートしていますが、マネージドテーブルではDelta Lakeのみをサポートしています。マネージドのParquetテーブルを直接Unity CatalogのマネージドDelta Lakeテーブルに変換するには、テーブルをUnity Catalogにアップグレードするをご覧ください。\n\n外部ParquetテーブルをUnity Catalogにアップグレードするには、外部テーブルをUnity Catalogにアップグレードするをご覧ください。\n\n外部ParquetテーブルをUnity Catalogに登録すると、それを外部Delta Lakeテーブルに変換することができます。Parquetテーブルがパーティショニングされている場合には、パーティショニング情報を指定しなくてはならないことに注意してください。"}
{"category":"closed_qa","context":"","instruction":"DatabricksにおけるACID保証とは？","response":"Databricksではデフォルトですべての読み書きにDelta Lakeを使用しており、オープンソースのDelta Lakeプロトコルによって提供されるACID保証の上に成り立っています。\n\n原子性(Atomicity)とは、すべてのトランザクションが完全に成功するか、完全に失敗するかのいずれかになることを意味します。\n一貫性(Consistency)は、同時のオペレーションにおいて、ある状態のデータがどの様に観測されるのかに関して保証します。\n分離性(Isolation)は同時のオペレーションが違いにどの様に競合するのかに関するものです。\n耐久性(Durability)はコミットされた変更が永続的であることを意味します。\n多くのデータ処理、ウェアハウジングの技術ではACIDトランサクションがあると言及されていますが、特定の保証内容はシステムによって異なり、Databricksにおけるトランザクションは皆様が使われている他のシステムとは異なる場合があります。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるトランザクションはどの様なスコープなのか？","response":"Databricksではテーブルレベルでトランザクションを管理します。トランザクションは常に一度に一つのテーブルに対して適用されます。同時実行されるトランザクションを管理するために、Databricksでは楽観的コンカレンシー制御を使用します。これは、テーブルの読み書きにおいてロックを行わず、デッドロックが発生しないことを意味します。\n\nデフォルトでは、Databricksでは読み込みにおいてはスナップショットのアイソレーション、書き込みにおいてはwrite serializableのアイソレーションを提供します。Write serializableアイソレーションは、スナップショットのアイソレーションよりも強力な保証を行いますが、書き込み時にのみこの強力なアイソレーションを適用します。\n\n複数のテーブルを参照する読み込みオペレーションは、アクセス時の現行テーブルバージョンを返却しますが、参照しているテーブルを変更する可能性がある同時実行されているトランザクションに干渉しません。\n\nDatabricksでは複数のオペレーションを一つのトランザクションにグループ化できるBEGIN/END構文は提供していません。複数のテーブルを変更するアプリケーションは、直列化された形でそれぞれのテーブルにトランザクションをコミットします。MERGE INTOを用いることで、テーブルに対するインサート、アップデート、デリートを一つの書き込みトランザクションにまとめることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksではどのように原子性を実装しているのか？","response":"トランザクションログがコミットの原子性を制御します。トランザクション中は、テーブルを構成するファイルディレクトリにデータファイルが書き込まれます。トランザクションが完了すると、トランザクション中に書き込まれたすべてのファイルへのパスを含む新規エントリーがトランザクションログにコミットされます。それぞれのコミットはテーブルバージョンを増加させ、読み込みオペレーションで新たなデータファイルが参照できる様になります。テーブルの現在の状態は、トランザクションログにおいて適正とマークされたすべてのファイルから構成されます。\n\nトランザクションログが新規バージョンを記録しない限り、データファイルは追跡されません。テーブルにデータファイルを書き込んだ後にトランザクションが失敗すると、これらのデータファイルがテーブルの状態を破損させることはありませんが、このファイルはテーブルの一部とはなりません。VACUUMオペレーションは、失敗したトランザクションによる未コミットのファイルを含む未追跡のデータファイルの全てを削除します。"}
{"category":"closed_qa","context":"","instruction":"Databricksではどのように耐久性を実装しているのか？","response":"Databricksでは、すべてのデータファイルとトランザクションログを格納するためにクラウドオブジェクトストレージを使用します。クラウドオブジェクトストレージは高い可用性と耐久性を提供します。トランザクションは完全に成功するか完全に失敗するかのいずれかであり、クラウドオブジェクトストレージに格納されるデータファイルとともにトランザクションログが存在し、Databricksのテーブルは格納されているクラウドオブジェクトストレージの耐久性保証を受け継ぎます。"}
{"category":"closed_qa","context":"","instruction":"Databricksではどのように一貫性を実装しているのか？","response":"Delta Lakeは書き込みにおけるトランザクション保証を提供するために、楽観的コンカレンシー制御を使用します。この機構のもとで、書き込みは3つのステージでオペレーションされます\n\n読み込み: どのファイルを修正(再書き込み)する必要があるのかを特定するために、利用できる最新バージョンのテーブルを(必要であれば)読み込みます。\n追加のみを行う書き込みは、書き込み前にテーブルの現在の状態を読み込みません。スキーマの検証ではトランザクションログのメタデータを活用します。\n書き込み: テーブルの定義で使用されているディレクトリにデータファイルを書き込みます。\n検証とコミット:\nスナップショットを読み込んだ以降、同時処理でコミットされた他の変更と提案された変更が競合しないかどうかをチェックします。\n競合しない場合、ステージングされたすべての変更は新規バージョンのスナップショットとしてコミットされ、書き込みのオペレーションが成功します。\n競合する場合、同時実行更新の例外で書き込みオペレーションが失敗します。失敗することによってデータの破損を防ぎます。\n楽観的コンカレンシーは、皆様のデータに対する同時実行トランザクションの多くが他のトランザクションと競合しないが、競合は起こりうるものであるという前提に立っています。Databricksにおけるアイソレーションレベルと書き込みの競合をご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksではどのように分離性を実装しているのか？","response":"Databricksでは、デフォルトですべてのテーブルへの書き込み、更新でwrite serializableアイソレーションを使用します。すべてのテーブル読み込みでは、スナップショットアイソレーションが使用されます。\n\n書き込みの高スループットを実現するために、書き込みのシリアライズ可能性と楽観的コンカレンシー制御が連携して動作します。現行の適正なテーブルの状態が常に利用でき、任意のタイミングでテーブルの書き込みをスタートすることができます。同時実行される読み込み処理は、メタストアとクラウドリソースのスループットによってのみ制限を受けます。"}
{"category":"closed_qa","context":"","instruction":"mlflow.spark.autologによるデータソース(ファイルパス、バージョン)のトラッキングが可能でしょうか？","response":"はい、MLflow.spark.autologでモデルをトラッキングする際に使うSparkデータソースの情報をMLflowで記録することができます。Delta Lakeと組み合わせることで、データのバージョンも追跡できる様になります。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでAmazon QuickSightをサポートしていますか？","response":"はい、Amazon QuickSightがDatabricksをサポートしています。Databricksレイクハウスに格納されているデータを用いて、QuickSightによる可視化が可能となりました。"}
{"category":"closed_qa","context":"","instruction":"取り込み時間クラスタリング(Ingestion Time Clustering) とは何でしょうか？","response":"取り込み時間クラスタリングは、データ取り込みを行う時間に基づいた自然なクラスタリングを可能にするDatabricksの書き込みの最適化機能です。取り込み時間クラスタリングによって、テーブルのクラスタリングは取り込み時間によってメンテナンスされ、日付や時間でフィルタリングするクエリーにおけるデータスキッピングによって、劇的なクエリーパフォーマンスの改善を実現し、クエリーに回答するために読み込む必要があるファイルの数を劇的に削減します。これを行うことで、時系列ファクトテーブルのレイアウトを最適化するためにお客様は作業することなしに、すぐに素晴らしいデータスキッピングを活用できる様になります。"}
{"category":"closed_qa","context":"","instruction":"Deltaにおけるデータクラスタリングに関連する課題は何ですか？","response":"現時点では、お客様は優れたパフォーマンスのためにデータレイアウトを最適化するために2つのパワフルなテクニックを利用することができます。パーティショニングとz-orderingです。データレイアウトに対するこれらの最適化技術によって、クエリーが読み込む必要があるデータ量を劇的に削減し、オペレーションごとにテーブルのスキャンに必要な時間を削減することができます。\n\nパーティショニングやz-orderingから得られるクエリーパフォーマンスは大きなものではありますが、いくつかのお客様においては、これらの最適化技術を実装、メンテナンスすることが困難となっていました。多くのお客様は、どのカラムを使用するのか、自分達のテーブルに対してz-orderを実行するかどうか、どの程度の頻度で実行するのか、パーティショニングが有用なのか、害を及ぼすのがいつなのかに関する質問を持っています。これらのお客さまの懸念を解決するために、我々はユーザーのアクション無しにこれらの最適化技術を提供することを目指しました。"}
{"category":"closed_qa","context":"","instruction":"取り込み時間クラスタリングされたテーブルに対する現実世界のクエリーパフォーマンスの結果を知りたいです？","response":"ベンチマークを行うために、大規模オンライン小売業者と取り組みを行い、彼らの分析データを表現するベンチマークを作成しました。このお客様のシナリオにおいては、売上が発生するたびに売上レコードが生成され、ファクトテーブルに取り込まれます。このテーブルに対するクエリーの多くは、多くの時間ベースの分析ワークロードでは共通のパターンにあるように、特定の時間ウィンドウで集計された売上レコードを返却します。このベンチマークでは、新規データ取り込みの時間、DELETEオペレーションの実行時間、さまざまなSELECTクエリーを計測し、取り込み時間クラスタリングにおけるクラスタリング保持機能を検証するために、全てを逐次実行しました。\n\nクラスタリングの保持に追加の処理があるにもかかわらず、取り込み時間クラスタリングによる性能劣化は認められませんでした。一方、DELETEとSELECTクエリーでは劇的なパフォーマンス改善が認められました。取り込み時間クラスタリングなしでは、DELETE文は意図したクラスタリングを解体することで、データスキッピングの効果を削減し、ベンチマークにおける以降のSELECT文のパフォーマンスをスローダウンさせました。取り込み時間クラスタリングが保持されることで、SELECTクエリーは平均19倍パフォーマンスが改善され、オリジナルの取り込み順で意図されたクラスタリングを保持することで、テーブルのクエリーに必要な時間を劇的に削減しました。"}
{"category":"closed_qa","context":"","instruction":"Databricksのサーバレスリアルタイム推論エンドポイントとは？","response":"サーバレスリアルタイム推論エンドポイントは、プロダクション用途を意図しており、秒間3000クエリー(QPS)までをサポートすることができます。サーバレスリアルタイム推論エンドポイントは、自動でスケールアップ・ダウンするので、エンドポイントはスコアリングのリクエストのボリュームに応じて自動で調整することを意味します。リクエストがない際にはノード数を0にまでスケールダウンすることも可能です。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおける機械学習モデルのデプロイメントのパターンを知りたいです。","response":"いくつのパターンがあります。2つの一般的なパターンを説明します。一つ目はコード・デプロイです。このパターンにおいては、モデルをトレーニングするコードは開発環境で開発でされます。同じコードがステージング、プロダクションに移行します。モデルはそれぞれの環境でトレーニングされます: 最初はモデル開発の一部として開発環境でトレーニングされ、インテグレーションテストの一部としてステージングで(限定的なデータのサブセットに対して)トレーニングされ、プロダクション環境では最終モデルを生成するために(完全なプロダクションデータに対して)トレーニングされます。二つ目はモデルのデプロイです。このパターンでは、モデルアーティファクトは開発環境のトレーニングコードによって生成されます。アーティファクトはプロダクションにデプロイされる前に、ステージング環境でテストされます。"}
{"category":"closed_qa","context":"","instruction":"MLOpsのコードデプロイというアプローチの利点は何ですか？","response":"MLOpsのコードデプロイというアプローチの利点:\n\nプロダクションデータへのアクセスが制限されている企業においては、このパターンを用いることでプロダクション環境のプロダクションデータを用いてトレーニングを行うことができます。\nトレーニングコードはプロダクションに向けてレビュー、テスト、承認されるので、自動モデル再トレーニングはより安全になります。\nサポートするコードはモデルトレーニングコードと同じパターンに従います。両方ともステージングのインテグレーションテストを通ることになります。"}
{"category":"closed_qa","context":"","instruction":"MLOpsのコードデプロイというアプローチの欠点は何ですか？","response":"MLOpsのコードデプロイというアプローチの欠点：関係者にコードを受け渡すためのデータサイエンティストの学習曲線は急峻なものになります。事前定義済みのテンプレートやワークフローが役立ちます。また、このパターンにおいては、データサイエンティストはML固有の問題を特定、修正できる知識を持っているので、プロダクション環境からトレーニング結果をレビューできる様になっている必要があります。"}
{"category":"closed_qa","context":"","instruction":"どの場合はMLOpsのモデルデプロイというアプローチを検討すべきか？","response":"以下の条件が一つ以上当てはまる場合には、モデルデプロイのアプローチを検討すべきです。\n\nモデルのトレーニングが非常に効果、あるいは再現が困難。\n単一のDatabricksワークスペースで全ての作業が行われた。\n外部のリポジトリやCI/CDプロセスを使っていない。"}
{"category":"closed_qa","context":"","instruction":"MLOpsのモデルデプロイというアプローチの利点は何ですか？","response":"MLOpsのモデルデプロイというアプローチの利点：データサイエンティストへの引き継ぎが簡単です。\nモデルトレーニングが高コストの場合、モデルトレーニングが一回のみで済みます。"}
{"category":"closed_qa","context":"","instruction":"MLOpsのモデルデプロイというアプローチ欠点は何ですか？","response":"MLOpsのモデルデプロイというアプローチ欠点：\n開発環境からプロダクションデータにアクセスできない場合(セキュリティ上の理由からあり得ることです)、このアーキテクチャは実現できません。\nこのパターンではモデルの再トレーニングの自動化はトリッキーです。開発環境で再トレーニングを自動化することはできますが、プロダクションにモデルをデプロイする責任を持つチームは、結果得られるモデルをプロダクションレディとして受け入れられないかもしれません。\n特徴量生成、推論、モニタリングで使用するパイプラインのようなコードは、別にプロダクションにデプロイする必要があります。"}
{"category":"closed_qa","context":"","instruction":"特徴量ストア(Feature store)のテーブルを使ってAutoMLの実行が可能でしょうか？","response":"はい、可能です。Databricks AutoMLでトレーニングを実行する際に、Databricks Feature Storeの特徴量テーブルと連携できるようになりました。"}
{"category":"closed_qa","context":"","instruction":"AutoMLでどのように特徴量ストア(Feature store)のテーブルを使うか？","response":"Databricks機械学習ランタイム11.3以降では、分類、回帰問題における入力データセットを拡張するために、Feature Storeの既存の特徴量テーブルを使用することができます。\n\n既存の特徴量テーブルを使うには、AutoMLのUIで特徴量テーブルを選択するか、AutoMLランの設定でfeature_store_lookupsを設定します。"}
{"category":"closed_qa","context":"","instruction":"なぜモデルを評価するのでしょうか？","response":"モデルの評価はMLライフサイクルの重要なパートです。これによって、データサイエンティストは自身のモデルのパフォーマンスの計測、解釈、説明が可能となります。モデルがなぜそのように動作しているのかに関して洞察を提供することで、モデル開発のタイムフレームを加速します。特にMLモデルの複雑性が増加するにつれて、MLモデルのパフォーマンスを迅速に観測、理解できる能力は、ML開発ジャーニーを成功させるためには重要となってきます。"}
{"category":"closed_qa","context":"","instruction":"MLflowにおけるモデル評価はどのように実施しますか？","response":"ユーザーは分類モデルと回帰モデルの両方の評価をサポートするmlflow.evaluate APIを通じてpython_function (pyfunc)モデルフレーバーのMLflowモデルのパフォーマンスを評価することができます。これは、ビルトインのタスク固有のパフォーマンスメトリクス、パフォーマンスのプロット、モデルの説明を計算してMLflowトラッキングサーバーに記録します。"}
{"category":"closed_qa","context":"","instruction":"ビルトインの評価メトリックのセットに含まれないカスタムメトリクスを用いてMLflowモデルをどうやって評価するか？","response":"カスタムメトリクスでモデルを評価するには、シンプルにカスタムメトリックの関数のリストをmlflow.evaluate APIに渡します。カスタムメトリックの関数は、以下の順序で2つの必須のパラメーター、1つのオプションのパラメーターを受け取る必要があります:\n\neval_df: predictionとtargetカラムを持つPandasあるいはSparkデータフレーム。\n\n例：モデルの出力が3つの値のベクトルである場合、データフレームeval_dfは以下の様になります。\n\nbuiltin_metrics: ビルトインメトリクスを含むディクショナリー。\n\n例：回帰モデルにおいては、builtin_metricsは以下の様になります。\n\n(オプション)artifacts_dir: カスタムメトリック関数が生成されたアーティファクトをMLflowに記録する前に一時的に保存するために使用する一時ディレクトリのパス。\n\n例：環境に応じて見え方が異なることに注意してください。"}
{"category":"closed_qa","context":"","instruction":"ビルトインのメトリクスを用いてMLflowモデルをどうやって評価するか？","response":"MLflowでは、分類、回帰モデルの両方に対して一般的に使われるパフォーマンスやモデル説明可能性に関するメトリクスのセットを同梱していまうs。これらのメトリクスによるモデルの評価はわかりやすいものです。必要なのは、テストデータとターゲットを含む評価データセットを作成し、mlflow.evaluateを呼び出すことです。\n\nモデルのタイプによって、異なるメトリクスが計算されます。ビルトインメトリクスに関する最新の情報については、mlflow.evaluateのAPIドキュメントのDefault Evaluator behavior sectionをご覧下さい。"}
{"category":"closed_qa","context":"","instruction":"プログラムからMLflowモデルの評価結果にどうやってアクセスするか？","response":"mlflow.evaluateによって返却されるEvaluationResultオブジェクトを通じてプログラムからアクセスすることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksのインフォメーションスキーマは何？","response":"INFORMATION_SCHEMAは、HIVE_METASTOREカタログ以外の全てのカタログに存在するSQL標準ベースのシステム提供のスキーマです。"}
{"category":"closed_qa","context":"","instruction":"Sparkにて引用符内に区切り文字が含まれるCSVファイルを取り込む際にどうしたら正しく読まれますか？","response":"Sparkにて引用符内に区切り文字が含まれるCSVファイルを取り込む際に `multiLine` を `true` に設定すべきです。"}
{"category":"closed_qa","context":"","instruction":"Fivetranを使ってどういうことができますか？","response":"Fivetranはレイクハウスへの自動データ取り込みことに活用できます。SalesforceやMySQLからのデータ取り込みにFivetranを使い、高信頼、メンテナンス可能、テスト可能なデータパイプラインを構築するための宣言型ETLフレームワークであるDelta Live Tables(DLT)を用いてデータを変換します。\nFivetranのマネージドコネクターによって、ユーザーは200以上のソースからDatabricksレイクハウスプラットフォームへのデータ取り込みを完全に自動化できます。\n\nコネクターを設定、テストするためのユーザーフレンドリーなUI。\nスキーマドリフト対応を含む自動スキーマ管理。\nAPI障害、レートリミットなどへの対応。\n完全、インクリメンタルなロード。"}
{"category":"closed_qa","context":"","instruction":"既存のDelta Lakeテーブルのコピーをどのように作成するか？","response":"cloneコマンドを用いてDatabricksにある既存のDelta Lakeテーブルのコピーを作成することができます。ディープあるいはシャロー(浅い)クローンを作成することができます。"}
{"category":"closed_qa","context":"","instruction":"クローンはDatabricksランタイムでデフォルトで有効化されているか？","response":"クローンはDatabricksランタイムでデフォルトで有効化されている機能です。"}
{"category":"closed_qa","context":"","instruction":"テーブルのクローンにいくつ種類があるか？","response":"テーブルのクローンにディープクローンとシャロークローンの二種類があります。\nディープクローンは、既存テーブルのメタデータに加えて、ソーステーブルのデータをクローンのターゲットにコピーします。さらに、ソーステーブルに対してDeltaテーブルに書き込みを行うストリームを停止し、クローンのターゲットでコピーした地点から再開できる様に、ストリームのメタデータもクローンされます。\nシャロークローンは、クローンターゲットにデータファイルをコピーしないクローンです。テーブルメタデータはソースと同じものになります。これらのクローンは低コストで作成できます。\nディープクローン、シャロークローンに対するすべての変更はクローン自身に反映され、ソーステーブルには影響を与えません。"}
{"category":"closed_qa","context":"","instruction":"ディープクローンでは何のコピーを作成しますか？","response":"ディープクローンは、既存テーブルのメタデータに加えて、ソーステーブルのデータをクローンのターゲットにコピーします。さらに、ソーステーブルに対してDeltaテーブルに書き込みを行うストリームを停止し、クローンのターゲットでコピーした地点から再開できる様に、ストリームのメタデータもクローンされます。"}
{"category":"closed_qa","context":"","instruction":"シャロークローンでは何のコピーを作成しますか？","response":"シャロークローンは、テーブルメタデータをクローンします。クローンされるメタデータには、スキーマ、パーティション情報、不変性、NULL可能性が含まれます。クローンターゲットにデータファイルをコピーしないので、シャロークローンはソースディレクトリのデータファイルを参照します。テーブルメタデータはソースと同じものになります。これらのクローンは低コストで作成できます。"}
{"category":"closed_qa","context":"","instruction":"ディープクローンにおいてクローンされるメタデータには何が含まれますか？","response":"クローンされるメタデータには、スキーマ、パーティション情報、不変性、NULL可能性が含まれます。ディープクローンにおいてのみ、ストリームとCOPY INTOのメタデータもクローンされます。クローンされないメタデータは、テーブルの説明文とユーザー定義のコミットメタデータです。"}
{"category":"closed_qa","context":"","instruction":"クローンされたテーブルのタイムトラベルはどうなるか？","response":"クローンされたテーブルは、ソーステーブルとは独立した履歴を持ちます。クローンされたテーブルに対するタイムトラベルは、ソーステーブルに対して同じ入力をしたとしても別の挙動をします。"}
{"category":"closed_qa","context":"","instruction":"グラフ分析を行うためにどの様にGraphFramesを使うのか","response":"GraphFramesの最適化されたインストレーションが含まれているので、Databricks機械学習ランタイムを使うことをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"構造化ストリーミングクエリーの障害が発生したらどうしたらいいですか？","response":"構造化ストリーミングは、ストリーミングクエリーにおける耐障害性とデータの一貫性を提供します。Databricksワークフローを用いることで、構造化ストリーミングの障害時に再起動する様に容易に設定することができます。ストリーミングクエリーのチェックポイント作成を有効化することで、障害の後にクエリーを再起動することができます。再起動されたクエリーは障害時点から処理を継続します。"}
{"category":"closed_qa","context":"","instruction":"構造化ストリーミングクエリーのチェックポイント作成をどうのように有効化するか？","response":"クエリーをスタートする前に、checkpointLocationオプションを指定すると有効になります。"}
{"category":"closed_qa","context":"","instruction":"複数のクエリーで同じロケーションを使えますか？","response":"いいえ、複数のクエリーで同じロケーションを使ってはいけません。チェックポイントのロケーションは、クエリーを特定する重要な情報のすべてを保持します。それぞれのクエリーごとのチェックポイントロケーションを設けるべきです。"}
{"category":"closed_qa","context":"","instruction":"コントロールプレーンとは何ですか？","response":"コントロールプレーンはDatabricksによって完全に管理され、Web UI、ノートブック、ジョブ、クエリー、クラスターマネージャから構成されています。"}
{"category":"closed_qa","context":"","instruction":"データプレーンとは何？","response":"データプレーンはお使いのAWSアカウントに存在しており、データを処理するためにDatabricksクラスターが動作しています。"}
{"category":"closed_qa","context":"","instruction":"レイクハウスアーキテクチャにおけるキーコンポーネントは何？","response":"クラウドオブジェクトストアは、他のクラウドデータベースやオンプレミスの代替案よりも、通常は安価に様々な種類のデータを格納できるので、レイクハウスアーキテクチャにおけるキーコンポーネントとなっています。AWS DatabricksのAmazon Simple Storage (S3)、Azure DatabricksのAzure Data Lake Storage、Databricks on Google CloudのGoogle Cloud Storage (GCS)はクラウドオブジェクトストアです。"}
{"category":"closed_qa","context":"","instruction":"Databricks Feature Store(特徴量ストア)とは？","response":"特徴量ストアは、データサイエンティストが特徴量を発見、共有できる集中管理されたリポジトリであり、モデルトレーニングと推論に使われる特徴量の値を計算するために同じコードが使われることを確実にします。"}
{"category":"closed_qa","context":"","instruction":"Feature store (特徴量ストア)で解決する課題は何でしょうか？","response":"特徴量ストアが解決する課題は、複数人のデータサイエンティストが同じデータを基点として機械学習を行う際に、それぞれが別個に特徴量エンジニアリングを行なって、各自で特徴量のデータ(テーブルやデータフレーム)を作ってしまい、特徴量の用途は同じながらも微妙にロジックや結果が異なり、しかも、作りっぱなしで再利用されないということです。\n\nさらに、モデルをトレーニングする際にモデルに入力する特徴量と、推論の際にモデルに入力する特徴量は同じロジックで計算されるべきですが、各自でロジックを作っていると、トレーニングで用いる特徴量と、推論で用いられる特徴量が異なるロジックで計算されてしまうということが起こってしまいます。これによって、期待した精度が出ないということも起こり得ます。これを「オフライン(トレーニング)とオンライン(推論)の偏り」と呼びます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Feature Storeはどのような機能を提供するか？","response":"Databricksが提供する特徴量ストアであるDatabricks Feature Storeは、他のDatabricksのコンポーネントと完全にインテグレーションされています。\n\n発見可能性。DatabricksワークスペースからアクセスできるFeature Store UIを用いることで、既存の特徴量をブラウズ、検索することができます。\nリネージュ。Feature Storeを用いて特徴量テーブルを作成すると、特徴量テーブルを作成するために使用されたデータソースが保存され、アクセス可能になります。特徴量テーブルのそれぞれの特徴量に対して、特徴量を使用しているモデル、ノートブック、ジョブ、エンドポイントにもアクセスすることができます。\nモデルスコアリング、モデルサービングとのインテグレーション。モデルをトレーニングするためにFeature Storeからの特徴量を使う際、モデルは特徴量メタデータと一緒にモデルがパッケージングされます。バッチスコアリングやオンライン推論にモデルを使う際、Feature Storeから自動で特徴量を取得します。呼び出し元は、これらに関して知る必要や、新規データをスコアリングするために特徴量を検索、joinするロジックを含める必要はありません。これによって、モデルのデプロイメントと更新をより簡単なものにします。\nポイントインタイムの検索。Feature Storeでは、ポイントインタイムの正確性を必要とする時系列、イベントベースのユースケースをサポートしています。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeを利用してどうやって特徴量の再利用を実現するか？","response":"Databricks Feature Storeを用いることで、特徴量エンジニアリングのロジックと特徴量データを、トレーニングや推論のノートブックから分離することができます。これによって、特徴量の再利用を促進し、オフライン・オンラインの偏りを回避することができます。"}
{"category":"closed_qa","context":"","instruction":"特徴量テーブルの値を更新するにはどの関数を使うか？","response":"特徴量テーブルの値を更新するには、write_tableを使います。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeはストリーミングデータをサポートするか？","response":"はい、サポートしています。df.isStreamingがtrueに設定されたデータフレームを渡すことでFeature Storeにストリーミングすることができます:"}
{"category":"closed_qa","context":"","instruction":"どのようにFeature Storeからトレーニングデータセットが作成されるのか？","response":"トレーニングデータセットは以下から構成されます:\n\n生の入力データ\n特徴量ストアの特徴量\n生の入力データは以下を含んでいるので必要になります:\n\n特徴量とjoinするために使用される主キー。\n特徴量ストアにないtrip_distanceのような生の特徴量。\nモデルトレーニングに必要なfareのような予測ターゲット。\n必要な特徴量それぞれにFeatureLookupを作成することで、モデルをトレーニングするための特徴量をFeature Storeからロードします。"}
{"category":"closed_qa","context":"","instruction":"Feature StoreのTrainingSetは何ですか？","response":"TrainingSetはfs.create_training_set(..)が呼び出されると作成されるモデルのトレーニングで使用するFeature Storeからの特定の特徴量を選択するオブジェクトです。"}
{"category":"closed_qa","context":"","instruction":"Feature StoreのTrainingSetはデータフレームですか？","response":"いいえ、Feature StoreのTrainingSetは特定の特徴量を選択するオブジェクトです。モデルのトレーニングのためデータフレームに変換する必要があります。"}
{"category":"closed_qa","context":"","instruction":"バッチデータに対してモデルを評価するためにFeature StoreのどのAPIを使いますか？","response":"バッチデータに対してモデルを評価するためにscore_batch APIを使い、Feature Storeから必要な特徴量を取得します。"}
{"category":"closed_qa","context":"","instruction":"FeatureStoreClient.log_modelを用いて記録したモデルを使ってバッチ推論にどのように実行しますか？","response":"バッチデータに対してモデルを適用するためにscore_batch APIを使い、Feature Storeから必要な特徴量を取得します。"}
{"category":"closed_qa","context":"","instruction":"Feature storeの特徴量テーブルの実データはどこに格納されていますか？","response":"特徴量テーブルはDeltaテーブルと他のメタストアに格納されます。"}
{"category":"closed_qa","context":"","instruction":"Feature storeの特徴量テーブルのメタデータは何？","response":"特徴量テーブルのメタデータは、テーブルの生成元となるデータソース、テーブルを作成、更新したノートブック、ジョブを追跡します。"}
{"category":"closed_qa","context":"","instruction":"特徴量テーブルを使ってリアルタイム推論ができるか？","response":"リアルタイム推論のために、特徴量テーブルをオンラインストアに公開する必要です。"}
{"category":"closed_qa","context":"","instruction":"特徴量テーブルのグルーピングと検索を容易にするのはどうしたらいいですか？","response":"グルーピングと検索を容易にするために、タグを作成し、特徴量テーブルに関連づけることができます。\n\n"}
{"category":"closed_qa","context":"","instruction":"Feature storeのオフラインストアとオンラインストアの違いを教えてください。","response":"オフラインの特徴量ストアは特徴量の検索、モデルのトレーニング、バッチ推論に使用されます。これにはDeltaテーブルの特徴量テーブルが含まれます。\nオンライン特徴量ストアは、リアルタイム推論に用いられる低レーテンシーのデータベースです。Databricks Feature Storeがサポートしているオンラインストアについては、Work with online storesをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricks Feature Storeはストリーミングをサポートしますか？","response":"はい、バッチ書き込みに加えて、Databricks Feature Storeはストリーミングをサポートします。ストリーミングのソースから特徴量を特徴量テーブルに書き込むことができ、特徴量の計算には生データのストリーミングを特徴量に変換するために構造化ストリーミングを活用できます。\n\nオフラインストアの特徴量テーブルからオンラインストアにストリーミングすることも可能です。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeから作成したトレーニングセットは何？","response":"トレーニングセットは特徴量のリストと生のトレーニングデータ、ラベル、特徴量を検索するための主キーから構成されます。特徴量ストアから特徴量を抽出してトレーニングセットを作成し、モデルトレーニングの際の入力データにすることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Feature Storeの特徴量を用いてトレーニングした機械学習モデルのパッケージングは何かできるか？","response":"Databricks Feature Storeの特徴量を用いてトレーニングした機械学習モデルは、特徴量との紐付けを維持します。推論を行う際にモデルが特徴量ストアから特徴量を取得することも可能です。モデルの呼び出し下は、モデルに使われた特徴量の主キー(例えば、user_id)を指定するだけで、モデルが特徴量ストアから必要な特徴量を取得します。\n\nバッチ推論においては、オフラインストアから特徴量が取得され、スコアリングの前にデータと結合されます。リアルタイム推論においては、特徴量はオンラインストアから取得されます。"}
{"category":"closed_qa","context":"","instruction":"モデルを特徴量のメタデータとパッケージングする場合にどの関数を使いますか？","response":"モデルを特徴量のメタデータとパッケージングするには、FeatureStoreClient.log_model()を使用します。"}
{"category":"closed_qa","context":"","instruction":"検索キーが主キーにマッチしない際にトレーニングデータセットがどのように作成されるか？","response":"トレーニングセットのカラム名のFeatureLookupで引数lookup_keyを使います。FeatureStoreClient.create_training_setは、特徴量テーブルが作成された際に指定された主キーの順序を用いて、lookup_key引数で指定されたトレーニングセットのカラム間の順序ありjoinを行います。"}
{"category":"closed_qa","context":"","instruction":"異なる特徴量テーブルから同じ名前を持つ二つの特徴量を含む場合、トレーニングデータセットをどのように作成するか？","response":"FeatureLookupでオプションの引数output_nameを使います。指定される名前がTrainingSet.load_dfで返却されるデータフレームの特徴量名として使用されます。"}
{"category":"closed_qa","context":"","instruction":"教師なし機械学習モデルのトレーニングデータセットを作成できるか？","response":"はい、できます。教師なし機械学習モデルのトレーニングセットを作成する際には、label=Noneを設定します。"}
{"category":"closed_qa","context":"","instruction":"特徴量メタデータがパッケージングされたモデルのスコアリングの際にカスタム特徴量をどのように使用するか？","response":"デフォルトでは、特徴量メタデータがパッケージングされたモデルは、推論時にFeature Storeから特徴量を取得します。スコアリングでカスタムの特徴量を使う際には、FeatureStoreClient.score_batch()に渡すデータフレームにそれらを含めます。\n"}
{"category":"closed_qa","context":"","instruction":"Feature Storeの特徴量とFeature Store外のデータを組み合わせてモデルをトレーニングすることができるか？","response":"はい、Feature Storeの特徴量とFeature Store外のデータを組み合わせてモデルをトレーニングすることができます。特徴量のメタデータをモデルにパッケージングすると、モデルは推論時にFeature Storeから特徴量を取得します。"}
{"category":"closed_qa","context":"","instruction":"MLOpsとは？","response":"MLOpsとは、コード、データ、モデルを管理するための一連のプロセスと自動化されたステップです。これは、DevOps、DataOps、ModelOpsを組み合わせたものです。"}
{"category":"closed_qa","context":"","instruction":"MLOpsを実現するため、推薦の環境設定はありますか？","response":"実行環境は、モデルやデータが作成され、コードによって活用される場所となります。それぞれの実行環境は、計算インスタンス、ランタイム、ライブラリ、自動化されたジョブから構成されます。\n\nMLコードとモデル開発に対して、ステージ間の移行が明確に定義された別々のステージを設けることをお勧めします。例えば、ステージに対する一般的な名称を用いてこのプロセスに従います。\n\n開発\nステージング\nプロダクション\n"}
{"category":"closed_qa","context":"","instruction":"MLOpsを実現するため、推薦のパイプラインやコードバージョン管理は何か？","response":"Gitバージョンコントロールを使いましょう。 パイプラインやコードバージョン管理のためにGitに格納されるべきです。MLロジックのステージ間の移行を、コードの開発ブランチから、ステージングブランチ、リリースブランチへの移動と解釈することができます。お使いのGitプロバイダーをインテグレーションし、Databricksワークスペースでノートブックやソースコードを同期するために、Databricks Reposをお使いください。また、DatabricksではGit連携、バージョン管理のためにその他のツールも提供しています。Developer tools and guidanceをご覧下さい。"}
{"category":"closed_qa","context":"","instruction":"MLOpsのため、推薦のデータ格納場所はどこですか？","response":"Deltaテーブルを用いたレイクハウスアーキテクチャにデータを格納しましょう。 お使いのクラウドアカウント内のレイクハウスアーキテクチャにデータを格納すべきです。生のデータと特徴量テーブルの両方が、誰が読み込みでき、誰が編集できるのかを決定するためのアクセスコントロールがなされたDeltaテーブルに格納されるべきです。"}
{"category":"closed_qa","context":"","instruction":"MLopsの継続的インテグレーション(CI)プロセスでは何を行うか？","response":"CIプロセスは主にユニットテストとインテグレーションテスト(CI)を実行します。\nユニットテスト\nCIプロセスは自動でソースコードをビルドし、ユニットテストを起動します。テストが失敗するとマージリクエストは却下されます。ユニットテストはデータや他のサービスとはやり取りしません。\n\n3. インテグレーションテスト(CI)\nそして、CIプロセスはインテグレーションテストを実行します。インテグレーションテストは全体的に機能が適切に動作することを確実にするために、(特徴量生成、モデルトレーニング、推論、モニタリングを含む)すべてのパイプラインを実行します。ステージング環境は、合理性があるといえるくらいプロダクション環境と類似したものである必要があります。"}
{"category":"closed_qa","context":"","instruction":"MLopsの継続的デプロイメント(CD)プロセスでは何を行うか？","response":"CDプロセスが、新たなモデル(モデルレジストリ上ではstage=None)を取り込んでテストし、テストに成功したらデプロイ(stage=Productionにプロモート)します。CDはモデルレジストリのwebhookあるいはお使いのCDシステムで実装することができます。\n\nこのパイプラインには3つのタスクがあります:\n\nコンプライアンスチェック。 これらのテストは、モデルレジストリからモデルをロードし、あなたの組織が必要とするコンプライアンスチェック(タグやドキュメントなど)を行い、テスト結果に基づいてリクエストを承認あるいは却下します。コンプライアンスに人間の専門性が必要である場合、この自動化ステップは手動によるレビューのために統計情報やビジュアライゼーションを生成します。結果の如何を問わず、タグのメタデータや説明文のコメントを用いて、モデルバージョンの結果をモデルレジストリに記録します。\n\n手動でステージ移行や移行リクエストを管理するためにMLflowのUIを使うことができます。あるいは、自動化するためにMLflow APIやwebhookを使うことができます。モデルがコンプライアンスチェックを通過したら、移行リクエストは承認され、モデルはstage=Stagingにプロモートされます。モデルが失敗すると、移行リクエストは却下されモデルはモデルレジストリでstage=Archivedに移行されます。\n\nステージングとプロダクションの比較。 パフォーマンスの劣化を防ぐために、現行のプロダクションバージョンとステージングにプロモートされたモデルのパフォーマンスを比較すべきです。比較のメトリクスや方法はユースケースに依存し、カナリアデプロイメントやA/Bテストや他の手法を含めることもできます。テストの比較結果はレイクハウスのメトリクステーブルに保存されるべきです。\n\nこれが最初のデプロイメントであり、まだプロダクションバージョンがない場合には、ステージングバージョンをベースラインとしてのビジネス経験則や他の閾値と比較することができます。\n\nモデルのプロダクションへの移行のリクエスト。 候補モデルが比較テストを通過したら、モデルレジストリでstage=Productionへの移行をリクエストすることができます。MLflowを用いて手動、あるいはMLflow APIやwebhookを用いて自動でこれを行うことができます。また、この時点で人間による承認を行うことは良いアイデアです。これが、モデルがプロダクションにリリースされ、既存のビジネスプロセスに組み込まれる前の最後のステップです。コンプライアンスチェック、パフォーマンスの比較、自動化が難しいその他のチェックを検証するための人間によるレビューを含めることもできます。"}
{"category":"closed_qa","context":"","instruction":"モデルのモニタリングは何を監視するか？","response":"入力データとモデルの予測結果の統計情報(データドリフトやモデルのパフォーマンスなど)、計算処理の性能(エラーやスループット)をモニタリングすべきです。"}
{"category":"closed_qa","context":"","instruction":"モデルの再トレーニングはどの場合実施するか？","response":"最新データを用いてモデルの再トレーニングを行うスケジュールジョブを作成するか、データやモデルにドリフトを検知した際に再トレーニングを起動するためのモニターをセットアップすることができます。モデルモニタリングメトリクスがパフォーマンスの問題を示した際には、データサイエンティストは開発環境に戻って新たなモデルバージョンを開発する必要があるかもしれません。"}
{"category":"closed_qa","context":"","instruction":"データ利用者はどのようにDatabricksマーケットプレイスのデータにアクセスできるのか？","response":"Databricksマーケットプレイスで欲しいデータ製品を探すには、シンプルに提供者のリスティングをブラウズ、検索し、欲しいデータ製品へのアクセスをリクエストします。いくつかのデータ製品はリクエストを行い、利用条件に同意すればすぐに利用できます。他のものにおいては、提供者の承認と提供者のインタフェースを用いたトランザクションの完了が必要となることがあります。いずれの場合でも、マーケットプレイスを支援するDelta Sharingプロトコルはセキュアに共有データにアクセスできることを保証します。"}
{"category":"closed_qa","context":"","instruction":"データ提供者はDatabricksマーケットプレイスにどのようにデータ製品を提供できるのか？","response":"Databricksマーケットプレイスは、データ提供者に対してデータサイエンティストやアナリストが自分達の組織の成功のために活用できるデータ製品を共有するためのセキュアなプラットフォームを提供します。Databricksマーケットプレイスでは、共有データに対するセキュリティとコントロールを提供するためにDelta Sharingを活用しています。公開データ、無料のサンプルデータ、有償のデータオファリングを共有することができます。データセットに加えて、ユースケースやお客様がデータ製品の利点を完全に活用できるようにする方法を示すために、Databricksノートブックやその他のコンテンツを共有することもできます。"}
{"category":"closed_qa","context":"","instruction":"Databricksマーケットプレイスでデータ製品を提供するには、何か必要ですか？","response":"Databricksマーケットプレイスでデータ製品を提供するには、以下が必要です:\n\nDatabricksアカウントとプレミアムワークスペース\nDatabricks Data Partner Programを通じた提供者の申込み"}
{"category":"closed_qa","context":"","instruction":"既存のテーブルやビューの定義やメタデータを用いてテーブルを作成できるか？","response":"はい、CREATE TABLE LIKEを使って既存のテーブルやビューの定義やメタデータを用いてテーブルを作成することができます。"}
{"category":"closed_qa","context":"","instruction":"CREATE TABLE LIKEを使ってコピーされるものは何ですか？","response":"CREATE TABLE LIKEを使ってDeltaテーブルからDeltaテーブルを作成する際には、以下がコピーされます。\n\nコメント\nカラム\nパーティションカラム\n設定\nテーブル制約\nDeltaプロトコル\n。テーブル定義やメタデータのみがコピーされるので、テーブル自体は空です。"}
{"category":"closed_qa","context":"","instruction":"PII保護の必要性は何？","response":"近年のいかなる量の顧客データの管理においては間違いなくPIIの保護が求められます。これは、シンプルな設定ミスによって数百万の機微な顧客レコードが盗まれることになったCapital Oneのデータ漏洩のようにいかなる規模の企業にとって大きなリスクです。ストレージデバイスの暗号化やテーブルレベルのカラムマスキングは効果的なセキュリティ対策ですが、この機微なデータに対する内部の許可されないアクセスは、依然として大きな脅威となります。このため、Databricksにおいて通常のユーザーがテーブルやファイルから機微情報を取得することを制限するソリューションを必要としました。\n\nしかし、機微情報を読み込むビジネスニーズを持つ人たちが読み込めるようにする必要もありました。我々は、それぞれのユーザーのテーブルの読み込み方法に違いを生じさせたくはありませんでした。データ分析やレポート作成におけるクエリーの作成をシンプルにするために、通常の読み込みと復号化された読み込みの両方は、同じDelta Lakeオブジェクトに対して行われる必要がありました。\n\n"}
{"category":"closed_qa","context":"","instruction":"データを保護するための暗号化に使うツールは何？","response":"暗号化によってでデータを保護するためのソリューションの一つがFernet Pythonライブラリです。"}
{"category":"closed_qa","context":"","instruction":"Fernetのデータ保護の仕組みを教えてください。","response":"Fernetは、いくつかの標準的な暗号化プリミティブを用いて構築されている対称性の暗号化を使用します。このライブラリは、データフレームの任意のカラムを暗号化できるようにする暗号化UDFで使用されます。暗号化キーを格納するために、我々のデータ取り込みプロセスのみがアクセスできるようにする設定がなされたDatabricksシークレットを使用します。データがDelta Lakeテーブルに格納されると、社会セキュリティ番号、電話番号、クレジットカード番号、その他のIDのような値を保持するPIIカラムは許可されないユーザーによる読み込みができなくなります。\n\n"}
{"category":"closed_qa","context":"","instruction":"UDFを永続化するには何か必要ですか？","response":"UDFはScalaで記述される必要があります。"}
{"category":"closed_qa","context":"","instruction":"センシティブなデータが書き込まれ、保護されると、権限のあるユーザーがどのようにセンシティブなデータを読み込むか？","response":"最初に行う必要があるのは、Databricksで稼働しているHiveインスタンスに追加するUDFを作成することです。このUDFは復号化を実行するために、暗号化書き込みに用いられたのと同じシークレットにアクセスし、このケースでは、クラスターのSpark設定に追加されます。これによって、権限のあるユーザー、権限のないユーザーにこのキーに対するアクセスをコントロールための、クラスターのアクセスコントロールが必要となります。UDFが作成されると、権限のあるユーザーが復号化されたデータを参照するために、ビュー定義でUDFを使えるようになります。"}
{"category":"closed_qa","context":"","instruction":"カスタムUDFを持つビューから保護データの読み込みの仕組みを教えてください。","response":"単一のデータセットに対して2つのビューオブジェクトを作成しており、それぞれが権限のあるユーザー、権限のないユーザー向けのものとなっています。権限のないユーザー向けのビューにはUDFがないので、暗号化された値としてPIIの値を参照します。権限のあるユーザー向けのもう一方のビューにはUDFがあるので、彼らは自身のビジネスニーズのために復号化された値を参照することができます。"}
{"category":"closed_qa","context":"","instruction":"カラムレベル暗号化手法は何？","response":"Fernet暗号化ライブラリ、ユーザー定義関数(UDF)、Databricksシークレットを組み合わせることでPIIなど情報の暗号化が可能です。"}
{"category":"closed_qa","context":"","instruction":"クラスターのCPUやメモリーの使用量を確認する機能はどこにありますか？","response":"クラスターのメトリクスタブからアクセスできます。左上のドロップダウンでハードウェアが選択されている際には、CPUやメモリーの使用量を確認することができます。"}
{"category":"closed_qa","context":"","instruction":"別のDatabricksワークスペースにどのように接続するか？","response":"Databricks Sparkコネクターを用いることで、別のDatabricksワークうsペースに設定されている計算リソースに接続し、現在のDatabricksワークスペースに結果を返却することができます。クエリーが成功するには、両方のワークスペースでアクティブな計算資源にアクセスできる必要があります。"}
{"category":"closed_qa","context":"","instruction":"別のDatabricksワークスペースに外部テーブルを作成することができますか？","response":"Databricksワークスペースに、別のDatabricksワークスペースにリンクされる外部テーブルを登録することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksレイクハウスプラットフォームは何？","response":"Databricksレイクハウスプラットフォームは、エンタープライズレベルの大規模データソリューションの構築、デプロイ、共有、維持のための統合されたツールセットを提供します。Databricksは、お使いのクラウドアカウントのクラウドストレージとセキュリティと連携し、あなたに代わってクラウドインフラストラクチャの管理、デプロイを行います。"}
{"category":"closed_qa","context":"","instruction":"Databricksの従業員によって立ち上げられたオープンソースプロジェクトの一覧をみたいです。","response":"以下のテクノロジーは、Databricksの従業員によって立ち上げられたオープンソースプロジェクトです。\n\nDelta Lake\nDelta Sharing\nMLflow\nApache Sparkと構造化ストリーミング\nRedash"}
{"category":"closed_qa","context":"","instruction":"DatabricksとAWSはどの様に動作するのか？","response":"Databricksプラットフォームのアーキテクチャは2つの主要なパーツから構成されています: Databricksがプラットフォームとサービスをデプロイ、設定、管理するために使うインフラストラクチャと、Databricksとお客さまとで共同管理するお客さま所有のインフラストラクチャです。\n\n数多くのエンタープライズデータベース企業と異なり、Databricksではプラットフォームを使うためにデータをプロプライエタリなストレージシステム移行することを強制しません。そうではなく、Databricksプラットフォームとお使いのクラウドアカウントの間でセキュアなインテグレーションを設定することでDatabricksワークスペースを構築すると、Databricksはオブジェクトストレージやあなたがコントロールしている連携済みサービスのデータを処理、格納するために、あなたのアカウントのクラウドリソースを用いて、一時的な計算クラスターをデプロイします。\nUnity Catalogはこのリレーションシップをさらに拡張し、Databricks内から馴染み深いSQL構文を用いて、データアクセス権限を管理できるようになります。\n\nDatabricksは、世界最大かつ最もセキュリティ意識の高いいくつかの企業のセキュリティとネットワーク要件に応えるワークスペースをデプロイしてきています。Databricksは新規ユーザーが使い始めやすいものとなっており、エンドユーザーがクラウドインフラストラクチャを操作する際の負荷や懸念点を排除しますが、経験豊富なデータ、オペレーション、セキュリティチームが必要とするカスタマイズやコントロールを制限しません。"}
{"category":"closed_qa","context":"","instruction":"Databricksは何に使うのか？","response":"BIから機械学習に至るソリューションを用いて、自分のデータセットを処理、格納、クレンジング、共有、分析、モデリング、マネタイズするためにDatabricksを活用しています。様々なデータペルソナによる数多くのアプリケーションを構築するためにDatabricksプラットフォームを活用することができます。レイクハウスを完全に導入しているお客様は、企業におけるイノベーションと洞察を強化するデータエンジアリングのワークフロー、機械学習モデル、分析ダッシュボードの開発、デプロイに我々の統合プラットフォームを活用しています。"}
{"category":"closed_qa","context":"","instruction":"Databricksワークスペースで提供しているユーザーインタフェースのリストをください。","response":"Databricksワークスペースは、以下の様なツールを含む、コアとなる多数のデータタスクのためのユーザーインタフェースを提供しています。\n\nインタラクティブなノートブック\nワークフロースケジューラーとマネージャ\nSQLエディタとダッシュボード\nデータ取り込みとガバナンス\nデータ検索、注釈、探索\n計算資源管理\n機械学習(ML)エクスペリメントのトラッキング\nMLモデルのサービング\n特徴量ストア\nGitによるソースコントロール"}
{"category":"closed_qa","context":"","instruction":"ワークスペースのUI以外にDatabricksにアクセスできるツールは何か？","response":"ワークスペースのUIに加えて、以下のツールを用いることでプログラムからDatabricksとやり取りすることができます。\n\nREST API\nCLI\nTerraform"}
{"category":"closed_qa","context":"","instruction":"Databricksの一般的なユースケースは何か？","response":"Databricksにおけるユースケースは、プラットフォームで処理されるデータや、自分の業務のコアにおいてデータを操作する従業員の数多くのペルソナと同様に多岐にわたります。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのETLとデータエンジニアリングツールを教えてください。","response":"Databricksは、比類なきETL(抽出、変換、ロード)体験を提供するために、Apache SparkとDelta Lakeとカスタムツールのパワーを組み合わせます。ETLロジックを組み立てるためにSQL、Python、Scalaを活用することができ、数クリックでスケジューリングされたジョブのデプロイメントをオーケストレーションすることができます。\nDelta Live Tablesは、インテリジェントにデータセット間の依存関係を管理し、あなたの仕様に応じてデータをタイムリーかつ正確にデリバリーできる様に、プロダクションのインフラストラクチャをデプロイ、スケールさせることでさらにETLをシンプルなものにします。\n\nDatabricksでは、クラウドオブジェクトストレージやデータレイクからデータレイクハウスにインクリメンタルかつ冪等性をもってデータをロードするための効率的かつスケーラブルなツールであるオートローダーを含む数多くのカスタムツールとのデータ取り込み手段を提供しています。"}
{"category":"closed_qa","context":"","instruction":"ディメンションモデリングは何ですか？","response":"ディメンションモデリングは、モダンデータウェアハウスを構築する際に最も人気のあるデータモデリング手法です。"}
{"category":"closed_qa","context":"","instruction":"ディメンションモデリングのベストプラクティスと推奨事項を知りたいです。","response":"ディメンションモデルを実装する際に推奨されるベストプラクティスをいくつかを示します。\n\nディメンションテーブルを非正規化すべきです。モデルの第三正規形あるいはスノーフレーク型ではなく、通常ディメンションテーブルは単一のディメンションテーブルの中で多対1リレーションシップによってflattenすることで、高度に非正規化されます。\n\n異なるディメンションテーブルの属性が同じカラム名やドメインコンテンツを持つ際には、準拠したディメンションを使用してください。この利点は、異なるファクトテーブルからのデータを、それぞれのファクトテーブルと関連づけられた準拠ディメンション属性を用いて、単一のレポートで結合することができるということです。\n\nディメンションテーブルにおける通常のトレンドは、as-isあるいはas-wasレポートをサポートするために、ディメンションの経年変化を追跡します。異なる要件に基づいてディメンションをハンドリングするために以下の基本的なテクニックを容易に適用することができます。\n\nタイプ1のテクニックでは、ディメンション属性の初期値を上書きします。\n最も人気のあるSCDテクニックであるタイプ2のテクニックでは、時間経過と共に追跡される正確な変更点を使用します。\nこれらは、Delta Live Tables(DLT)実装を用いることで容易に達成することができます。\n\nAPPLY CHANGES INTOを用いたDelta Live Tablesによって、容易にSCDタイプ1やSCDタイプ2を実行することができます。\n主キーと外部キー制約によって、ユーザーはテーブル間のリレーションシップを理解することができます。\n\nアイデンティティカラムを用いることで、新規行が追加された際にユニークな整数値が自動で生成されます。アイデンティティカラムはサロゲートキーの一形態です。詳細はこちらのブログ記事をご覧ください。\n\nCHECK制約の強制によって、あなたに這い寄るデータ品質や正確性の問題に心配する必要が無くなります。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeを用いた典型的な機械学習ワークフローを知りたいです。","response":"Feature Storeを用いた典型的な機械学習ワークフローは以下のパスを踏みます。\n\n生のデータを特徴量に変換するコードを記述し、必要な特徴量を格納するSparkデータフレームを作成します。\nFeature Storeで特徴量テーブルとしてデータフレームを書き込みます。\n特徴量ストアからの特徴量を用いてモデルをトレーニングします。これを行う際、モデルはトレーニングで使われた特徴量の仕様を格納します。モデルを推論に使う際、自動で適切な特徴量テーブルから特徴量を結合します。\nモデルをモデルレジストリに登録します。\nこれで、新規データに対する予測を行う際にモデルを使うことができる様になります。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeを用いてリアルタイムサービングと推論はどのように行いますか？","response":"リアルタイムサービングのユースケースでは、特徴量をオンラインストアに公開します。\n\n推論時には、モデルはオンラインの特徴量ストアから事前計算済みの特徴量を読み込み、モデルサービングエンドポイントにクライアントからリクエストされたデータと結合します。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeのサンプルノートブックはありますか？","response":"はい、こちらFeature Storeタクシーサンプルノートブックでは、特徴量の作成、更新、モデルトレーニングとバッチ推論での活用のプロセスを説明します。https://docs.databricks.com/_extras/notebooks/source/machine-learning/feature-store-taxi-example.html"}
{"category":"closed_qa","context":"","instruction":"Databricksジョブは何？","response":"バンドルしてサブミットできるコードの単一のユニットです。Databricksジョブは単一のSparkContextのSparkアプリケーションと同じものとなります。エントリーポイントはライブラリ(JAR、egg、wheelなど)やノートブックとなります。洗練されたリトライ、アラート機構を用いてDatabricksジョブをスケジュール実行することができます。ジョブ実行の主要なインタフェースはJobs APIとUIとなります。"}
{"category":"closed_qa","context":"","instruction":"インスタンス プールは何？","response":"Databricksによって管理されるあなたのアカウント上のインスタンスのセットですが、アイドル状態の際にはDatabricksの課金は発生しません。プールで複数のジョブをサブミットすることで、ジョブがクイックに起動します。プールのインスタンスにガードレール(インスタンスタイプ、インスタンス制限など)とオートスケーリングのポリシーを設定することができます。プールは、他のSparkプラットフォームにおけるオートスケーリングクラスターと同じものです。"}
{"category":"closed_qa","context":"","instruction":"プールでのジョブの実行はできますか？","response":"Jobs APIあるいはUIを用いてプールでジョブを実行することができます。それぞれのジョブに対してクラスターのスペックを指定する必要があります。ジョブが起動しようとすると、Databricksは自動でプールから新規クラスターを作成します。ジョブが完了するとクラスターは自動で停止します。ジョブが実行された時間分だけ課金されることになります。"}
{"category":"closed_qa","context":"","instruction":"Databricksでジョブを実行する際のコスト効率が最も高い方法を教えてください。","response":"Jobs APIあるいはUIを用いてプールでジョブを実行することができます。それぞれのジョブに対してクラスターのスペックを指定する必要があります。ジョブが起動しようとすると、Databricksは自動でプールから新規クラスターを作成します。ジョブが完了するとクラスターは自動で停止します。ジョブが実行された時間分だけ課金されることになります。これは、Databricksでジョブを実行する際のコスト効率が最も高い方法となります。"}
{"category":"closed_qa","context":"","instruction":"プールでジョブを実行する方法はいくつありますか？","response":"プールでジョブを実行するには、API/CLI、Airflow、UIという三つの方法があります。"}
{"category":"closed_qa","context":"","instruction":"DatabricksでどのようにAirflowを使ってジョブをサブミットするか？","response":"DatabricksでジョブをサブミットするためにAirflowを使いたい場合、DatabricksではAirflow operatorを提供しています。DatabricksのAirflowオペレーターは、DatabricksにジョブをサブミットするためにJobs APIのTrigger a new job runオペレーション(POST /jobs/run-now)をコールします。Orchestrate Databricks jobs with Apache Airflowをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"ジョブのログの転送を教えてください。","response":"お使いのクラウドストレージロケーションにクラスターログを転送することができます。選択したロケーションにログを転送するには、new_cluster structureのcluster_log_confパラメーターを使います。"}
{"category":"closed_qa","context":"","instruction":"クラスターのメトリクスのスナップショットをどの頻度で取っていますか？","response":"Databricksはクラスターのメトリクスのスナップショットを15分ごとに作成して格納します。"}
{"category":"closed_qa","context":"","instruction":"ジョブのトラブルシュートのためにどのようにアラートを設定するか？","response":"ジョブ失敗時にアラートを受け取れる様に、Jobs APIのCreate a new jobオペレーション(POST /jobs/create)でemail_notificationsを使います。\n\nまた、これらのメールのアラートをPagerDuty、Slackや他のモニタリングシステムに転送することもできます。"}
{"category":"closed_qa","context":"","instruction":"サーバレスリアルタイム推論は何？","response":"サーバレスリアルタイム推論は、お使いのMLflow機械学習モデルをスケーラブルなREST APIエンドポイントとして公開します。この機能ではサーバレスコンピュートを使用しており、このエンドポイントと関連する計算リソースはDatabricksクラウドアカウントで管理、実行されることを意味します。生じる利用料とストレージコストは無料ですが、課金を開始する際にはDatabricksから連絡を行います。"}
{"category":"closed_qa","context":"","instruction":"クラシックのMLflowモデルサービングはどういうクラスターを使っているか？","response":"クラシックのMLflowモデルサービングでは、現在ではクラシックデータプレーンと呼ばれるお客様自身のアカウントの元で稼働するシングルノードクラスターを使用します。データプレーンには仮想ネットワークと、ノートブックやジョブで使用するクラスター、proおよびクラシックSQLウェアハウス、クラシックモデルサービングエンドポイントの様な関連づけられた計算資源が含まれます。"}
{"category":"closed_qa","context":"","instruction":"なぜサーバレスリアルタイム推論を用いるのでしょうか？","response":"サーバレスリアルタイム推論は以下を提供します。\n\n1クリックでエンドポイントを起動できます: Databricksがあなたのモデルのためにプロダクションレディの環境を準備し、計算処理のためのサーバレス設定オプションを提供します。\n高可用性とスケーラビリティ: サーバレスリアルタイム推論は、プロダクション用途を意図しており、秒間3000クエリー(QPS)までをサポートすることができます。サーバレスリアルタイム推論エンドポイントは、自動でスケールアップ・ダウンするので、エンドポイントはスコアリングのリクエストのボリュームに応じて自動で調整することを意味します。\nダッシュボード: QPS、レーテンシー、エラー率のようなメトリクスを用いて、モデルエンドポイントの健康状態をモニターするために、ビルトインのサーバレス推論ダッシュボードを活用します。\n特徴量ストアのインテグレーション: お使いのモデルが、Databricks Feature Storeの特徴量を用いてトレーニングした場合、モデルは特徴量のメタデータとともにパッケージされます。オンラインストアを設定した際、スコアリングのリクエストを受け取ると、これらの特徴量がリアルタイムで組み込まれます。"}
{"category":"closed_qa","context":"","instruction":"サーバレスリアルタイム推論でサポートしているQPSはどのぐらい？","response":"サーバレスリアルタイム推論は、プロダクション用途を意図しており、秒間3000クエリー(QPS)までをサポートすることができます。"}
{"category":"closed_qa","context":"","instruction":"サーバレスリアルタイム推論のダッシュボードにサポートしているメトリクスは何？","response":"QPS、レーテンシー、エラー率のようなメトリクスを用いて、モデルエンドポイントの健康状態をモニターするために、ビルトインのサーバレス推論ダッシュボードを活用します。"}
{"category":"closed_qa","context":"","instruction":"サーバレスリアルタイム推論にFeature store (特徴量ストア)をサポートしていますか？","response":"はい、特徴量ストアとののインテグレーションをサポートしています。お使いのモデルが、Databricks Feature Storeの特徴量を用いてトレーニングした場合、モデルは特徴量のメタデータとともにパッケージされます。オンラインストアを設定した際、スコアリングのリクエストを受け取ると、これらの特徴量がリアルタイムで組み込まれます。"}
{"category":"closed_qa","context":"","instruction":"サーバレス モデルサービング のレーテンシーはどのぐらい？","response":"レーテンシーのオーバーヘッドはベストエフォートで100m秒以下です。"}
{"category":"closed_qa","context":"","instruction":"サーバレス モデルサービング のペイロードサイズの制限がありますか？","response":"はい、あります。ペイロードサイズの上限はリクエストあたり16MBです。"}
{"category":"closed_qa","context":"","instruction":"サーバレス モデルサービング の内向き通信に関しては？","response":"IP許可リストが有効化されていない場合、サーバレスリアルタイム推論エンドポイントは内向き通信に関しては、インターネットにオープンとなります。IP許可リストはエンドポイントにも適用されます。"}
{"category":"closed_qa","context":"","instruction":"サーバレス モデルサービングにステージングとプロダクションにデプロイするのにどのぐらい時間がかかる？","response":"新たに登録されたモデルバージョンのデプロイには、モデルコンテナイメージの作成とモデルエンドポイントの廃部が含まれます。このプロセスは~5分程度かかることがあります。"}
{"category":"closed_qa","context":"","instruction":"サーバレス モデルサービングへのリクエストの処理時間の制限は何？","response":"モデル計算処理が60秒以上要するとリクエストはタイムアウトします。モデルの計算処理が60秒以上かかることが事前にわかっているのであれば、Databricksサポート窓口にコンタクトしてください。"}
{"category":"closed_qa","context":"","instruction":"モデルサービングのためにエンドポイントを作成するに必要な権限は何？","response":"モデルサービングのためにエンドポイントを作成するために以下の権限が必要となります。\n\nワークスペースでのクラスター作成権限。\nサービングする登録モデルに対するCan Manage Production Versions権限。"}
{"category":"closed_qa","context":"","instruction":"Databricksノートブックにおけるpdbの使い方を教えてください。","response":"Databricksノートブックにおけるpdbの使い方を説明します。\n\n最後の例外からデバッグするには%debugを使います。これは、予期しないエラーに遭遇し、原因をデバッグする際に役立ちます(pdb.pm()と似ています)。\n例外後(しかし、プログラム終了前)にインタラクティブなデバッガーを自動で起動するには%pdb onを使います。これらのコマンドを使う際には、他のセルを実行する前にデバッガーの使用を停止する必要があることに注意してください。"}
{"category":"closed_qa","context":"","instruction":"pdbデバッガーを終了する方法は何？","response":"デバッガーを終了する方法はいくつかあります。\n\n実行中のセルを終了するためにcあるいはcontinueを入力\nエラーをスローしコード実行を停止するためにexitを入力\n出力ボックスの隣にあるCancelをクリックしてコマンドをキャンセル"}
{"category":"closed_qa","context":"","instruction":"Databricksノートブックで事後のデバッグの手順を知りたいです。","response":"Databricksノートブックで%debugを使うには:\n\n例外が起きるまでノートブックのコマンドを実行します。\n新規セルで %debug を実行します。セルの出力エリアでデバッガーが動作を始めます。\n変数を調査するには、入力フィールドに変数名を入力し Enter を押します。\n以下のコマンドを用いることで、変数の調査の様に、コンテキストを切り替えたり他のデバッガータスクを行うことができます。デバッガーの完全なコマンドの一覧に関しては、pdb documentationをご覧ください。文字列を入力し、 Enter を押します。\nn: 次の行\nu: 現在のスタックフレームを抜けて1レベル上に移動\nd: 現在のスタックフレームを抜けて1レベル下に移動\nこのノートブックの最初のセルで説明した方法のいずれかでデバッガーを抜けます。"}
{"category":"closed_qa","context":"","instruction":"Databricksノートブックで%pdb onを使う手順を教えてください。","response":"Databricksノートブックで%pdb onを使うには:\n\nノートブックの最初のセルで%pdb onを実行して自動pdbをオンにします。\n例外が起きるまでノートブックでコマンドを実行します。インタラクティブなデバッガーが起動します。\n変数を調査するには、入力フィールドに変数名を入力し Enter を押します。\n以下のコマンドを用いることで、変数の調査の様に、コンテキストを切り替えたり他のデバッガータスクを行うことができます。デバッガーの完全なコマンドの一覧に関しては、pdb documentationをご覧ください。文字列を入力し、 Enter を押します。\nn: 次の行\nu: 現在のスタックフレームを抜けて1レベル上に移動\nd: 現在のスタックフレームを抜けて1レベル下に移動\nこのノートブックの最初のセルで説明した方法のいずれかでデバッガーを抜けます。\n"}
{"category":"closed_qa","context":"","instruction":"Databricks内で使えるPython APIは何？","response":"Databricksは、シングルマシンと分散Pythonワークロードの両方で実行することができます。シングルマシンの処理においては、通常通りPython APIとライブラリを使用することができます。例えば、pandasやscikit-learnは「普通に動作します」。分散Pythonワークロードに関しては、Databricksはすぐに利用できる2つの人気のあるAPIを提供します。Pandas API on SparkとPySparkです。"}
{"category":"closed_qa","context":"","instruction":"Pandas API on Sparkは？","response":"Pandas API on Sparkは、Apache Spark上で動作するpandasと同等のAPIを提供するライブラリです。このオープンソースのAPIは、pandasには慣れているがApache Sparkには慣れていないデータサイエンティストにとっては理想的な選択肢となります。"}
{"category":"closed_qa","context":"","instruction":"pandasは何？","response":"pandasは、データ分析やデータ操作を行うデータサイエンティストで広く使われているPythonパッケージです。"}
{"category":"closed_qa","context":"","instruction":"PySpark APIは？","response":"PySparkはApache Sparkに対する公式のPython APIです。このAPIはPandas API on Sparkよりも優れた柔軟性を提供します。"}
{"category":"closed_qa","context":"","instruction":"DatabricksノートブックはJupyterとの違いは何ですか？","response":"ノートブックはJupyterと同じ様な機能を提供しますが、ビッグデータを用いたビルトインのビジュアライゼーション、デバッグやパフォーマンス監視のためのApache Sparkインテグレーション、機械学習エクスペリメントのトラッキングのためのMLflowインテグレーションのような機能が追加されています。"}
{"category":"closed_qa","context":"","instruction":"Pythonノートブックでカーネルを再起動するにどうすればいいですか？","response":"Pythonノートブックでカーネルを再起動するには、左上のクラスタードロップダウンをクリックし、Detach & Re-attachをクリックします。これによって、ノートブックがクラスターからデタッチ・再アタッチされ、Pythonプロセスが再起動します。"}
{"category":"closed_qa","context":"","instruction":"Databricks Reposはどういう機能ですか？","response":"Databricks Reposを用いることで、ユーザーはノートブックや他のファイルをGitリポジトリと同期できる様になります。Databricks Reposはコードのバージョン管理やコラボレーションの役に立ち、コードのすべてのリポジトリのDatabricksへのインポート、過去のノートブックのバージョンの参照、IDE開発とのインテグレーションをシンプルにします。"}
{"category":"closed_qa","context":"","instruction":"Databricksクラスターはどのようなクラスターですか？","response":"Databricksクラスターは、シングルノードクラスターから大規模クラスターに至る任意のサイズでのクラスターの計算資源の管理を行います。必要に応じてクラスターのハードウェアやライブラリをカスタマイズすることができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksクラスターに事前インストールされているライブラリの完全な一覧はどこにありますか？","response":"事前インストールされているライブラリの完全な一覧に関しては、Databricks runtime releasesをご覧ください。https://docs.databricks.com/release-notes/runtime/releases.html"}
{"category":"closed_qa","context":"","instruction":"ノートブックやジョブで使用できる様に追加のサードパーティライブラリやカスタムPythonライブラリをどのようにインストールするか？","response":"PyPIや他のリポジトリにあるライブラリを用いてノートブックやジョブの環境を変更することができる、ノートブックスコープPythonライブラリを用いて環境をカスタマイズします。%pip install my_libraryマジックコマンドは、あなたが現在アタッチしているクラスターのすべてのノードにmy_libraryをインストールしますが、共有クラスターの他のワークロードには干渉しません。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのPythonノートブックに提供しているビジュアライゼーションはありますか？","response":"はい、DatabricksのPythonノートブックには様々なタイプのビジュアライゼーションのビルトインサポートがあります。また、レガシーなビジュアライゼーションを使うこともできます。\nまた、サードバーティライブラリを使ってデータを可視化することができます。いくつかはDatabricksランタイムに事前インストールされていますが、カスタムライブラリをインストールすることもできます。有名な選択肢には以下の様なものがあります。\n\nBokeh\nmatplotlib\nPlotly"}
{"category":"closed_qa","context":"","instruction":"Databricksジョブではどういうことができるか？","response":"Databricksジョブを用いることで、Pythonワークロードをスケジューリングあるいは起動することができます。ジョブでは、ノートブック、Pythonスクリプト、Python wheelsを実行することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksで利用できるMLアルゴリズムは何？","response":"Databricksで利用するMLアルゴリズムに関しては、scikit-learn、TensorFlow、Keras、PyTorch、Apache Spark MLlib、XGBoostのような人気のあるPythonツールを含むDatabricks機械学習ランタイムに事前インストールされているライブラリを使うことができます。カスタムライブラリをインストールすることもできます。"}
{"category":"closed_qa","context":"","instruction":"MLflowを使ったMLOpsはどのようなプロセスですか？","response":"MLflowトラッキングを用いることで、モデルの開発を記録し、再利用なフォーマットでモデルを保存することができます。MLflowモデルレジストリを用いることで、モデルのプロダクションへのプロモートを管理、自動化することができます。そして、ジョブとサーバレスリアルタイム推論によるモデルサービングや、クラシックMLflowモデルサービングを用いることで、バッチやストリーミングジョブ、RESTエンドポイントとしてモデルをホストすることができます。"}
{"category":"closed_qa","context":"","instruction":"DatabricksノートブックでPythonコードを開発する以外に、外部の統合開発環境(IDE)を用いて開発を行うことができますか？","response":"はい、PyCharm、Jupyter、Visual Studio Codeのような外部の統合開発環境(IDE)を用いて開発を行うことができます。"}
{"category":"closed_qa","context":"","instruction":"外部の開発環境(IDE)とDatabrikcsの間で成果物を同期するには何か選択肢がありますか？","response":"外部の開発環境とDatabrikcsの間で成果物を同期するにはいくつかの選択肢があります。\n\nコード: Gitを用いてコードを同期することができます。Databricks ReposによるGit連携をご覧ください。\nライブラリとジョブ: 外部で(wheelsのような)ライブラリを作成し、Databricksにアップロードすることができます。これらのライブラリは、Databricksノートブックでインポートすることができ、ジョブ作成に使用することもできます。Databricksにおけるライブラリ管理やDatabricksにおけるジョブ管理をご覧ください。\nリモードマシンでの実行: インタラクティブな開発やテストのためにローカルのIDEからコードを実行することができます。IDEはDatabricksクラスターで大規模な計算処理を実行するために、Databricksとコミュニケーションすることができます。この接続を生成するためにどの様にDatabricks Connectを使うのかを学ぶには、Use IDEs with Databricksをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"Databricksでは、自動化と外部のツールとの連携をサポートしているか？","response":"はい、Databricksでは、自動化と外部のツールとの連携をサポートするREST APIの完全なセットを提供しています。クラスター、ライブラリ、コードや他のワークスペースオブジェクト、ワークロードやジョブの様なリソースを管理するためにAPIを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricks Academyでは何を提供しているか？","response":"Databricks Academyでは、数多くのトピックに対するセルフペース、インストラクター指導のコースを提供しています。"}
{"category":"closed_qa","context":"","instruction":"PySparkとpandasの相互運用性をサポートする機能を知りたいです。","response":"PySparkとpandasの相互運用性をサポートする機能\nDatabricksにおけるpandas function API\nDatabricksにおけるSpark pandasユーザー定義関数\nConvert between PySpark and pandas DataFrames"}
{"category":"closed_qa","context":"","instruction":"PythonとSQLデータベースの接続方法について知りたいです。","response":"方法は二人あります。Databricks SQL Connector for Pythonを用いることで、Databricksリソース上でSQLコマンドを実行するためにPythonコードを使用することができます。\npyodbcを用いることで、ローカルのPythonコードからODBCを通じて、Databricksレイクハウスに格納されているデータに接続することができます。"}
{"category":"closed_qa","context":"","instruction":"20ノードのSparkクラスターを作成したのですが、私のpandasのコードが速くなりません。何が悪いのでしょうか？","response":"シングルノードでの動作を前提としたライブラリを使用しており、Databricksに移行しても、これらは分散処理されません。Apache SparkのPython APIであるPySparkを使ってご自身のコードを書き直す必要があります。\n\nあるいは、Apache Sparkデータフレームのデータにアクセスするために、pandasデータフレームAPIを使用できるPandas API on Sparkを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"私が慣れているアルゴリズムがsklearnにありますが、Spark MLではサポートしていません(例えば、DBSCAN)。どうすればこのアルゴリズムを使いつつSparkのメリットを活かせるのでしょうか？","response":"いくつの方法があります。\n Sparkクラスターでタスクを分散するためのjoblibのApache Sparkバックエンドであるjoblib-sparkを使用します。\npandasユーザー定義関数を使います。\nハイパーパラメーターチューニングにはHyperoptを使います。"}
{"category":"closed_qa","context":"","instruction":"Spark MLにおけるデプロイメントオプションには何がありますか？","response":"ベストなデプロイメントの選択肢はアプリケーションのレーテンシーの要件に依存します。\n\nバッチ予測に関しては、推論、予測のためにモデルをデプロイするを参照ください。\nストリーミングアプリケーションに関しては、構造化ストリーミングを参照ください。\n低レーテンシーのモデル推論に関しては、MLflowモデルサービング、あるいはAmazon SageMakerのようなクラウドプロバイダーのソリューションの活用を検討ください。\n"}
{"category":"closed_qa","context":"","instruction":"pandasや他のライブラリをどのようにインストール、アップデートすることができますか？","response":"ライブラリをインストール、アップデートする方法はいくつか存在します。\n\nクラスターを利用する全てのユーザーが使用するライブラリをインストール、アップデートするにはクラスターライブラリを使います。\n特定のノートブックでのみPythonライブラリやライブラリのバージョンを使えるようにするには、ノートブックスコープライブラリを使います。"}
{"category":"closed_qa","context":"","instruction":"ドライバーノードからどのようにDBFSのデータを参照できますか？","response":"ファイルパスの先頭に/dbfs/を追加してください。ローカルファイルAPIをご覧ください。"}
{"category":"closed_qa","context":"","instruction":"どのようにデータをDatabricksに取り込みますか？\n","response":"いくつの方法があります。マウントします。DBFSへのオブジェクトストレージのマウントをご覧ください。\n\nデータタブを使います。データのインポート、読み込み、変更をご覧下さい。\n\n%sh wgetを使います。\n\nデータファイルのURLを参照できるのであれば、Sparkドライバーノードにデータをインポートするために%sh wget <url>/<filename>を使用することができます。"}
{"category":"closed_qa","context":"","instruction":"Pandas function APIでは何かできますか？","response":"pandas function APIを用いることで、pandasインスタンスを入力として受け取り、pandasインスタンスを出力するネイティブのPython関数をPySparkデータフレームに直接適用できるようになります。"}
{"category":"closed_qa","context":"","instruction":"pandas function APIとpandasユーザー定義関数 (Pandas UDF)の違いは何ですか？","response":"pandas function APIはpandasユーザー定義関数と同様に、データを転送し、pandasがデータを操作する際にApache Arrowを使用します。しかし、pandas function APIではPython型ヒントはオプションです。"}
{"category":"closed_qa","context":"","instruction":"pandas function APIに何種類がありますか？","response":"pandas function APIには3つのタイプがあります。\n\nGrouped map\nMap\nCogrouped map"}
{"category":"closed_qa","context":"","instruction":"pandas function APIとpandas UDFの同じところは何ですか？","response":"pandas function APIは、pandas UDFの実行で使用されるのと同じ内部ロジックを活用します。このため、PyArrow、サポートされるSQLの対応、設定などはpandas UDFと同じ特性を共有します。"}
{"category":"closed_qa","context":"","instruction":"'pandas function API'で「split-apply-combine」パターンをどのように実装されているか？","response":"「split-apply-combine」パターンを実装するために'groupBy().applyInPandas()'を通じて、グループ分けされたデータの変換を行います。split-apply-combineは3つのステップから構成されます。\n\nDataFrame.groupByを用いてデータをグループに分割します。\nそれぞれのグループに関数を適用します。関数の入力、出力は両方ともpandas.DataFrameです。入力データにはそれぞれのグループの全ての行と列が含まれます。\n新たなDataFrameに結果を統合します。"}
{"category":"closed_qa","context":"","instruction":"'groupBy().applyInPandas()'を使用するには何か必要ですか？","response":"groupBy().applyInPandas()を使用するには、以下を定義する必要があります。\n\nそれぞれのグループに対する処理を定義するPython関数\n出力DataFrameのスキーマを定義するStructTypeオブジェクトあるいは文字列 \n 返却されるpandas.DataFrameのカラムのラベルは、出力スキーマが文字列として定義された場合はフィールド名が一致する必要があり、文字列でない場合には、integerのインデックスなどフィールのデータ型がマッチする必要があります。"}
{"category":"closed_qa","context":"","instruction":"'pandas function API'の'groupBy().applyInPandas()'を使ってアウトオブメモリー例外が発生しました。どうすればいいですか？","response":"'pandas function API'の'groupBy().applyInPandas()'関数を適用する前に、グループの全データがメモリーにロードされます。特にグループのサイズに偏りがある場合、アウトオブメモリー例外を引き起こすことがあります。設定maxRecordsPerBatchはグループには適用されないので、ご自身の手で、グループ分けされたデータがメモリーに収まるようにする必要があります。"}
{"category":"closed_qa","context":"","instruction":"'pandas function API'の'DataFrame.mapInPandas()'の仕組みを知りたいです。","response":"DataFrame.mapInPandas()を用いることでpandasのインスタンスを用いたmapオペレーションを実行し、pandas.DataFrameのイテレータを現在のPySparkデータフレームを表現するpandas.DataFrameの別のイテレータに変換し、結果をPySparkデータフレームとして返却します。"}
{"category":"closed_qa","context":"","instruction":"'pandas function API'のCogrouped mapオペレーションの仕組みを知りたいです。","response":"pandasインスタンスを用いたCogrouped mapオペレーションには、共通のキーでグループ結合される2つのPySparkDataframeに対してDataFrame.groupby().cogroup().applyInPandas()を使用し、それぞれの共通グループにPython関数が適用されます。以下のステップから構成されます。\n\nそれぞれのデータフレームでキーを同じくするグループのデータがシャッフルされ、同じ共通グループにまとめられます。\nそれぞれの共通グループに関数を適用します。関数の入力はpandas.DataFrameです(オプションとしてキーを表現するタプル)。関数の出力はpandas.DataFrameです。\n全てのグループのpandas.DataFrameを新たなPySpark DataFrameで結合します。\n"}
{"category":"closed_qa","context":"","instruction":"'pandas function API'の'groupBy().cogroup().applyInPandas()'を使用するには何か必要ですか？","response":"groupBy().cogroup().applyInPandas()を使用するには、以下を定義する必要があります。\n\nそれぞれの共通グループに対する計算処理を定義するPython関数\n出力DataFrameのスキーマを定義するStructTypeオブジェクトあるいは文字列\n返却されるpandas.DataFrameのカラムのラベルは、出力スキーマが文字列として定義された場合はフィールド名が一致する必要があり、文字列でない場合には、integerのインデックスなどフィールのデータ型がマッチする必要があります。"}
{"category":"closed_qa","context":"","instruction":"'pandas function API'の'groupBy().cogroup().applyInPandas()'を使ってアウトオブメモリー例外が発生しました。どうすればいいですか？","response":"関数を適用する前に、グループの全データがメモリーにロードされます。特にグループのサイズに偏りがある場合、アウトオブメモリー例外を引き起こすことがあります。設定maxRecordsPerBatchはグループには適用されないので、ご自身の手で、グループ分けされたデータがメモリーに収まるようにする必要があります。"}
{"category":"closed_qa","context":"","instruction":"pandasのユーザー定義関数(UDF)は何？","response":"ベクトライズドUDFとしても知られるpandasのユーザー定義関数(UDF)は、データを操作するためのpandasにデータを転送するためにApache Arrowを用いるユーザー定義関数です。"}
{"category":"closed_qa","context":"","instruction":"'pandas UDF' と'Python UDF'の性能違いはどのぐらい？","response":"pandasのUDFを用いることで、行単位で処理を行うPython UDFと比べて100倍もの性能改善を実現することができます。"}
{"category":"closed_qa","context":"","instruction":"'pandas UDF'はどうやって定義しますか？","response":"デコレーターとしてpandas_udfキーワードを用いてpandas UDFを定義し、Python型ヒントで関数をラップします。"}
{"category":"closed_qa","context":"","instruction":"'シリーズ to シリーズUDF'の裏ではどのようなことが実行されているか？","response":"Python関数はpandasのSeriesを入力として受け取り、同じ長さのpandas Seriesを返却する必要があり、これらをPythonの型ヒントで指定する必要があります。Sparkはカラムをバッチに分割し、データのサブセットとしてそれぞれのバッチに対して関数を呼び出し、結果を結合することでpandas UDFを実行します。"}
{"category":"closed_qa","context":"","instruction":"'シリーズイテレーター to シリーズイテレーターUDF'の裏ではどのようなことが実行されているか？","response":"イテレーターUDFは以下の点を除いてスカラーのpandas UDFと同じです。\n\nPython関数\n入力として単一のバッチを受け取るのではなく、バッチのイテレーターを受け取ります。\n単一のアウトプットバッチを返却するのではなく、アウトプットバッチのイテレーターを返却します。\nイテレーターにおけるアウトプット全体の長さは、インプット全体の長さと同じである必要があります。\nラップされたpandas UDFは、入力として単一のSparkカラムを受け取ります。\nPythonの型ヒントとしてIterator[pandas.Series] -> Iterator[pandas.Series]を指定する必要があります。"}
{"category":"closed_qa","context":"","instruction":"'シリーズイテレーター to シリーズイテレーターUDF'はどういう場合に有用ですか？","response":"このpandas UDFは、UDFの実行で何かしらの状態の初期化、例えば、それぞれの入力バッチに対して推論を適用するために機械学習モデルファイルをロードするような場合には有用です。"}
{"category":"closed_qa","context":"","instruction":"'複数シリーズイテレーター to シリーズイテレーターUDF'はどの場合に有用ですか？","response":"複数シリーズイテレーター to シリーズイテレーターUDFはシリーズイテレーター to シリーズイテレーターUDFと同様の特定、制限を持っています。指定された関数は、バッチのイテレーターを受け取り、バッチのイテレーターを出力します。この関数も、UDFの実行で何かしらの状態初期化が必要な場合に有用です。\n\n"}
{"category":"closed_qa","context":"","instruction":"'Pandas UDF'を使う場合にアウトオブメモリー例外を回避する方法を知りたいです。","response":"Sparkにおけるデータのパーティションは、Arrowのレコードバッチに変換され、JVMにおけるメモリー使用量の増大を引き起こします。アウトオブメモリー例外を回避するために、spark.sql.execution.arrow.maxRecordsPerBatch設定を、それぞれのバッチにおける最大レコード数を決定する整数値に設定することで、Arrowのレコードバッチのサイズを調整することができます。デフォルト値はバッチあたり10,000レコードとなっています。カラム数が大きい場合には、それに応じて値を調整する必要があります。この制限を用いることで、データのパーティションは、処理を行うための1つ以上のレコードバッチに分割されます。"}
{"category":"closed_qa","context":"","instruction":"Sparkの内部にどのタイムゾーンを利用しているか？","response":"Sparkは内部でタイムスタンプをUTCの値として保持し、特定のタイムゾーンの指定がない場合、取り込まれるタイムスタンプデータは、ローカル時間として、マイクロ秒の解像度でUTCに変換されます。"}
{"category":"closed_qa","context":"","instruction":"オールパーパスクラスターとジョブクラスターの違いは何ですか？","response":"オールパーパス(all-purpose)クラスターは複数のユーザーで共有でき、アドホック分析、データ探索、開発においてはベストな選択肢と言えます。処理の実装が完了して本格運用の準備が整ったら、その処理をジョブクラスターで実行するように切り替えます。ジョブクラスターはジョブが終了した際に削除されるので、リソースの利用料とコストを削減することができます。"}
{"category":"closed_qa","context":"","instruction":"クラスターのコストの主要要素は何？","response":"クラスターのコストの主要要素は、クラスターによって消費されるDatabricksユニット(DBU)と、クラスターと実行するのに必要となるインフラリソースのコストとなります。目立たない副次的なコストには、SLAを満たせないことによるビジネスコストや、貧弱なコントロールによって失わられるリソース、従業員の生産性が挙げられます。"}
{"category":"closed_qa","context":"","instruction":"Databricksでサポートしているクラスターモードは何？","response":"Databricksでは3つのクラスターモードをサポートしています。Standard、High ConcurrencyとSingle Nodeです。多くの一般的なケースではStandardとSingle Nodeクラスターを使用します。"}
{"category":"closed_qa","context":"","instruction":"Standard cluster(標準クラスター)は？","response":"Standard cluster(標準クラスター)は、大規模データをApache Sparkで処理するのに適しています。"}
{"category":"closed_qa","context":"","instruction":"Single Node cluster(シングルノードクラスター)は？","response":"Single Node cluster(シングルノードクラスター)は、小規模データを利用するジョブや、シングルノードの機械学習ライブラリのような非分散のワークロードを行うためのものです。"}
{"category":"closed_qa","context":"","instruction":"High Concurrency cluster(ハイコンカレンシークラスター)は？","response":"High Concurrency cluster(ハイコンカレンシークラスター)は、複数のユーザーがリソースを共有したい場合やアドホックなジョブを実行する際に最適です。通常、管理者がハイコンカレンシークラスターを作成します。ハイコンカレンシークラスターではオートスケーリングを活用することをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"オンデマンド、スポットインスタンスはサポートしていますか？","response":"Databricksにおいては、オンデマンドインスタンスと(カスタムスポットプライスを指定した)スポットインスタンスを組み合わせてクラスターを構成することができ、ユースケースに合わせてクラスターを構築することができます。例えば、下の設定においては、ドライバーノードと4台のワーカーノードはオンデマンドインスタンス、残りの4台のワーカーノードはスポットインスタンス(最大のスポットインスタンスプライスはオンデマンドの100%)として起動すべきと設定しています。"}
{"category":"closed_qa","context":"","instruction":"オートスケーリングを活用するメリットは何か？","response":"オートスケーリングを活用することで、リソース利用量を最適化するためにワークロードに応じて、ワーカーノードの数を自動で増減することができます。オートスケーリングを有効化することで、DatabricksはSparkジョブを実行するのに最適なワーカーの数を自動的に決定します。オートスケーリングを用いることで、ワークロードに適した数のクラスターを気にすることなしに、クラスターの高い利用率を実現できます。これにより二つのメリットを享受することができます：\n\n固定サイズのクラスターと比較して多くのケースでコストを削減できます。\n固定サイズのクラスターで能力不足が生じた場合と比較して、オートスケーリングのワークロードは高速に動作します。\nspark-submitジョブやいくつかのpythonパッケージなど、いくつかのワークロードはオートスケーリングクラスターとの互換性がありません。\nシングルユーザーのオールパーパスクラスターにおいて、最小ワーカー数が少なく設定されている場合、オートスケーリングによって開発や分析がスローダウンするケースがあります。これは実行しているコマンドやクエリーが数分に渡ることがあり、クラスターがアイドル状態になった際にコストを節約するためにスケールダウンが発生するためです。次のコマンドが実行された際には、クラスターマネージャーはスケールアップしようとしますが、クラウドプロバイダーからインスタンスを取得するのに数分を要することがあります。この際、ジョブは不十分なリソースで実行しなくてはならず、結果を主とkするのに時間を要することになります。ワーカーの最小数を引き上げることで改善はできますが、このことはコストも引き上げることになります。これは、コストと性能のバランスを取るべきもう一つの例となります。\nDelta Cachingを使用している際には、ノードが削除された際にはノードにキャッシュされたいかなるデータも削除されることに留意してください。ワークロードにおいてキャッシュデータの維持が重要な場合には、固定サイズのクラスターを利用することを検討してください。\nETLワークロードでジョブクラスターを利用している場合、ジョブが当面変更されない場合には適切にクラスターをチューニングすることでサイジングすることができます。しかし、データサイズが増加する際にはオートスケーリングによって柔軟性を手にすることができます。また、最適化されたオートスケーリングによって、長い期間実行されるジョブのクラスターが想定されるより利用されない、他のプロセスの結果を待つ必要がある場合において、コストを削減することができます。繰り返しになりますが、クラスターが適切にスケールアップする際にも微小な遅れが生じる場合が相rます。ジョブに対して厳しいSLAが求められる場合には、固定サイズのクラスターが優れた選択肢かもしれませんし、Databricksのプールを活用してクラスター起動時間を削減した方が良いかもしれません。\n"}
{"category":"closed_qa","context":"","instruction":"Databricksではローカルストレージのオートスケーリングもサポートしていますか？","response":"はい、Databricksではローカルストレージのオートスケーリングもサポートしています。ローカルストレージのオートスケーリングによって、クラスターのSparkワーカーのディスク空き容量を監視します。ワーカーのディスクの空きが減ってきた際、Databricksは自動的にディスク容量が枯渇する前にマネージドのボリュームを追加します。"}
{"category":"closed_qa","context":"","instruction":"インスタンス プールを活用するメリットは何？","response":"プールを活用することで、利用可能なインスタンスの群を維持し、クラスターの起動、スケールアップに要する時間を短縮できます。コストを最小化しつつ処理時間を改善するためにプールの活用をお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Databricksランタイムのバージョンをどうやって選択するか？","response":"オールパーパスクラスターにおいては、最新のDatabricksランタイムを活用することをお勧めします。最新のバージョンを利用することで、最新の最適化、コードとロード済みパッケージの互換性を活用できます。\n\n運用のワークロードを実行するジョブクラスターに関しては、長期サポート(LTS)Databricksランタイムの利用をご検討ください。LTSバージョンを利用することで、互換性に関する問題を回避し、ワークロードをアップグレードする前に包括的なテストが行えます。機械学習やゲノミクスの最新ユースケースを活用する際には、特定のDatabricksランタイムのバージョンの利用を検討ください。"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーを使って何かできるか？","response":"Databricksのクラスターポリシーを活用することで、管理者はクラスターの作成、設定に制限をかけることができます。"}
{"category":"closed_qa","context":"","instruction":"クラスターの自動停止をどのように設定するか？","response":"特定期間(デフォルトは120分)が経過した後にクラスターは自動で停止します。\n\n管理者はクラスターポリシーを作成する際にデフォルトの設定を変更することができます。設定値を引き下げることで、クラスターのアイドル時間を減らしコストを削減できます。クラスターが停止しされた際には、すべての変数、一時テーブル、キャッシュ、関数、オブジェクトなどすべての状態が失われることに注意してください。クラスターが再起動した際に、これらの状態が復元される必要があるかもしれません。開発者が30分の昼食に席を外した場合、同じ状態に戻すまでに同じ時間を無駄にすることになるかもしれません。"}
{"category":"closed_qa","context":"","instruction":"アイドル状態のクラスターには課金が発生しますか？","response":"はい、アイドル状態のクラスターはDBUを消費し続け、停止前のアイドル状態のクラウドインスタンスにも課金が発生します。"}
{"category":"closed_qa","context":"","instruction":"ガーベージコレクションに注意すべきですか？","response":"はい、ガーベージコレクションに注意を払うことで、クラスターのジョブ実行性能を最適化できる可能性があります。大容量のRAMによって、ジョブの性能を改善できるかもしれませんが、ガーベージコレークションの際に遅延を生じる可能性があります。\n\n長期間のガーベージコレクションの影響を最小化するために、それぞれのインスタンスに大容量のRAMを割り当てることを避けてください。ワーカーに対して大容量のメモリーを割り当てることは、長期間のガーベージコレクションを引き起こします。代わりに、小規模のRAMをインスタンスに割り当て、ジョブが大量のメモリを必要とするのであれば、より大量のインスタンスをデプロイするようにしてください。しかし、クラスターのサイジングの検討で議論しているような、多くのシャッフルを必要とするようなワークロードのように、大容量メモリの少数のノードが適しているケースがあります。"}
{"category":"closed_qa","context":"","instruction":"クラスターのアクセスコントロールをどのように設定するか？","response":"クラスターのアクセスコントロールに２種類のクラスター権限を設定できます。\n\nユーザーがクラスターを作成できるかどうかを制御できるAllow Cluster Creation権限\n特定のクラスターを利用、編集できるかを設定するクラスターレベルの権限\nクラスターの権限設定に関しては、cluster access controlを参照ください。\n\nクラスター作成(Allow Cluster Creation)権限がある、あるいはポリシーの設定に基づいたクラスター作成を許可するクラスターポリシーにアクセスできる場合にはクラスターを作成することができます。クラスターの作成者はオーナーとなり、Can Manage権限を有することになります。これによって、クラスターに対するデータアクセス権限の制約の中で、他のユーザーにクラスターを共有することができます。\n\n一般的なシナリオにおいては、クラスター設定を決定するためにクラスター権限とクラスターポリシーを理解することが重要になります。"}
{"category":"closed_qa","context":"","instruction":"クラスタータグのいいことは何？","response":"組織内の異なるグループがクラウドリソースを使う際にクラスタータグを利用することで、容易にコストをモニタリングできます。"}
{"category":"closed_qa","context":"","instruction":"クラスタータグをどうやって作成するか？","response":"クラスターを作成する際に、キーバリューの形式でタグを指定することで、DatabricksはインスタンスのEBSなどのクラウドリソースにタグを付与します。"}
{"category":"closed_qa","context":"","instruction":"クラスターのサイジングのベストプラクティスを教えてください。","response":"Databricksにおいては、一つのワーカーノードあたり一つのエグゼキューターが稼働します。このため、Databricksアーキテクチャーの文脈においてはワーカーとエグゼキューターが同じ意味で用いられます。クラスターサイズはワーカーの数で決定されると考えるかもしれませんが、他に考慮すべき要因があります。\n\nトータルのエグゼキューターのコア(コンピュート): 全エグゼキューターのトータルのコア数。この値がクラスターの最大並列実行数を決定します。\nトータルのエグゼキューターのメモリー： 全エグゼキューターのトータルのRAM。ディスクに溢れる前にメモリに蓄積できるデータ量を決定します。\nエグゼキューターのローカルストレージ: ローカルディスクストレージのタイプと総量。主にシャッフルとキャッシュの際のメモリ溢れ(スピル)が発生した場合に使用されます。\n追加で検討すべきことには、上記の要因に影響を与えるワーカーのインスタンスタイプとサイズがあります。クラスターのサイジングをする際には以下を検討してください。\n\nワークロードはどれだけのデータを消費するのか？\nワークロードの計算の複雑性はどの程度か？\nどこからデータを読み込むのか？\n外部ストレージにおいてデータはどのようにパーティショニングされているのか？\nどれだけの並列性を必要とするのか？\nこれらの疑問に答えることは、ワークロードに最適なクラスター設定を決定するのに役立ちます。狭い変換(入力パーティションが一つの出力パーティションに影響を及ぼす)のみを行うシンプルなETLワークロードにおいては、計算最適化設定に注力します。大量のシャッフルが予想され、メモリーおよびデータ溢れのためのディスクサイズが重要になります。シャッフルが大量に発生するワークロードにおいて、少数の大規模インスタンスはマシン間のデータ転送のネットワークI/Oを削減します。\n\nワーカーの数とワーカーインスタンスタイプのサイズにはトレードオフの関係があります。40コア、100GBメモリーを持つ2つのワーカーのクラスターは、10コア、25GBメモリーを持つ8台のワーカーと同程度の計算性能、メモリーを有することになります。\n\n同じデータを何度も読み込む場合には、キャッシュの恩恵を受けることができます。Delta Cacheとストレージ最適化設定を検討してください。"}
{"category":"closed_qa","context":"","instruction":"機械学習モデルのトレーニングに使うクラスターのサイズはどうすればいいですか？","response":"初回の機械学習モデルのトレーニングは実験的なものであるため、クラスターAのような小規模クラスターがお勧めです。小規模クラスターはシャッフルの影響も軽減します。\n\nステージが進むなどして、安定性が課題になるのであれば、より規模の大きいクラスターBやクラスターCがお勧めです。"}
{"category":"closed_qa","context":"","instruction":"機械学習モデルのトレーニングにストレージ最適化インスタンスの利用はおすすめですか？","response":"同じデータを繰り返し読み込むため、Delta Cachingを有効化したストレージ最適化インスタンスの活用、トレーニングデータのキャッシュがお勧めです。ストレージ最適化インスタンスの計算能力、ストレージサイズが不十分である場合には、GPU最適化ノードの利用を検討ください。この場合、Delta Cachingを利用できないというデメリットがあります。"}
{"category":"closed_qa","context":"","instruction":"分析のワークロードの場合の推奨のクラスター利用方法はありますか？","response":"同じデータを繰り返し読み込むため、Delta Cachingを有効化したストレージ最適化インスタンスの活用、トレーニングデータのキャッシュがお勧めです。ストレージ最適化インスタンスの計算能力、ストレージサイズが不十分である場合には、GPU最適化ノードの利用を検討ください。この場合、Delta Cachingを利用できないというデメリットがあります。\n\n分析のワークロードで推奨する他の機能には以下のものがあります。\n\n利用されない時間が続いた後に、クラスターが停止されるように自動停止を有効化してください。\n分析者特有のワークロードに応じてオートスケーリングの活用を検討ください。\n事前に定義されたインスタンスタイプ、一貫性のある設定にクラスターを統一できるようにプールの活用を検討ください。"}
{"category":"closed_qa","context":"","instruction":"複雑なバッチETLを行う場合にクラスターのサイジングはどうすればいいですか？","response":"unionや複数テーブルのjoinなどの処理を必要とするより複雑なETLジョブにおいては、シャッフルされるデータ量を最小化することが重要となります。クラスターのワーカー数を削減することでシャッフルを最小化できますので、ハイスペックのクラスターではなく、少数のクラスターを検討すべきです。\n複雑な変換処理は計算資源を多く必要としますので、ワークロードに最適なコアを割り当てるためには、クラスターにノードを追加する必要があるかもしれません。\n\nシンプルなETLジョブと同様、計算最適化のインスタンスタイプをお勧めします。これらは比較的安価であり、このタイプのワークロードは大量のメモリー、ストレージを必要としません。また、シンプルなETLジョブ同様に、クラスターの起動時間を削減し、ジョブパイプライン実行に要するトータルの時間を削減するためにプールの活用を検討ください。"}
{"category":"closed_qa","context":"","instruction":"クラスター提供の推奨アプローチを教えてください。","response":"クラスター提供の推奨アプローチは、オートスケーリングを伴うクラスターを提供するというハイブリッドのアプローチです。ハイブリッドアプローチでは、オンデマンドインスタンスの数とスポットインスタンスの数を定義し、インスタンスの際少数、再代数を指定してオートスケーリングを有効化します。このクラスターは常時稼働であり、グループのユーザーが利用することができ、負荷に応じてスケールアップ・ダウンします。\n\nユーザーにはクラスターを開始・停止する権限はありませんが、初期状態から設定されているオンデマンドインスタンスはユーザーからのクエリに即座に反応します。ユーザーのクエリがより多くのキャパシティを必要とした際には、ワークロードに対応できるようにオートスケーリングがより多くのノード(多くはスポットインスタンス)を配置します。"}
{"category":"closed_qa","context":"","instruction":"データサイエンティストが複雑なデータ探索や機械学習アルゴリズムを実行するといった、組織内の特定のユースケース、チームにクラスターを提供する必要があります。このタイプのワークロードにおけるベストなアプローチを知りたいです。","response":"ベストなアプローチは、デフォルト値、固定値、設定のレンジを事前定義した設定のクラスターポリシーを作成するというものです。これにはインスタンスの数、インスタンスタイプ、スポットとオンデマンドインスタンスの比率、ロール、インストールすべきライブラリなどが含まれます。クラスターポリシーを活用することでユーザーは高度な要件に対しても、ユースケースに応じた設定を行い迅速にクラスターを起動でき、ポリシーによってコスト、コンプライアンスを制御することができます。\nこのアプローチは、ユーザーに対してクラスターを起動することのできるより強い権限を与えるものですが、依然として事前に定義した設定の制限をかけることでコストをコントロールすることが可能です。また、このアプローチにおいては、異なるグループのユーザーに対して、異なるデータへのアクセスが許された異なるクラスターを割り当てることも可能です。\n\nこのアプローチにおける欠点は、クラスターにおけるあらゆる変更、設定、ライブラリなど全てにおいて管理者の関与が必要となることです。"}
{"category":"closed_qa","context":"","instruction":"バッチワークロードのクラスター提供アプローチを知りたいです。","response":"ベストプラクティスはそれぞれのジョブ実行で新規にクラスターを起動するというものです。新規クラスターによるジョブ実行によって、共有クラスターで他の作業と競合することによるSLA違反や処理失敗を避けることができます。ジョブの重要度レベルに応じて、SLAを満足するためにオンデマンドインスタンを使用できますし、コスト低減のためにスポットインスタンスとオンデマンドインスタンスを組み合わせることも可能です。"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーは何に対して制限するか？","response":"クラスターポリシーは、ルールセットに基づいてクラスターの設定を行う能力を制限します。ポリシールールはクラスター作成で利用できる属性や属性値を制限します。クラスターポリシーには利用を特定のユーザーやグループに制限するACLがあります。"}
{"category":"closed_qa","context":"","instruction":"クラスタポリシーを使って何かできるか？","response":"クラスタポリシーを用いることで以下のことが可能となります。\n\nユーザーによるクラスターの作成を事前に定義された設定に限定する。\n(いくつかの値を固定値にし非表示にすることで)ユーザーインタフェースをシンプルにし、より多くのユーザーが自身のクラスターを作成できるようにする。\n(時間あたりの価格に影響する属性値に制限をかけることで)クラスターあたりの最大コストを制限することによるコストコントロール"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーのアクセス権によってできることは？","response":"クラスターポリシーのアクセス権によって、ユーザーがクラスターを作成する際のポリシードロップダウンで、どのポリシーを選択できるのかを制限することができます。\n\nクラスターの作成権限を持つユーザーは、制限なしポリシーを選択でき、完全に設定可能なクラスターを作成することができます。\nクラスター作成権限を持ちクラスターポリシーにアクセスできるユーザーは制限なしポリシーとアクセス権があるポリシーを選択することができます。\nクラスターポリシーにのみアクセスできるユーザーは、アクセス権があるポリシーを選択することができます。"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーの強制ルールは？","response":"ポリシールールでは以下のタイプの制約を表現することができます。\n\n無効化された制御要素に対する固定値\nUIで非表示にされた制御要素に対する固定値(JSONビューでは値は見えます)\n値のセットに限定された属性値(許可リストあるいはブロックリスト)\n指定された正規表現に合致する属性値\n特定のレンジに限定された数値属性\n有効化された制御要素においてUIで使用されるデフォルト値"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーで管理できるクラスター属性は何？","response":"クラスターポリシーはClusters API 2.0で制御される全てのクラスター属性をサポートしています。(タイプとクラスターのフォームのUI要素との関係に基づいて)サポートされる特定のタイプの制約は、フィールドごとに異なる場合があります。\n\nさらに、クラスターポリシーでは以下の合成属性をサポートしています。\n\n時間単位でクラスターが使用できる最大DBUである\"max DBU-hour\"メトリック。このメトリックは個々のクラスターレベルでのコストを直接コントロールする方法となります。\nクラスターを作成するソースの制限: ジョブサービス(jobクラスター)、クラスターUI、クラスターREST API(all-purposeクラスター)。"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーで管理できないクラスター属性は何？","response":"以下のクラスター属性をクラスターポリシーで制限することはできません。\n\nLibraries API 2.0で管理されるライブラリ。ワークアラウンドはカスタムコンテナかinitスクリプトを使うというものです。\nユーザーあたり作成できるクラスターの数(合計も同時に存在する数も)。ポリシーのスコープは単一のクラスターであり、ユーザーによって作成されたクラスターを知ることができません。\n別のAPIで管理されるクラスターのアクセス権(ACL)"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーのタイプのリストをください。","response":"固定ポリシー\n禁止ポリシー\n制限ポリシー：共通フィールド\n許可リストポリシー\nブロックリストポリシー\n正規表現ポリシー\nレンジポリシー\n無制限ポリシー"}
{"category":"closed_qa","context":"","instruction":"データフレームとは？","response":"データフレームは、異なるタイプを持つことがあるカラムを持つ2次元のラベル付きデータ構造です。データフレームをスプレッドシート、SQLテーブル、シリーズオブジェクトのディクショナリーのようなものと考えることができます。Apache Sparkデータフレームは、一般的なデータ分析問題を効率的に解決できる様に、豊富な機能セット(カラムの選択、フィルター、join、集計)を提供します。\n\nApache Sparkデータフレームは、耐障害性分散データセット(RDD)の上に構築された抽象化レイヤーです。SparkデータフレームとSpark SQLは統合されたプランニングと最適化エンジンを使用するので、Databricksでサポートされているすべての言語(Python、SQL、Scala、R)でほぼ同じパフォーマンスを得ることができます。"}
{"category":"closed_qa","context":"","instruction":"パーソナルコンピュート(Personal Compute)とは？","response":"パーソナルコンピュートは、すべてのDatabricksワークスペースで利用できるDatabricks管理のデフォルトクラスターポリシーです。このポリシーを用いることで、ユーザーは個人で利用するためのシングルマシンの計算資源を簡単に作成することができ、すぐにワークロード実行を開始し、計算資源管理のオーバーヘッドを最小化することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksではワークスペースの意味は？","response":"Databricksではワークスペースには2つの意味があります。\n\nDatabricksのすべての資産にアクセスするためにチームが使用する統合環境として機能するクラウド上のDatabricksデプロイメントです。皆様の組織においては、必要に応じて単一のワークスペースか複数のワークスペースを持つことができます。\nDatabricksのペルソナベースの環境のためのUIです。"}
{"category":"closed_qa","context":"","instruction":"DBUは何？","response":"Databricksユニットです。Databricksは、VMインスタンスタイプに基づく時間あたりの計算能力の単位であるDatabricksユニット(DBU)に基づいて課金します。"}
{"category":"closed_qa","context":"","instruction":"Databricksノートブックは？","response":"実行可能なコマンド、ビジュアライゼーション、ナラティブなテキストを含むドキュメントのwebベースのインタフェースです。"}
{"category":"closed_qa","context":"","instruction":"Databricksダッシュボードは？","response":"ビジュアライゼーションへのアクセスを整理するインタフェースです。"}
{"category":"closed_qa","context":"","instruction":"ライブラリとは？","response":"お使いのクラスターで実行されるノートブックやジョブで利用できるコードのパッケージです。Databricksランタイムには多くのライブラリが含まれており、自分で追加することもできます。"}
{"category":"closed_qa","context":"","instruction":"Repoとは？","response":"リモートGitリポジトリと同期することで、コンテンツを共同バージョン管理できるフォルダーです。"}
{"category":"closed_qa","context":"","instruction":"エクスペリメントとは？","response":"機械学習モデルトレーニングに対応するMLflowランのコレクションです。"}
{"category":"closed_qa","context":"","instruction":"DatabricksのUIに何を提供している？","response":"DatabricksのUIはワークスペースのフォルダーとそこに含まれるオブジェクト、データオブジェクト、計算資源に対して使いやすいGUIを提供します。"}
{"category":"closed_qa","context":"","instruction":"Repoとは？","response":"リモートGitリポジトリと同期することで、コンテンツを共同バージョン管理できるフォルダーです。"}
{"category":"closed_qa","context":"","instruction":"Databricksファイルシステム(DBFS)とは？","response":"blobストア上にあるファイルシステム抽象化レイヤーです。ファイルや他のディレクトリを格納するディレクトリが含まれます。DBFSには、皆様がDatabricksを学ぶために活用できるいくつかのデータセットが自動的に登録されます。"}
{"category":"closed_qa","context":"","instruction":"テーブルとは？","response":"構造化データ表現です。Apache Spark SQLやApache Spark APIでテーブルにクエリーを実行することができます。。"}
{"category":"closed_qa","context":"","instruction":"メタストアとは？","response":"カラムやカラムタイプ情報、データの読み書きに必要なシリアライザ、デシアライザを含むデータウェアハウスの様々なテーブルやパーティションに関するすべての構造情報と、データが格納される対応ファイルのすべてを格納するコンポーネントです。すべてのDatabricksデプロイメントにはテーブルメタデータを永続化するために、すべてのクラスターからアクセスできる中央Hiveメタストアが含まれています。また、外部Hiveメタストアを選択することもできます。"}
{"category":"closed_qa","context":"","instruction":"クラスターとは？","response":"ノートブックやジョブを実行する一連の計算資源と構成です。クラスターにはall-purpose(汎用)とjobの2つのタイプがあります。\n\nUI、CLI、REST APIを用いてall-purposeクラスターを作成します。手動でall-purposeクラスターを停止、再起動することができます。コラボレーティブかつインタラクティブ分析を行うために、この様なクラスターを複数ユーザーで共有することができます。\n新規ジョブクラスターでジョブを実行する際に、Databricksジョブスケジューラーがジョブクラスターを作成し、ジョブが完了するとクラスターを停止します。ジョブクラスターを再起動することはできません。"}
{"category":"closed_qa","context":"","instruction":"Databrikcsランタイムとは？","response":"クラスターで実行されるDatabricksによって管理される一連のコアコンポーネントです。Databricksではいくつかのタイプのランタイムを提供しています。\n\nDatabricksランタイムには、Apache Sparkだけではなく、ビッグデータ分析における使いやすさ、性能、セキュリティを劇的に改善する多数のコンポーネントとアップデートが含まれています。\nDatabricks機械学習ランタイムはDatabricksランタイムをベースとしており、機械学習やデータサイエンスですぐに利用できる環境を提供します。TensorFlow、Keras、PyTorch、XGBoostの様な様々な人気のあるライブラリが含まれています。\nDatabricks Lightは、オープンソース版のApache SparkランタイムのDatabricksパッケージです。Databricksランタイムによって提供される高度なパフォーマンス、信頼性、オートスケーリングのメリットが不要な場合にジョブに対するランタイムの選択肢を提供します。JAR、Python、spark-submitジョブを実行するためにクラスターを作成する際にのみDatabricks Lightを選択することができます。インタラクティブやノートブックジョブのワークロードを実行するクラスターではこれを選択することはできません。"}
{"category":"closed_qa","context":"","instruction":"プールとは？","response":"クラスターの起動時間とオートスケーリングの時間を削減するアイドル状態で利用可能なインスタンスのセットです。プールにアタッチされると、クラスターはプールからドライバーノードとワーカーノードを割り当てます。プールにクラスターのリクエストに答えるための十分なアイドルリソースがない場合には、プールはインスタンスプロバイダーから新規インスタンスを割り当てることて拡張します。アタッチされたクラスターが停止されると、使用されたインスタンスはプールに返却され、別のクラスターで再利用できる様になります。\n\n"}
{"category":"closed_qa","context":"","instruction":"ワークフローとは？","response":"データ処理パイプラインを開発、実行するためのフレームワークです。\n\nCreate, run, and manage Databricks Jobs: ノートブックやライブラリを即時あるいはスケジュールベースで実行する非インタラクティブなメカニズムです。\nDelta Live Tables introduction: 高信頼、維持可能、テスト可能なデータ処理パイプラインを構築するためのフレームワークです。"}
{"category":"closed_qa","context":"","instruction":"ワークロードとは？","response":"Databricksでは異なる課金スキームに対応する2つのタイプのワークロードを認識します。データエンジニアリング(job)とデータアナリティクス(all-purpose)です。\n\nデータエンジニアリング: Databricksジョブスケジューラーがワークロードごとに作成するジョブクラスターで実行される(自動化)ワークロードです。\nデータアナリティクス: all-purposeクラスターで実行される(インタラクティブな)ワークロードです。インタラクティブなワークロードは、通常Databricksノートブックのコマンドによって実行されます。しかし、既存のall-purposeクラスターでのジョブ実行もインタラクティブワークロードと見做されます。"}
{"category":"closed_qa","context":"","instruction":"実行コンテキストとは？","response":"サポートされるそれぞれのプログラミング言語に対応するREPLの状態です。サポートされる言語はPython、R、Scala、SQLです。"}
{"category":"closed_qa","context":"","instruction":"エクスペリメントとは？","response":"機械学習モデル開発をトラッキングするメインの整理ユニットです。エクスペリメントは、モデルトレーニングコードの記録された個々のランを整理、表示、アクセスコントロールを行います。"}
{"category":"closed_qa","context":"","instruction":"特徴量ストアとは？","response":"集中管理された特徴量のリポジトリです。Databricks Feature Storeを用いることで、企業内での特徴量の共有、発見が可能となり、モデルトレーニングと推論で同じ特徴量計算コードを使用することを保証することもできます。"}
{"category":"closed_qa","context":"","instruction":"モデルとは？","response":"モデルレジストリに登録されたトレーニング済み機械学習モデル、ディープラーニングモデルです。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLとは？","response":"Databricks SQLは、SQLクエリーとBIツールを主に用いて作業するデータアナリスト向けのものです。お使いのデータレイクに格納されているデータに対してアドホックのクエリーを実行し、ダッシュボードを作成するための直感的な環境を提供します。このUIはデータサイエンス & データエンジニアリングやDatabricksマシンラーニング環境とは大きく異なります。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLのREST APIとは？","response":"REST API: Databricks SQLのオブジェクトに対するタスクを自動化できるインタフェースです。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLのSQLウェアハウスとは？","response":"SQLクエリーを実行する計算資源です。"}
{"category":"closed_qa","context":"","instruction":"Apache Airflowは何？","response":"Apache Airflowは、Pythonを用いてプログラム的に、データと機械学習パイプライン(Airflowの用語ではDAGと呼ばれます)を作成、スケジュール、モニタリングすることができる人気かつ拡張可能なプラットフォームです。Airflowには、データベースからクラウドストレージに至る全てとのやりとりを容易にするビルトインのオペレーターが多数含まれています。"}
{"category":"closed_qa","context":"","instruction":"Apache Airflowに対するDatabricksプロバイダーのパッケージ名は何？","response":"'apache-airflow-providers-databricks'です。"}
{"category":"closed_qa","context":"","instruction":"AirflowからDelta Live Tablesのパイプラインを起動できるか？","response":"はい、AirflowからDelta Live Tablesのパイプラインを起動できるようになりました。"}
{"category":"closed_qa","context":"","instruction":"MLflowモデルレジストリはどのような機能を提供していますか？","response":"モデルレジストリは以下の機能を提供します。\n\n時系列順のモデルのリネージュ(どの時点でどのMLflowエクスペリメントとランがモデルを生成したのか)。\nサーバレスリアルタイム推論、あるいはクラシックMLflowモデルサービングによるモデルサービング。\nモデルのバージョン管理。\nステージの移行(ステージングからプロダクション、アーカイブへなど)。\nレジストリのイベントに基づいて自動でアクションを起動するWebhook。\nモデルのイベントに対するメールの通知。\nまた、モデルの説明文を作成、参照したり、コメントすることができます。"}
{"category":"closed_qa","context":"","instruction":"モデルレジストリにモデルをどのように登録するか？","response":"モデルレジストリにモデルを登録する方法は2つあります。MLflowに記録された既存のモデルを登録するか、新規の空のモデルを作成、登録し、過去に記録したモデルを割り当てることができます。"}
{"category":"closed_qa","context":"","instruction":"新規登録モデルを作成し、記録されたモデルを割り当てる操作を教えてください。","response":"登録モデルページで新規の空のモデルを作成し、記録されたモデルを割り当てるためにCreate Modelボタンを使うことができます。"}
{"category":"closed_qa","context":"","instruction":"APIでどのようにモデルを登録するか？","response":"プログラムからモデルレジストリにモデルを登録する方法は3つあります。すべての方法はMLflowモデルレジストリによって管理されるセキュアな場所にモデルをコピーします。\nMLflowエクスペリメントの過程でモデルを記録し、指定された他名前で登録するには、mlflow.<model-flavor>.log_model(...)メソッドを使います。指定された名前の登録モデルが存在しない場合には、このメソッドは新規モデルを登録してバージョン1を作成し、ModelVersion MLflowオブジェクトを返却します。指定された名前の登録モデルがすでに存在しているのであれば、このメソッドは新規モデルバージョンを作成し、バージョンオブジェクトを返却します。\nすべてのエクスペリメントのランが完了した後に、指定した名前でモデルを登録し、どのモデルをレジストリに追加するのが最も適切であるのかを決定したのであれば、mlflow.register_model()メソッドを使います。このメソッドには、引数mlruns:URIにランIDを指定する必要があります。指定された名前の登録モデルが存在しない場合、このメソッドは新規モデルを登録してバージョン1を作成し、ModelVersionMLflowオブジェクトを返却します。すでに指定された名前のモデルが登録されているのであれば、このメソッドは新規モデルバージョンを作成し、新規バージョンオブジェクトを返却します。\n指定された名前で新たに登録モデルを作成するには、MLflow Client APIのcreate_registered_model()メソッドを使います。モデル名が存在する場合には、このメソッドはMLflowExceptionをスローします。\nまた、Databricks Terraformプロバイダーやdatabricks_mlflow_modelを用いてモデルを登録することができます。"}
{"category":"closed_qa","context":"","instruction":"モデルバージョンのステージはどんなもの？","response":"モデルバージョンは以下のステージのいずれかに存在することになります：None、Staging、Production、Archivedです。Stagingステージは、モデルのテストや検証を意味しますが、Productionステージはテストやレビュープロセスが完了したモデルバージョン向けのためのものであり、ライブなスコアリングのためのアプリケーションにデプロイされます。削除できると考えた時点でアーカイブされたモデルバージョンは、不活性状態と見なされます。バージョンが異なるモデルは異なるステージになることができます。"}
{"category":"closed_qa","context":"","instruction":"モデルのStagingステージにはどういう意味を持つか？","response":"Stagingステージは、モデルのテストや検証を意味します。"}
{"category":"closed_qa","context":"","instruction":"モデルのProductionステージにはどういう意味を持つか？","response":"Productionステージはテストやレビュープロセスが完了したモデルバージョン向けのためのものです。"}
{"category":"closed_qa","context":"","instruction":"モデルステージの移行の権限がない場合はどうすればいいですか？","response":"権限がない場合には、ステージ移行をリクエストし、モデルバージョンの移行できる権限を持つユーザーが、リクエストを承認、却下、キャンセルします。"}
{"category":"closed_qa","context":"","instruction":"一つのステージに登録モデルの複数のバージョンが存在することを許容しますか？","response":"はい、モデルレジストリでは、1つ以上のバージョンの登録モデルがそれぞれのステージに存在することを許容します。"}
{"category":"closed_qa","context":"","instruction":"プロダクションに1つのバージョンのみのモデルが存在する様にしたい場合にはどうすればいいですか？","response":"プロダクションに1つのバージョンのみのモデルが存在する様にしたい場合には、Transition existing Production model versions to Archivedをチェックすることで、現在プロダクションにあるすべてのモデルバージョンをアーカイブすることができます。"}
{"category":"closed_qa","context":"","instruction":"どうやってAPIによるモデルのステージ移行を行いますか？","response":"モデルバージョンを新たなステージにアップデートするには、MLflow Client APIのtransition_model_version_stage()メソッドを使います。"}
{"category":"closed_qa","context":"","instruction":"どのようにモデルを推論に使うか？","response":"モデルレジストリにモデルを登録した後に、バッチ推論やストリーミング推論のためのモデルを使用するノートブックを自動で生成することができます。あるいは、Databricksのサーバレスリアルタイム推論、あるいはクラシックMLflowモデルサービングを用いて、リアルタイムサービングにモデルを使用するエンドポイントを作成することができます。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesを用いたストリーミング推論をどのように設定するか？","response":"モデル名のフォルダーの配下のフォルダーDLT-Inference配下にノートブックが保存されます。\n\nStreaming (Delta Live Tables) タブをクリックします。\n\nModel versionドロップダウンから、使用するモデルバージョンを選択します。ドロップダウンの最初の2つの項目は、(存在する場合には)モデルの現在のプロダクションとステージングバージョンになります。これらの選択肢のいずれかを選択すると、ノートブックは自動でランの実行時のプロダクションとステージングバージョンを使用します。モデルの開発を進めている際にはこのノートブックをアップデートする必要はありません。\n\nInput tableの隣のBrowseボタンをクリックします。Select input dataダイアログが表示されます。必要に応じて、Computeドロップダウンでクラスターを変更することができます。\nモデルに対する入力データを含むテーブルを選択し、Selectをクリックします。生成されたノートブックは、ソースとしての入力テーブルを使用するデータ変換処理を作成し、モデル予測を実行するためにMLflowのPySpark推論UDFとインテグレーションします。モデルを適用する前後で追加の変換処理が必要であれば、生成されたノートブックを編集することができます。\n出力のDelta Live Table名を指定します。ノートブックは指定された名前でライブテーブルを作成し、モデル予測結果を格納するために使用します。必要に応じてターゲットデータセットをカスタマイズするために、生成されたノートブックを編集することができます。例えば、出力としてストリーミングライブテーブルを定義し、スキーマ情報やデータ品質制約を追加します。\nこのノートブックを用いて新たなDelta Live Tablesパイプラインを作成するか、追加のノートブックライブラリとして既存のライブラリに追加します。"}
{"category":"closed_qa","context":"","instruction":"どうやってモデルバージョンを比較するか？","response":"モデルレジストリでモデルバージョンを比較することができます。\n\n登録モデルページで、モデルバージョンの左のチェックボックスをクリックすることで2つ以上のモデルバージョンを選択します。\nCompareをクリックします。\n<N>バージョンを比較する画面が表示され、選択されたモデルバージョンのパラメーター、スキーマ、メトリクスを比較するテーブルが表示されます。画面の下には、プロットのタイプ(scatter、contour、parallel coordinates)とプロットするパラメーターやメトリクスを選択することができます。"}
{"category":"closed_qa","context":"","instruction":"指定した登録モデルとモデルバージョンにおけるアクティビティに関して通知設定できるか？","response":"はい、指定した登録モデルとモデルバージョンにおけるアクティビティに関して、メール通知を行う様にモデルレジストリを設定することができます。\n\n登録モデルページで、Notify me aboutメニューに3つのオプションが表示されます。\nAll new activity: このモデルのすべてのモデルバージョンのすべてのアクティビティについてメールの通知を送信します。登録モデルを作成した際、この設定がデフォルトになります。\nActivity on versions I follow: フォローしているモデルバージョン位関してのみメール通知を行います。この選択によって、フォローしているすべてのモデルバージョンの通知を受け取ります。特定のモデルバージョンの通知をオフにすることはできません。\nMute notifications: この登録モデルのアクティビティについてメール通知を送信しません。"}
{"category":"closed_qa","context":"","instruction":"モデル画面の設定できる通知はどんなイベントによって発信するか？","response":"以下のイベントによって通知が行われます。\n\n新規モデルバージョンの作成\nステージ移行のリクエスト\nステージ移行\n新規コメント\n以下のいずれかを行った際にはモデルの通知を自動で購読します。\n\nモデルバージョンへのコメント\nモデルバージョンのステージの移行\nモデルのステージの移行をリクエスト"}
{"category":"closed_qa","context":"","instruction":"モデルレジストリでのアクティビティ通知のメール送信の最大数は何？","response":"モデルレジストリは、アクティビティごとの日毎のユーザーに通知するメールの数に制限を設けています。例えば、ある登録モデルにおいて新規モデルバージョンに関するメールを1日に20回受け取った場合、モデルレジストリは1日の制限に達したことを告げるメールを送信し、次の日までイベントに関するメールを送信しません。\n\n許可するメールの数の制限を増やすには、Databricks担当者にお問い合わせください。"}
{"category":"closed_qa","context":"","instruction":"Webhookを用いることで何かできるか？","response":"Webhookを用いることで、お使いのインテグレーションが自動でアクションを起動できる様に、モデルレジストリのイベントをリッスンできるようになります。自動化や既存のCI/CDツールやワークフローとお使いの機械学習パイプライン解いて具レーションするためにwebhookを使うことができます。例えば、新規モデルバージョンが作成された際にCIのビルドを起動したり、モデルのプロダクションへの移行がリクエストされるたびにSlackを通じてチームメンバーに通知を行うことができます。"}
{"category":"closed_qa","context":"","instruction":"MLflowに登録しているモデルの名前を変更することができるか？","response":"はい、APIのみ対応できます。登録モデルの名称を変更するには、MLflow Client APIのrename_registered_model()メソッドを使います。"}
{"category":"closed_qa","context":"","instruction":"モデルを検索する際にどんな権限が必要ですか？","response":"モデルを検索する際には、少なくとも読み取り権限を持っているモデルのみが返却されます。"}
{"category":"closed_qa","context":"","instruction":"APIによるモデルの検索のメソッドを教えてください。","response":"すべての登録モデルの一覧を取得するには、MLflow Client APIのlist_registered_models()メソッドを使います。"}
{"category":"closed_qa","context":"","instruction":"ワークスペース間でどのようにモデルを共有するか？","response":"Databricksでは、複数ワークスペースでのモデル共有をサポートしています。例えば、お使いのワークスペースでモデルを開発・記録し、中央管理されているモデルレジストリに登録することができます。これは、複数のチームがモデルにアクセスする際に有用です。複数のワークスペースを作成し、これらの環境横断でモデルを利用、管理することができます。"}
{"category":"closed_qa","context":"","instruction":"ワークスペース間でどのようにMLflowオブジェクトをコピーするか？","response":"お使いのDatabricksワークスペース間でMLflowオブジェクトをインポート、エクスポートするために、ワークスペース間でMLflowエクスペリメント、モデル、ランを移行するためのコミュニティ主導のオープンソースプロジェクトであるMLflow Export-Importを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"'MLflow Export-Import'ツールを使って何かできるか？","response":"'MLflow Export-Import' ツールを使うことで、以下のことが可能となります。\n\n同一あるいは異なるトラッキングサーバーにいる他のデータサイエンティストとの共有、コラボレーション。例えば、他のユーザーからのエクスペリメントを、あなたのワークスペースにコピーすることができます。\n開発ワークスペースからプロダクションワークスペースの様に、別のワークスペースへのモデルのコピー。\nお使いのローカルトラッキングサーバーからMLflowエクスペリメントやランを、Databricksワークスペースにコピー。\nミッションクリティカルなエクスペリメントやモデルを別のDatabricksワークスペースにバックアップ。"}
{"category":"closed_qa","context":"","instruction":"AWS上でDatabricksのワークスペースをデプロイする方法を教えてください。","response":"AWS上でDatabricksのワークスペースをデプロイするには、以下の2つの方法があります。\n\nAWSリソースを自動生成(背後ではCloudFormationを使用)し、ワークスペースを自動でデプロイする(メニューではCreate Workpace > Quickstart)\nAWSリソースをユーザー側でマニュアルで準備して、それらを用いてワークスペースをデプロイする(メニューではCreate Workpace > Custom AWS configuration)"}
{"category":"closed_qa","context":"","instruction":"Information Schemaって何？","response":"INFORMATION_SCHEMA は SQL 標準ベースのスキーマであり、Unity カタログで作成されたすべてのカタログで提供されます。"}
{"category":"closed_qa","context":"","instruction":"Information Schemaでどんな事ができるか？","response":"1. 全てのカタログ、スキーマ（データベース）とテーブル一覧の収集\n2. テーブルの権限確認\n3.最近1時間以内に作成されたテーブル情報\n4.　カラム数の多いテーブルを多い順にリストする。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesを用いてどのようなデータの変換処理ができるか？","response":"Delta Live Tablesは高信頼、維持可能、テスト可能なデータ処理パイプラインを構築するためのフレームワークです。お手元のデータに対して実行する変換処理を定義すると、Delta Live Tablesはタスクー消すとレーション、クラスター管理、モニタリング、データ品質、エラーハンドリングを管理します。Delta Live Tablesを用いてすべてのデータ処理ワークフローを構築することもできますし、複雑なデータ処理ワークフローをオーケストレーションするために、Databricksジョブのワークフローの中にパイプラインをインテグレーションすることもできます。"}
{"category":"closed_qa","context":"","instruction":"Apache Sparkは？","response":"Apache Sparkは、Databricksレイクハウスプラットフォームの心臓であり、プラットフォームの計算クラスターとSQLウェアハウスを強化するテクノロジーです。DatabricksはApache Sparkに最適化されたプラットフォームであり、Apache Sparkのワークロードを実行するための効率的かつシンプルなプラットフォームを提供します。"}
{"category":"closed_qa","context":"","instruction":"Apache SparkとDatabricksはどのような関係ですか？","response":"Databricksという会社はApache Sparkのオリジナルクリエイターによって創業されました。Apache SparkはオープンソースソフトウェアプロジェクトとしてDatabricksを含む数多くのトップカンパニーからのコミッターを有しています。\n\nDatabricksでは、Apache Sparkの開発と機能のリリースを継続しています。Databricksランタイムには、更なる最適化とApache Sparkをベースとした専用の機能や、C++で再構築された最適化バージョンのApache SparkであるPhotonを含む拡張版Apache Sparkが含まれています。"}
{"category":"closed_qa","context":"","instruction":"Apache SparkはDatabricks上でどのように動作するのですか？","response":"Databricks上でコンピュートクラスターやSQLウェアハウスをデプロイする際には常に、仮想マシンにApache Sparkが設定されデプロイされます。Databricksによって管理されているので、SparkコンテキストやSparkセッションの設定や初期化を機にする必要はありません。"}
{"category":"closed_qa","context":"","instruction":"Apache Sparkを使うことなしにDatabricksを使うことはできますか？","response":"Databricksでは数多くのワークロードをサポートしており、Databricksランタイム内には多数のオープンソースライブラリが含まれています。Databrikcs SQLは内部ではApache Sparkを使用していますが、エンドユーザーはデータベースオブジェクトにクエリーするために標準的なSQL構文を使用します。\n\nDatabricks機械学習ランタイムはMLのワークロードに最適化されており、多くのデータサイエンティストがDatabricksで作業する際に、TensorFlow やSciKit Learnのような主要なオープンソースライブラリを使用しています。Databricksによってデプロイ、管理されている計算リソースに対して任意のワークロードをスケジュールするためにワークフローを活用することができます。"}
{"category":"closed_qa","context":"","instruction":"なぜDatabricksではApache Sparkを使っているのですか？","response":"Databricksレイクハウスプラットフォームは、皆さまのビジネスをスケールさせるエンタープライズソリューションを開発、デプロイするためのセキュアかつコラボレーティブな環境を提供します。Databricksの従業員の多くは、Apache Sparkについて最も知識を持っているメンテナーであり、世界中のユーザーでもあります。我々はユーザーの皆様がApache Sparkが稼働する最速の環境を利用できる様に、継続的に新たな最適化を開発し、リリースし続けています。"}
{"category":"closed_qa","context":"","instruction":"Azure Databricks にて、Azure Storage（Azure Data Lake Storage Gen2）に SAS キーで接続する際 400 エラーへの対応方法は何？","response":"SASキーをコピー＆ペーストすると、最初の ? が不要なことが原因でした。SASキーの最初の ? を削除します。"}
{"category":"closed_qa","context":"","instruction":"変数値をストレージ上に区切りテキスト（CSV・TSV等）として保存する方法を教えてください。","response":"変数に含まれる最初と最後の改行を削除してから、dbutils.fs.put により書き込ます。Synapse Spark pool で実施する場合には、mssparkutils.fs.put で同様のことが実施できるはずです。"}
{"category":"closed_qa","context":"","instruction":"データウェアハウスにおけるデータモデリングの重要性を教えてください。","response":"データモデルはデータウェアハウス構築におけるフロントと中心を担っています。通常、このプロセスはセマンティックなビジネス情報モデルからスタートし、論理データモデル、そして最後には物理データモデル(PDM)となります。すべては、ビジネス情報モデルとプロセスフローが最初に作成され、企業内のビジネスプロセスと同じ様に、キーとなるビジネスエンティティ、属性、それらのインタラクションがキャプチャされる適切なシステム分析と設計フェーズからスタートします。そして、エンティティが互いにどの様に関係するのかを示す論理データモデルが作成されますが、これは特定の技術に依存しないモデルとなります。最後に、背後の技術プラットフォームに基づいて、データの読み書きが効率的に行われることを保証するために、PDMが作成されます。我々はみんな知っていますが、データウェアハウスにおいては、スタースキーマやデータボルトのような分析フレンドリーなモデリングスタイルが非常に人気があります。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおける物理データモデル作成のベストプラクティスを教えてください。","response":"定義されたビジネス問題に基づき、データモデル設計の狙いは再利用可能性、柔軟性、スケーラビリティにおいて容易な方法でデータを表現することとなります。以下に、それぞれのトランザクションを保持するSalesファクトテーブルと、データをスライスアンドダイスするための顧客、商品、店舗、日付のような様々なディメンションテーブルを示す典型的なスタースキーマデータモデルを示します。特定の月において最も人気のある商品は何か、ある四半期で最もパフォーマンスが優れている店舗はどれかと言う様な特定のビジネス上の質問に答えるために、ディメンションがファクトテーブルとjoinすることができます。これらがDatabricksでどの様で実装されているのかを見ていきましょう。"}
{"category":"closed_qa","context":"","instruction":"データモデルを作成する際には、主キーと外部キーの定義をどのようにすればいいですか？","response":"PK/FK定義をサポートする能力を持つことで、Databricksにおけるデータモデルの定義が非常に簡単になります。また、アナリストが効率的にクエリーを記述するために、Databricks SQLウェアハウスでjoinの関係をクイックに明らかにする役にも立ちます。多くの他のMassively Parallel Processing (MPP)、EDW、クラウドデータウェアハウスのように、PK/FK製薬は情報を提供するためだけのものです。Databricksでは、PK/FKリレーションシップの矯正をサポートしていませんが、セマンティックデータモデルの設計を容易にするために定義することが可能です。"}
{"category":"closed_qa","context":"","instruction":"アイデンティティカラムは何？","response":"アイデンティティカラムは、データの新規の行のそれぞれにユニークなID番号を自動で生成するデータセット内のカラムです。通常これらはデータウェアハウスでサロゲートキーとして使用されます。サロゲートキーは、行のユニーク性を特定するために、自然な主キーや複数フィールドを結合したものに頼ることがない様に、システムが生成する無意味なキーです。通常、これらのサロゲートキーは、データウェアハウスで主キーや外部キーとして使用されます。"}
{"category":"closed_qa","context":"","instruction":"データ品質のためのカラム制約のサポートを教えてください。","response":"主キーや外部キーの情報提供を目的とした制約に加え、Databricksではテーブルに追加されるデータの品質や統一性を保証するために強制される、カラムレベルのデータ品質チェック制約もサポートしています。これらの制約は自動で検証されます。これらの良い例はNOT NULL制約とカラム値の制約です。他のクラウドデータウェアハウスと異なり、特定のカラムのデータ品質を保証するために、カラム値チェックよりも先に進んだ機能を提供しています。以下に示している様に、valid_sales_amountチェック制約は、テーブルにデータを追加する前にすべての既存の行が制約(例: sales amount > 0)を満たしていることを検証します。詳細な情報はこちらから確認できます。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesでパイプラインの依存関係について教えてください。","response":"Delta Live Tablesはパイプラインにおける依存関係をサポートしています。例えば、%pip installコマンドを用いて任意のPythonパッケージをインストールすることができます。また、Delta Live Tablesはグローバルあるいはクラスタースコープのinit scriptをサポートしています。しかし、これらのパイプラインにおける外部依存関係、特にinit scriptはランタイムアップグレード時のリスクを増加させます。これらのリスクを軽減するために、パイプラインにおけるinit scriptの使用は最小限に留めてください。処理でinit scriptが必要な場合には、早い段階で問題を検知するためにパイプラインのテストを自動化してください。次のランタイムバージョンを用いたパイプラインのテストを自動化するをご覧下さい。init scriptを使う際には、テストの頻度を高めることをお勧めします。"}
{"category":"closed_qa","context":"","instruction":"Databricks SQLのクエリーフェデレーションとは？","response":"Databricks SQLを用いることで、すべてのサーバレスウェアハウス、pro SQLウェアハウスに含まれているドライバーにより人気のデータベースソリューションに対する読み取り専用接続を設定することができます。"}
{"category":"closed_qa","context":"","instruction":"クエリーフェデレーションはレイクハウスのどこにフィットするのか？","response":"レイクハウスでは、データの冗長性や分断を削減するためにデータを集中して格納することを強調していますが、多くの企業においてはプロダクションのデータシステムを多数持っています。多くの理由から接続されたシステムのデータをクエリーしたいと考えるかもしれません。\n\nアドホックのレポート。\nPOCの作業。\n新たなETLパイプラインやレポートの開発。\nインクリメンタルな移行におけるワークロードのサポート。\nDatabricksにいくつかのデータセットを移行したり、取り込んだりしないという選択をしつつも、隔離されたユースケースにおいて何人かのユーザーにアクセスを与える必要がある場合があります。"}
{"category":"closed_qa","context":"","instruction":"Databricksにおけるクエリーフェデレーションとは？","response":"Apache Sparkは、さまざまなシステムやデータソースから得られるさまざまなフォーマットのデータに接続するサポートを常に提供しています。数多くの外部データソースとインテグレーションするためにDatabricksランタイムはこれらのオープンソースコネクター上に構築されており、追加のライブラリをバンドルしています。\n\n多くのデータベースへの接続ではApache SparkのJDBCコネクターを使用しています。これらの接続の並列性をチューニングするために数多くのオプションを指定することができ、必要に応じてクエリーをソースシステムにプッシュダウンすることができます。\n\nDatabricksランタイム11.3以降では、PythonやR、Scalaに加えてSQLでもシークレットがサポートされ、検閲された文字列を用いることでユーザースコープの認証情報を設定することができます。"}
{"category":"closed_qa","context":"","instruction":"Databricksは他のシステムからのクエリーフェデレーションを許可するのか？","response":"Databricksでは数多くのBIツールと互換性のあるJDBCやODBCドライバーを提供しています。\nDelta Sharingは数多くのクライアントから接続するユーザーにDelta Lakeテーブルを共有するためのオープンソースプロトコルを提供しています。\nDelta Lakeは数多くのインテグレーションを提供する完全にオープンソースなストレージプロトコルです。\nDatabricksは、レイクハウスのデータのクエリーをサポートするために数多くのBIとビジュアライゼーションツールとパートナーシップを結んでいます。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tablesを用いてより簡単にデータパイプラインを構築、維持するか？","response":"DLTがデータエンジニアの負荷を削減し、洞察をより迅速に得られる様にする特定の方法として3つのポイントをハイライトしたいと思います。\n\n自動化されたインフラストラクチャの管理: タスクオーケストレーション、エラーハンドリング、パフォーマンス最適化のような複雑かつ時間がかかるアクティビティを自動化することでオーバーヘッドを除去。\nストリーミングとバッチをシンプルに: 別々のパイプラインを構築する必要なしに、ストリーミングとバッチを統合し、順序を守らないデータを取り扱うためにAuto Loaderとインテグレーションし、データが到着するとすぐに利用可能に。\nデータ構造変更の自動ハンドリング: レコードの変更と部分的アップデートを組み合わせ、単一かつ完全、そして最新のレコードに。"}
{"category":"closed_qa","context":"","instruction":"インクリメンタルなデータ取り込みにはどういう機能を使うか？","response":"インクリメンタルなデータ取り込みには、Delta Live TablesでAuto Loaderを使用することをお勧めします。Delta Live TablesはApache Sparkの構造化ストリーミングを拡張し、数行の宣言型PythonあるいはSQLを記述することで、以下の機能を持つプロダクション品質のデータパイプラインをデプロイすることができます。\n\nコスト削減のためにオートスケーリングする計算インフラストラクチャ\nエクスペクテーションによるデータ品質チェック\nスキーマ進化の自動ハンドリング\nイベントログのメトリクスを通じたモニタリング"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderによって検知されたファイルへどのようなクエリーを行うか？","response":"Auto Loaderは、ストリームの状態を調査するためのSQL APIを提供しています。cloud_files_state関数を用いることで、Auto Loaderのストリームによって検知されたファイルに関するメタデータを取得することができます。Auto Loaderのストリームに関連づけられたチェックポイントのロケーションを指定して、シンプルにcloud_files_stateにクエリーを行います。"}
{"category":"closed_qa","context":"","instruction":"ストリームアップデートのリッスンをどのように行うか？","response":"Apache SparkのStreaming Query Listenerインタフェースを使うことをお勧めします。\n\nAuto Loaderはそれぞれのバッチにおいて、Streaming Query Listenerにメトリクスをレポートします。ストリーミングクエリーの進捗ダッシュボードのRaw Dataタブの下のnumFilesOutstandingとnumBytesOutstandingメトリクスでバックログにどれだけのファイルが存在するのか、バックログがどれだけ大きいのかを確認することができます。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderを実行する際のコストの主要要素は何？","response":"Auto Loaderを実行する際、コストの主要因となるのは計算資源とファイル検知のコストになるでしょう。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderの計算コストをどのように削減するか？","response":"計算コストを削減するためには、低レーテンシーの要件がない限り、連続的に稼働させるのではなく、Trigger.AvailableNow(Databricksランタイム10.1以降)やTrigger.Onceを用いたバッチジョブとしてAuto LoaderをスケジュールするためにDatabricksジョブを使用することをお勧めします。\n\nファイル検知は、ディレクトリ一覧モードでお使いのストレージアカウントに対するLISTオペレーションの実行、サブスクリプションサービスに対するAPIリクエスト、ファイル通知モードのキューサービスによってコストを発生させることがあります。以下のことをお勧めします。\n\nディレクトリ一覧モードでAuto Loaderを継続的に実行する際にはProcessingTimeトリガーを指定します。\n可能であればインクリメンタルな一覧を活用できる様に、語順でファイルをストレージアカウントにアップロードする様に設計します。\n特に深くネストされたディレクトリに対しては、Databricksランタイム9.0やディレクトリ一覧モードを使用します。\nインクリメンタル一覧が利用できない場合には、ファイル通知を利用します。\nコストを追跡するために、Auto Loaderによって作成されたリソースをタグ付けするためにリソースタグを使用します。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderをスケジュールする時にTrigger.AvailableNowを設定すると何かできるか？","response":"Trigger.AvailableNowを用いることで、バッチジョブとしてDatabricksジョブの中でAuto Loaderを実行する様にスケジュールすることができます。AvailableNowトリガーは、クエリー開始時間の前に到着したすべてのファイルを処理する様にAuto Loaderに指示します。ストリームが起動した後にアップロードされた新規ファイルは次のトリガーまで無視されます。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderで複数のマイクロバッチでデータを処理することができるか？","response":"Trigger.AvailableNowを用いることで、データ処理とは非同期にファイル検知が行われ、レート制限を用いた複数のマイクロバッチでデータを処理することができます。デフォルトではAuto Loaderはマイクロバッチごとに最大1000ファイルを処理します。マイクロバッチでどれだけのファイル数やバイト数を処理すべきかを設定するために、cloudFiles.maxFilesPerTriggerやcloudFiles.maxBytesPerTriggerを設定することができます。ファイルの制限はハードリミットですが、バイトの制限はソフトリミットであり、指定されたmaxBytesPerTriggerよりも多いバイト数が処理される場合があることを意味します。両方のオプションが指定された場合には、Auto Loaderはいずれかの制限に達するまでに可能な限り多くのファイルを処理します。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderは確実に一度(exactly-once)の取り込みの保証をすることができるか？","response":"Auto Loaderは確実に一度(exactly-once)の取り込みの保証をするために、RocksDBを用いたチェックポイントの場所に検知したファイルの履歴を保持します。ボリュームの大きいデータセットにおいては、ストレージコストとAuto Loader起動時間を削減するために、チェックポイントの場所からイベントを廃棄するためにcloudFiles.maxFileAgeオプションを使用することができます。cloudFiles.maxFileAgeに設定できる最小値は\"14 days\"です。RocksDBにおける削除はトゥームストーンのエントリーとして表示されるので、安定する前にイベントが廃棄されるので、一時的にストレージの利用量が一時的に増加する場合があります。"}
{"category":"closed_qa","context":"","instruction":"PythonとSQLのセルをどの様にフォーマットするのか\n","response":"コードをフォーマットするにはノートブックのCan Edit権限が必要となります。\n\n以下の方法で、フォーマッターを起動することができます。\n\n単一セルのフォーマット\n\nキーボードショートカット：Cmd+Shift+Fを押します。\nコマンドコンテキストメニュー\nSQLセルのフォーマット：SQLセルのコマンドコンテキストドロップダウンメニューのFormat SQLを選択します。このメニュー項目はSQLノートブックセルか%sql言語マジックを持つセルでのみ表示されます。\nPythonセルのフォーマット：PythonセルのコマンドコンテキストドロップダウンメニューのFormat Pythonを選択します。このメニュー項目はPythonノートブックセルか%python言語マジックを持つセルでのみ表示されます。\n複数セルのフォーマット\n\n複数のセルを選択し、Edit > Format Cell(s) を選択します。1つ以上の言語のセルを選択した場合には、PythonとSQLのセルがフォーマットされます。これには%sqlや%pythonを使用しているものも含まれます。\n\nノートブック内のすべてのPythonとSQLセルのフォーマット\n\nEdit > Format Notebookを選択します。1つ以上の言語のセルが含まれている場合には、PythonとSQLのセルがフォーマットされます。これには%sqlや%pythonを使用しているものも含まれます。\n\n"}
{"category":"closed_qa","context":"","instruction":"Pythonセルのフォーマットの制限を教えてください。","response":"ノートブック内のコードをフォーマットするためにBlackを使用します。\nBlackは4スペースのインデントに対してPEP 8を強制します。インデントの設定はできません。\nSQL UDFの中に埋め込まれたPython文字列のフォーマットはサポートされていません。同様に、Python UDF内のSQL文字列のフォーマットもサポートされていません。"}
{"category":"closed_qa","context":"","instruction":"毎回のクラスター起動時に大量のライブラリーなどをインストールしていては起動時間が遅くなってしまっているので、改善方法を教えてください。","response":"DatabricksではRuntime環境をカスタマイズして保存しておくことができます。これがDatabricks Container Serviceとなります。Databricksのクラスターインスタンス上ではDocker　Containerが動作するため、それを予め作成しておくことで起動時に読み込む事ができます。"}
{"category":"closed_qa","context":"","instruction":"コンテナのベース イメージはどこにホストされていますか？","response":"コンテナのベース イメージは、https://hub.docker.com/u/databricksruntime の Docker Hub でホストされています。 これらのベースの生成に使用される Dockerfile は https://github.com/databricks/containers にありますのでベースの中身が知りたい方はご確認ください。"}
{"category":"closed_qa","context":"","instruction":"DBUとは？","response":"Databricksユニット(DBU)はプラットフォームにおけるコンサンプションの内部単位です。SQLウェアハウスを除き(SQLウェアハウスは基本的にクラスターのグループであり、DBUのレートはウェアハウスを構成するクラスターのDBUレートの合計となります)、消費されるDBUの量は、それぞれのクラスターのノードの数と内部のVMのインスタンスタイプの計算能力に基づくものとなります。"}
{"category":"closed_qa","context":"","instruction":"クラスターポリシーとは？","response":"クラスターポリシーを用いることで、管理者は新規クラスターを作成する際に利用できる設定をコントロールすることができ、これらのポリシーを個別のユーザーやユーザーグループに割り当てることができます。デフォルトでは、すべてのユーザーにはワークスペースにおける「制限なしのクラスター作成」権限が付与されています。これは、割り当てられたポリシーの範囲外で制限なしにクラスターを作成できてしまい、管理されないままコストを垂れ流すことになるので滅多なことで使うべきではありません。"}
{"category":"closed_qa","context":"","instruction":"Photonは？","response":"Photonは、低コストで超高速クエリーパフォーマンスを提供するDatabricksレイクハウスプラットフォームの次世代エンジンです。"}
{"category":"closed_qa","context":"","instruction":"データ品質とは？","response":"データ品質とは、業務の現実を適切に反映されたデータを利用者のニーズを満たす程度。複数の定義があるが、データの利用時の要求を満たすことができるかが共通の観点のようである。"}
{"category":"closed_qa","context":"","instruction":"データ品質の方法論について教えてください。","response":"データ品質の方法論を、チーム、ドメイン、グローバルなどの組織の規模に関わらず、次のように実施。\n\nデータ品質方針の策定を行い、優先順位が高いデータに対してデータ品質保証を、データ品質保証をプロジェクトとしてではなく、継続的な業務として実施する。\nデータ品質SLAやデータの認証を定めることで、組織内でのデータ品質に対するコンセンサスを得て、3M（ムリ・ムダ・ムラ）を取り除きながら、データオーナーと共にデータ品質保証を依頼する。\n組織のあるべき姿を成熟度モデル評価軸に落とし込み、その成熟度モデルに照らし合わせながら組織の改革改善に取り組む。"}
{"category":"closed_qa","context":"","instruction":"PySpark(Python) によるデータ品質処理方法は何？","response":"Spark(+Delta Lake)にてデータ品質保証を行うには、次のような方法がある。\n\nDelta Lake の機能を利用する方法\nPySpark によるデータ品質処理を実施する方法\nPython ライブラリを利用する方法\nSpark プロバイダーの機能を利用する方法"}
{"category":"closed_qa","context":"","instruction":"データエンジニアリング時にデータ品質要件を満たさないデータが含まれる場合には、どんな対応方法がありますか？","response":"データエンジニアリング時にデータ品質要件を満たさないデータが含まれる場合には、次の対応方法がある。データエンジニアリング後のデータを利用するシステム（ダウンストリーム）へのデータ提供が実施されないような仕組みが必要。\n\n無効なレコードを保持\n無効なレコードを分割\nエラー終了"}
{"category":"closed_qa","context":"","instruction":"Python ライブラリを用いてデータ品質要件を満たさないデータが含まれる場合の対応方法を教えてください。","response":"Python ライブラリを用いる方法では、下記のライブラリがある：Deequ、PyDeequ、pandera。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tableを用いたデータ品質処理を教えてください。","response":"Databicks の Delta Live Table における expectations（データ品質チェック）機能を利用してデータ品質処理できます。"}
{"category":"closed_qa","context":"","instruction":"CI/CD とは？","response":" CI/CD（Continuous Integration（継続的インテグレーション）/ Continuous Delivery（継続的デリバリー））は、コード開発から本番環境への統合までの品質の保証と期間の最小化を担保しつつ、変更点の統合を行いながらシステム運用を行うプロセスである。"}
{"category":"closed_qa","context":"","instruction":"CI/CD はどのようなプロセスを実施するか？","response":"CI/CD は次のようなプロセスを実施することが多い。CI にてリリース可能なリソース（一般的には main ブランチのコード）の準備を行い、 CD にて他リソースと統合した本番相当の環境で検証を行ったうえで本番環境へリリースする。リソースの管理は Git で行い、 Git の運用ルールは Git-flow と呼ぶ。Git-flow はチームのスキルセットに応じて方針を定める必要があり、Git の機能を多用するような複雑な設計を行うと運用コストが高くなる可能性がある。継続的な開発と運用を行えるように、シンプルな設計が望ましい。CI/CD の処理群を CI/CD パイプラインと呼び、Github Actions や Azure Pipelines などのサービスで実行する。\n\n"}
{"category":"closed_qa","context":"","instruction":"Code や Build では、開発支援の観点、及び、コードの品質管理の観点により、どのような項目の実施が求められるか？","response":"Code や Build では、開発支援の観点、及び、コードの品質管理の観点により、次のような項目の実施が求められる。\n\nLinter ツールによる静的解析\nFormatter ツールによるコード整形\nテスト結果の取得、及び、テスト結果の継続的な管理\nテストのコードカバレッジの取得、及び、テストのコードカバレッジの継続的な管理"}
{"category":"closed_qa","context":"","instruction":"Azure Pipelines とは？","response":"Azure Pipelines とは、Azure DevOps の１機能であり、 CI/CD（Continuous Integration（継続的インテグレーション）/ Continuous Delivery（継続的デリバリー））を自動化させる pipeline を構築できるサービスである。pipeline は、stage -> job -> task の階層構造となってり、Trigger により実行され、 Stage により論理的境界線を定義し、 job にエージェントを紐づけて task で指定した処理を行う。"}
{"category":"closed_qa","context":"","instruction":"単体（ユニット）テストとは？","response":"単体（ユニット）テストとは、ソフトウェアを構成する小さな単位（ユニット）に対して詳細設計の仕様の充足度を確認するテストです。他システムや他モジュールに依存しないようにテストを実施することが多いが、Databricks や Synapse などの多機能サービス上でのソフトウェア開発ではスコープが広範囲となる特徴がある。"}
{"category":"closed_qa","context":"","instruction":"Databricks における単体テストの方針を教えてください。","response":"Databricks Repos のファイル管理仕様、Databricks Workspace 上ではノートブックであるが Git 上では Python ファイルとして管理される仕様、を活用して、次のような単体テストの実施方針を定めた。ノートブック形式で pytest を実行する方法、および、ノートブック形式でコードカバレッジのファイルを出力する方法を、確立できていないため、pytest と unittest を併用している。CI/CD 時には、主に pytest にてテストを行うが、ノートブック形式での検証を行うために unittest においてもテストを行う。"}
{"category":"closed_qa","context":"","instruction":"レイテンシーに応じたデータ処理方法の選択肢は何？","response":"レイテンシーに応じた処理方法は以下の３つあります：バッチ、準リアルタイムとイベント駆動、リアルタイム"}
{"category":"closed_qa","context":"","instruction":"リアルタイム処理をどのように実装するか？","response":"リアルタイム処理を実装する際には、システム間を直接連携（例： Kafka -> Spark ）させるのではなく、ストレージに対するオートローダーによる処理（例: Kafka -> ストレージ -> Databricks ）がコストや可用性の観点で有効な場合もある。"}
{"category":"closed_qa","context":"","instruction":"メダリオンアーキテクチャとは？","response":"メダリオンアーキテクチャとはソースシステムのデータを、Bronze、Silver、Goldの三層で管理する手法であり、それをベースにデータレイヤーの設計を行うべき。Bronzeは生データを保持するレイヤー。Silverはソースシステムと同様の粒度で検証済みのデータを保持するレイヤー。Goldは集計したデータを保持するレイヤー。"}
{"category":"closed_qa","context":"","instruction":"Bronze にどのような特徴を持つか？","response":"Bronze の特徴は以下です。\n\n取り込んだ生データのコピーを、履歴として保持。\nデータを削除する場合には、物理削除ではなく、論理削除が推奨。\nスキーマ展開を許可するなどソースシステム側の変更対応を容易化。\nデータ型を文字型として保持するなどシステムエラーの発生を低減。"}
{"category":"closed_qa","context":"","instruction":"Silver にどのような特徴を持つか？","response":"Silver の特徴は以下です。\n\nBronze のデータに基づき、ソースシステムと同等のデータ粒度で保持。\nスキーマを適用し、dropDuplicates 関数を利用した重複排除等によるデータ品質チェック処理を実施。"}
{"category":"closed_qa","context":"","instruction":"Gold にどのような特徴を持つか？","response":"Gold の特徴は以下です。\n\nデータ利活用（消費）の目的に合致するように編成・集計したデータを保持。\nACLや行レベルセキュリティ等のデータアクセス制御を考慮することが多い。"}
{"category":"closed_qa","context":"","instruction":"PySpark(Spark SQL)、pandas-on-Spark、Pandas の利用指針を教えてください。","response":"PySpark(Spark SQL)、pandas-on-Spark、Pandas の順での利用が推奨。PySpark -> pandas-on-Spark -> PySpark の順で利用する場合には性能に課題をかかえることがある。\n\nExcel や SAS などのファイルを読み込む際には、Pandas で読み込み、PySpark に変換することで追加のコンポーネントが必要なくなる場合があり、データ量を想定した上で対応方針を検討する。"}
{"category":"closed_qa","context":"","instruction":"Spark テーブルにおける外部テーブルとマネージドテーブルの利用指針は何？","response":"Spark テーブルには、外部テーブル（アンマネージドテーブル）とマネージドテーブルがあるが、基本的には外部テーブルを利用すること。\n\nテスト実行時や短期的な利用を目的とする場合には、マネージドテーブルを利用してもよい。マネージドテーブルのロケーションを参照した外部テーブルを作成することも可能。\n\nマネージドテーブルを削除する際には、データサイズによっては事前にデータを削除する必要あり。"}
{"category":"closed_qa","context":"","instruction":"シークレットの利用をどうすればいいですか？","response":"Spark を利用する際には、パスワード等のシークレットが平文で表示されないように、Spark プロバイダーが提供するシークレット管理機能を用いる必要がある。"}
{"category":"closed_qa","context":"","instruction":"Spark におけるタイムスタンプ型処理の注意事項があるか？","response":"Spark におけるタイムスタンプ型には、次の注意事項がある。\n\nタイムゾーンのオフセットは、Spark で処理する際に損失し、Spark セッションのタイムゾーンに統一される。\nデータ取得元のタイムゾーンを保持する場合には、別カラムにタイムゾーンの値を保持する方法、あるいは、タイムゾーンのオフセットを加減した値を保持する方法のいずれかを実施する必要がある。\nSpark ではTIMESTAMP WITH SESSION TIME ZONEの timestamp 型のみがサポートさていることから、後者の方法を採用する場合にはspark.sql.session.timeZoneの固定化が必要となる。"}
{"category":"closed_qa","context":"","instruction":"破損したファイル（corrupt files）を参照する場合の対応方法を教えてください。","response":"CSV や JSON などのファイルを適切に読めこめないことをファイル破損（corrupt files）と言われており、PySpark では次の方法により破損理由を探すことができる。\n\nファイル内容を確認する方法\n\nデータ型が適切であるかを確認する方法"}
{"category":"closed_qa","context":"","instruction":"Delta Lake におけるタイムトラベル機能はどのような機能ですか？","response":"Delta Lake形式のテーブルでは、parquet ファイルが追記される仕様であることから、過去の時点のバージョンへのクエリ（タイムトラベル）が可能。"}
{"category":"closed_qa","context":"","instruction":"SparkSession の並列化処理時の注意事項を教えてください。","response":"定義したSparkSessionを multiprocessing などの並列化ライブラリーで共有すると正常に動作しないことがあるため、処理ごとにSparkSessionの定義が必要となる。\n\nDatabricks ではdbutils.notebook.run()メソッドの利用が推奨されているなど、Spark プロバイダーごとに対応方法が異なる。"}
{"category":"closed_qa","context":"","instruction":"Spark table の永続化はどうすればいいですか？","response":"Spark tableは、メタストアに格納されることで、セッション終了後にも利用すること（テーブルの永続化）ができる。\n\nメタストアとして、 Hive メタストアが利用されることが多いが、Spark プロバイダー固有のメタストアが提供されていることもある。"}
{"category":"closed_qa","context":"","instruction":"データレイクのテーブルとデータの分離はどういう意味ですか？","response":"データレイクを理解するために、テーブルとデータが分離されているという概念を理解することが重要である。リレーショナルデータベース（RDB）ではテーブル = データとなっていることが多く、RDBに慣れている方はテーブルを管理すればデータも管理できると考える傾向にある。データレイクでは、１つのデータ（ディレクトリ）に複数のテーブルが紐づくことから、テーブルで管理する方法では重複操作が実施される可能性がある。管理単位を、テーブルとして捉えるのではなく、ディレクトリとして捉える必要がある。"}
{"category":"closed_qa","context":"","instruction":"'Dataframe in, Dataframe out'は何？","response":"Spark のデータエンジニアリングを実施する際には、Dataframe in, Dataframe out、つまり、データフレームを引数としてデータフレームをリターンとするメソッドを組み合わせて開発を行うことが推奨されている。"}
{"category":"closed_qa","context":"","instruction":"Spark Dataframeの不変性は？","response":"PySpark における Spark データフレームは不変であるという特徴があり、処理の定義をリスト型変数や辞書型変数に格納できる。１つの変数に格納することで、開発時のデバックの実施が容易となる。"}
{"category":"closed_qa","context":"","instruction":"Spark SQLにてPython変数をどのように利用するか？","response":"spark.conf 経由で変数を渡すことが可能。"}
{"category":"closed_qa","context":"","instruction":"他ファイルで定義したクラスや関数の整理をどうすればいいですか？","response":"ノートブック型の開発環境では%runコマンドにより他ファイルで定義したクラスや関数を読み込むことができるが、関数をクラスのstaticmethod （暗黙の第一引数を受け取らないメソッド）として定義することで管理が容易となる。"}
{"category":"closed_qa","context":"","instruction":"変数からデータフレームを作成する方法は何？","response":"変数からデータフレームを作成する主な方法としては、次の３種類がある。単体テストのデータを用意する際などに利用する。\n\n辞書型リストを用いる方法\n多次元リストを用いる方法\npyspark.sql.Rowメソッドを用いる方法"}
{"category":"closed_qa","context":"","instruction":"Spark SQL を PySpark で利用する方法は何？","response":"PySparkでは主にpyspark.sql.functionsを利用するのですが、次のメソッドでは Spark SQL(Spark SQL, Built-in Functions)が利用可能。\n\nspark.sql\nselectExpr\nexpr\n次のメソッドでは、SQL とほぼ同じ記法が利用可能。\n\nfilter\nデータフレームに別名（alias）を設定する際には次のメソッドを利用\n\nalias"}
{"category":"closed_qa","context":"","instruction":"Bloomフィルターは何？","response":"Bloomフィルターは、偽陽性の確率を用いてセットにキーがあるかどうか、セットにアイテムが あるかもしれない ことを教えてくれます。インデックスで使用すると、Bloomフィルターは他のテクニックでは高速化できない様なフィールドに対して「藁山から針を探す」ようなクエリーの高速化に役立ちます。"}
{"category":"closed_qa","context":"","instruction":"Bloomフィルターインデックスの動作原理は何？","response":"Bloomフィルターは、定義された偽陽性の確率(FPP)を用いて、確実にファイルにデータが無い、あるいは、おそらくファイルにあるということを宣言することでオペレーションを行います。\n\nDatabricksではファイルレベルのBloomフィルター、それぞれのデータファイルには、関連づけられた単一のBloomフィルターインデックスファイルが割り当てられます。ファイルを読み込む前に、Databricksはインデックスファイルをチェックし、インデックスによってファイルがデータフィルターにマッチするかもしれないということが示された場合にのみファイルが読み込まれます。インデックスが存在しない、あるいはクエリーされたカラムにBloomフィルターが定義されていない場合、Databricksは常にデータファイルを読み込みます。\n\nBloomフィルターのサイズは、Bloomフィルターを作成したセット内のエレメント数と必要となるFPPに依存します。FPPが低いほど、エレメントごとに使用されるビットの数は高くなり精度は高まりますが、ディスク容量を多く消費し、ダウンロードが遅くなります。例えば、FPP10%にはエレメントあたり5ビットが必要となります。"}
{"category":"closed_qa","context":"","instruction":"Apache Kafkaは何？","response":"Apache Kafkaは、オープンソースのイベントバスです。Kafkaは、特定の期間でメッセージがバッファされる、追記のみのイベントの分散ログであるトピックの概念を使用しています。"}
{"category":"closed_qa","context":"","instruction":"ストリーミングデータパイプラインはどのように作成するか？","response":"データフローパイプラインにおいては、Delta Live Tablesとそれらの依存関係は標準的なSQLのCreate Table As Select (CTAS)文とDLTのキーワードliveで宣言することができます。\n\nPythonでDLTを開発する際には、Delta Liveテーブルを作成するために@dlt.tableデコレーターを使用します。パイプラインのデータ品質を保証するために、DLTは不正なレコードを持つパイプラインの挙動を定義するシンプルなSQLの制約であるエクスペクテーションを使用します。\n\nストリーミングワークロードは、多くの場合予期できないデータボリュームを伴うので、Databricksでは、不要なインフラストラクチャをシャットダウンすることでコストを削減しつつも、全体的なエンドツーエンドのレーテンシーを最小化するために、データフローに対して強化オートスケーリングを適用します。"}
{"category":"closed_qa","context":"","instruction":"Delta Live Tableはストリーミングエンジンから直接取り込むことができるか？","response":"はい、Pythonで記述されたDelta Live Tableは、Spark構造化ストリーミングを用いることでKafkaのようなイベントバスから直接データを取り込むことができます。コンプライアンスの問題を回避し、コストを削減するためにKafkaのトピックの保持期間を短くすることで、Deltaが提供する安価かつ弾力性のあるガバナンスの効いたストレージのメリットを享受することができます。"}
{"category":"closed_qa","context":"","instruction":"DLTパイプラインにおけるSQLとPythonの混在は問題ありますか？","response":"複数のノートブックからDLTパイプラインを構成することができますが、一つのDLTノートブックは、全体がSQLかPythonで記述される必要があります(単一のノートブックでセルごとに異なる言語を記述できるDatabricksノートブックとは異なります)。\n\nこのため、SQLが好みであるならば、PythonのノートブックでApache Kafkaからデータ取り込みのコーディングを行い、別のSQLノートブックでデータパイプラインの変換ロジックを記述することになります。"}
{"category":"closed_qa","context":"","instruction":"DLTでチェックポイントを設定することが必要でしょうか？","response":"いいえ、Spark構造化ストリーミングでは一度のみの処理を保障するために障害復旧のためにチェックポイントが必要となりますが、DLTは手動の設定や明示的なチェックポイントを必要とすることなしに、自動で状態をハンドリングします。"}
{"category":"closed_qa","context":"","instruction":"中間クラウドオブジェクトストレージを用いたストリーミング取り込みをどのように実施するか？","response":"Databricksワークスペースでは、クラウドベンダー固有のオブジェクトストアは、クラウドに依存しないフォルダーとしてDatabricksファイルシステム(DBFS)にマッピングされます。データがオフロードされたら、Databricksのオートローダーでファイルを取り込むことができます。"}
{"category":"closed_qa","context":"","instruction":"Databricks on Google Cloudは何？","response":"Databricks on Google Cloudは、皆様の分析、AIワークロードの全てを統合するためにデータウェアハウスとデータレイクの良いところを組み合わせたシンプルかつオープンなレイクハウスプラットフォームに皆様のすべてのデータを格納することができる共同開発のサービスです。Google Kubernetes Engine (GKE)で動作し、Google Cloud Platform (GCP)上でホストされており、Google CloudのID、Google Cloud Storage、BigQuery、その他のGoogle Cloudのテクノロジーとのビルトインのインテグレーションを提供します。このプラットフォームによって、データエンジニア、データサイエンティスト、データアナリスト、SecOps、クラウドエンジニアリングを含む企業のいかなるペルソナ間のコラボレーションを実現します。Google Kubernetes Engine (GKE)で動作し、Google Cloud Platform (GCP)上でホストされており、Google CloudのID、Google Cloud Storage、BigQuery、その他のGoogle Cloudのテクノロジーとのビルトインのインテグレーションを提供します。このプラットフォームによって、データエンジニア、データサイエンティスト、データアナリスト、SecOps、クラウドエンジニアリングを含む企業のいかなるペルソナ間のコラボレーションを実現します。"}
{"category":"closed_qa","context":"","instruction":"GCP環境に自分のネットワークを持ち込みたいですが、どの様にセットアップするのでしょうか？","response":"企業のお客様は、GCP環境に自分のデプロイメントを行える機能である顧客管理仮想プライベートクラウド(VPC)を使い始めるべきです。顧客管理VPCを用いることで、マネージドプラットフォームの使いやすさとデフォルトでセキュアなデプロイメントを組み合わせるデータとAIに対するPlatform-as-a-Serviceアプローチを提供しながらも、数多くの社内外のセキュリティポリシーとフレームワークに準拠することができます。"}
{"category":"closed_qa","context":"","instruction":"Delta Lakeへのストリーミングデータのハイレベルのデータ取り込みフロー","response":"様々なデータソースからのストリーミングデータは、レイクハウスに取り込まれる前にメッセージバスやオブジェクトストレージにステージングされます。ステージングエリアからのデータは、レイクハウスにデータを書き込むApache Sparkの構造化ストリーミング(SS)によって処理されます。"}
{"category":"closed_qa","context":"","instruction":"オブジェクトストアからのデータ取り込みにはどういう機能を使えますか？","response":"Auto Loaderを使えます。Auto Loaderはクラウドオブジェクトストレージに新規ファイルが到着するとインクリメンタルに処理を行い、ユーザーはカスタムアプリケーションを開発する必要がないので、ストリーミングデータの取り込みをシンプルなものにします。内部状態を保持することで、これまでに処理したファイルを追跡します。処理に失敗した場合には、最後に処理したファイルからスタートするためにこの状態情報を使用します。さらに、データを再実行、再処理する必要がある場合には、ディレクトリ内の既存ファイルを処理するオプションを提供します。"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderのメリットはどのようなものですか？","response":"Auto Loaderのメリットには以下の様なものがあります。\n\n数十億のファイルを処理できる能力\n計算リソースの最適利用を用いた非同期バックフィル\nパフォーマンスを改善するための最適化ディレクトリ一覧\nスキーマの推定とスキーマドリフトへの対応\n自動ファイル通知サービスを活用することによるコスト効率の高いファイル通知\n"}
{"category":"closed_qa","context":"","instruction":"Auto Loaderはどの様に動作しますか？","response":"Auto Loaderは新規ファイルを検知するための2つのモードがあります。ファイル通知とディレクトリ一覧です。\n\nファイル通知: Auto Loaderは入力ディレクトリからのイベントをサブスクライブする通知、キューサービスを自動でセットアップすることができます。ファイル通知モードは高性能であり、大量のファイルを格納する入力ディレクトリにスケールすることもできますが、追加のクラウドのアクセス権が必要となります。このオプションは、ファイルが語順に到着しない場合には好適なものとなり、キューや通知の明示的なセットアップを不要のものにします。このモードを有効化するには、オプションcloudFiles.useNotificationsをtrueに設定し、クラウドリソースを作成するのに必要なアクセス権を指定します。ファイル通知の詳細についてはこちらを参照ください。\n\nディレクトリ一覧: 新規ファイルを特定する別の方法は、Auto Loaderで設定された入力ディレクトリを一覧するというものです。ディレクトリ一覧モードを用いることで、データへのアクセス権以外の権限なしにAuto Loaderのストリームを起動することができます。Databricksランタイム9.1以降では、Auto Loaderはクラウドストレージにファイルが語順に到着しているかどうかを自動で検知し、新規ファイルを検知するのに必要なAPI呼び出しの総数を劇的に削減します。デフォルトモードでは、7回の連続したインクリメンタルなディレクトリ一覧の後に完全なディレクトリの一覧を行います。しかし、設定cloudFiles.backfillIntervalを用いることで、完全なディレクトリ一覧の頻度を調整することができます。設定cloudFiles.useIncrementalListingを用いることで、インクリメンタルな一覧を明示的にオンオフすることができます。明示的にこの設定が有効化されると、Auto Loaderは完全な一覧をトリガーしません。ディレクトリ一覧の詳細に関しては、こちらをご覧ください。\n\n新規ファイルが発見されると、それらのメタデータはAuto Loaderパイプラインのチェックポイントの格納場所にあるスケーラブルなキーバリューストア(RocksDB)に永続化されます。これは、これまでに処理したファイルの履歴を保持する状態情報として動作します。このパイプラインは、既存ファイルを格納するディレクトリに対するバックフィルとファイル通知を通じて発見される新規ファイルの同時処理の両方を実行します。"}
{"category":"closed_qa","context":"","instruction":"Databricksのローシャッフルマージはどのような機能ですか？","response":"Databricksのローシャッフルマージは、修正されていない行と修正された行を一緒に処理するのではなく、より整流化された別の処理モードで変更されていない行を処理することで優れたパフォーマンスを提供します。これによって、シャッフルされるデータの量が劇的に削減され、パフォーマンスの改善につながります。また、ローシャッフルマージはMERGEオペレーションを実行した後にユーザーがOPTIMIZE ZORDER BYコマンドを再実行する必要性も削減します。"}
{"category":"closed_qa","context":"","instruction":"パフォーマンス強化のためのDatabricksの推奨事項は何？","response":"ソースデータセットのディープコピー、シャローコピーを作成するために、Databricksでテーブルをクローンすることができます。\nコストベースのオプティマイザは、テーブルの統計情報を活用することで、クエリーパフォーマンスを高速にします。\n最適化書き込みと自動ファイルコンパクションを用いることでDeltaテーブルを自動で最適化することができます。これは特に長時間処理を行う構造化ストリーミングジョブでは有用です。\n高階関数は、一般的なSparkオペレーターを持たない多くのオペレーションに対して、ビルトインかつ最適化されたパフォーマンスを提供します。高階関数はユーザー定義関数よりも高いパフォーマンスのメリットを提供します。\nDatabricksでは、配列、struct、JSON文字列を含む複雑なデータ型を操作するためのビルトインのオペレーターと特殊な構文を数多く提供しています。\nレンジを含む、あるいは、大きな偏りを持つjoinの設定を手動でチューニングすることができます。"}
{"category":"closed_qa","context":"","instruction":"モデルのトレーニングを記録するためにどのようにMLflow logging APIを使うか？","response":"PythonでMLflowトラッキングを使い始める最も簡単な方法は、MLflowのautolog() APIを使うことです。それぞれのトレーニングランにおいて記録するメトリクスを制御したい、あるいは、テーブルやプロットの様な追加のアーティファクトを記録したい場合には、このノートブックで説明されている様にmlflow.log_metric()やmlflow.log_artifact() APIを使うことができます。"}
{"category":"closed_qa","context":"","instruction":"モデルのトレーニングをどのように記録するか？","response":"機械学習モデルを作成する際に、mlflow.log_param()、mlflow.log_metric()、mlflow.log_model()、mlflow.log_artifact()を用いて、モデル、モデルのパラメーター、評価メトリクス、その他のアーティファクトを記録します。これらの機能によって、どのパラメーター、メトリクスを記録するのかを正確に制御することができ、テーブルやプロットの様なランの他のアーティファクトを記録することもできます。"}
{"category":"closed_qa","context":"","instruction":"MLflowのオートロギングでは何が自動的に記録されるか？","response":"mlflow.sklearn.autolog()を用いて、パラメーター、メトリクス、モデル自身を記録します。"}
{"category":"closed_qa","context":"","instruction":"H3とは何？","response":"H3は同名のグローバルインデクシングシステムとライブラリとなります。もともとは、空間データパターンを可視化、探索する目的においてUberによって開発されました。グリッドシステムは、表面(この場合は地球の表面)のモザイクを形成するために長方形や三角形の様なシェイプをしようします。階層型グリッドシステムは、異なる解像度でモザイクを形成します(基本的なシェイプは異なるサイズで提供されます)。H3システムは、六角形(一部五角形)を使用する様に設計され、そして階層型システムとして異なる16の解像度を操作することができます。特定の解像度でデータをインデクシングすると、分析に使用する1つ以上のH3のセルIDが生成されます。例えば、解像度15のH3セルは約1m2\n�\n2\n(異なるH3の解像度の詳細についてはこちらをご覧ください)をカバーします。"}
{"category":"closed_qa","context":"","instruction":"なぜH3を使うのか？","response":"H3システムを用いることで、膨大な量のデータを理解できる様になります。例えば、大規模なNYCのタクシー乗降データセットを用いることで、空間的なパターンをより理解するためにデータを空間的に集計することができます。10億以上のオーバーラップするデータを見たとしても、パターンを特定することはできませんが、H3を活用することで即座にパターンが明らかになり、さらならる探索に取り組める様になります。"}
{"category":"closed_qa","context":"","instruction":"Tempoは何をするのか？","response":"エントリーポイントは時系列データフレーム(TSDF)のオブジェクトとなります。タイムスタンプのカラムは必須となります。パーティション、シーケンスカラムはオプションですが、自動特徴量生成を含むさまざまなユースケースで役立ちます。TSDFを用いることで、いくつかのネイティブな機能を活用できる様になります。\n\nAsof join - ソーステーブルの最新のレコードをselectし、ベースのファクトテーブルにマージするためにウィンドウを使用します。\n移動平均 - Approximate Exponential Moving AverageとSimple Moving Averageを利用できます。\n再サンプリング - 頻度に基づくアップサンプリングと集計関数\n"}
{"category":"closed_qa","context":"","instruction":"Tempoはどの様に動作するのか？","response":"データフレームとタイムスタンプのカラム、オプションで1つ以上のパーティションカラム、シーケンスカラムを指定する必要があることがわかります。タイムスタンプカラムはソートに使用されます。オプションのパーティションカラムやシーケンスカラムは特徴量生成に使用されます。タイムスタンプカラムは文字列である必要がありますが、パーティションカラムは文字列あるいは文字列のリストである必要があります。\n\nTSDFをDeltaテーブルに書き込む際には、write関数を呼び出してTDSFとSparkのコンテキスト、Deltaテーブル名と最適化のためのカラム(あれば)を指定します。これによって内部でDeltaテーブルに書き込みが行われます。"}
{"category":"closed_qa","context":"","instruction":"DBFSルートに格納したデータは誰かアクセスできるか？","response":"DBFSルートはワークスペースの全てのユーザーがアクセスできるので、ここに格納されたデータには全てのユーザーがアクセスできます。センシティブなデータを格納する際にこのロケーションを使わないようにユーザーに指示することが重要となります。"}
{"category":"closed_qa","context":"","instruction":"Filestoreディレクトリには何が格納されるのか？","response":"DatabricksのUIからアップロードされたデータやライブラリは、デフォルトでは/Filestoreに格納されます。また、生成されたプロットもこのディレクトリに格納されます。"}
{"category":"closed_qa","context":"","instruction":"databricks-datasetsディレクトリには何が格納されるのか？","response":"Databricksは、このディレクトリで数多くのオープンソースのデータセットを提供しています。Databricksが提供するチュートリアル、デモの多くはこれらのデータセットを参照していますが、皆様ご自身でDatabricksの機能を探索するために活用することもできます。"}
{"category":"closed_qa","context":"","instruction":"databricks-resultsディレクトリには何が格納されるのか？","response":"databricks-resultsには、クエリーの完全な結果をダウンロードすることで生成されるファイルが格納されます。"}
{"category":"closed_qa","context":"","instruction":"'/databricks/init'ディレクトリには何が格納されるのか？","response":"このディレクトリにはグローバルinitスクリプトが格納されます。"}
{"category":"closed_qa","context":"","instruction":"'/user/hive/warehouse'ディレクトリには何が格納されるのか？","response":"デフォルトでは、hive_metastoreのマネージドテーブルがこのディレクトリに格納されます。"}
{"category":"closed_qa","context":"","instruction":"Databricksのルートパスとは何か？","response":"Databricksにおけるルートパスは、実行されるコードによって異なります。\n\nDBFSルートはSparkとDBFSコマンドのルートパスとなります。これには以下が含まれます。\n\nSpark SQL\nSparkデータフレーム\ndbutils.fs\n%fs\nドライバーにアタッチされるブロックストレージボリュームは、ローカルで実行されるコードのルートパスとなります。"}
{"category":"closed_qa","context":"","instruction":"ローカルファイルAPIの制限は何？","response":"以下では、DatabricksランタイムのFUSEにおけるローカルファイルAPI使用時の制限を列挙します。\n\nクライアントサイド暗号化が有効化されたAmazon S3マウントをサポートしていません。\nランダムの書き込みはサポートしていません。ランダムの書き込みが必要となるワークロードにおいては、以下のようにローカルディスクでオペレーションを行い、/dbfsに結果をコピーするようにしてください。"}
{"category":"closed_qa","context":"","instruction":"DBFSで何ができるか？","response":"DBFSは、クラウドオブジェクトストレージのURIを相対パスにマッピングすることで以下のようなメリットを提供します。\n\nクラウド固有のAPIコマンドではなく、ディレクトリとファイルのセマンティクスを用いてオブジェクトストレージを操作できるようになります。\nストレージの資格情報をDatabricksワークスペースのパスにマッピングできるように、クラウドオブジェクトストレージのロケーションをマウントできるようになります。\nオブジェクトストレージにファイルを永続化するプロセスをシンプルにし、クラスター停止時に、安全に仮想マシンとアタッチされたボリュームを削除できるようになります。\nクラスター初期化のためのinitスクリプト、JAR、ライブラリ、設定情報を格納する便利なロケーションを提供します。\nOSSのディープラーニングライブラリを用いたモデルトレーニングの過程で、チェックポイントファイルを保存する便利なロケーションを提供します。"}
{"category":"closed_qa","context":"","instruction":"HorovodRunnerは何？","response":"HorovodRunnerは、Horovodフレームワークを用いたDatabricks上で分散ディープラーニングワークロードを実行するための汎用APIです。"}
{"category":"closed_qa","context":"","instruction":"分散ディープラーニングに何を使えばいいですか？","response":"HorovodRunnerによるディープラーニングの分散トレーニングが可能です。HorovodRunnerを用いることで、HorovodトレーニングジョブをSparkジョブとして起動することができます。"}
{"category":"closed_qa","context":"","instruction":"HorovodRunnerを用いた分散トレーニングプログラムの開発流れを教えてください。","response":"HorovodRunnerを用いた分散トレーニングプログラムを開発する一般的なアプローチは以下の通りとなります。\n\nノードの数を用いて初期化されたHorovodRunnerインスタンスを作成します。\nHorovod usageで説明されている方法に従って、Horovodのトレーニングメソッドを定義し、メソッド内にimport文を追加するようにします。\nHorovodRunnerインスタンスにトレーニングメソッドを引き渡します。"}
{"category":"closed_qa","context":"","instruction":"本番における機械学習 (Machine Learning in Production)の研修ではどういうことを学びますか？","response":"機械学習エンジニア、データエンジニア及びデータサイエンティストが、実験やモデルの管理からさまざまなデプロイ方式と本番環境に関連する様々な課題対応まで、機械学習のライフサイクル全体管理のベストプラクティスを学びます。MLflowを使用してデータ管理、実験追跡、モデル管理などを含むエンドツーエンドでの機械学習モデルの再現性から始め、バッチ、ストリーミング、リアルタイムでモデルのデプロイ、そして監視、アラート、CI/CDへの対応対策を学びます。サンプルコードはすべてのモジュールに付属しています。このコースでは、\n ・まず、データ、モデル、実験の追跡などを含むエンドツーエンドでの再現性に焦点を当て、MLflowを使用した実験プロセスの管理について説明します。\n ・次に、MLflowモデルレジストリへのモデルの登録、artifactsと環境の管理、モデルテストの自動化など、さまざまなダウンストリームのデプロイツールと統合することにより、モデルの運用を学びます。\n ・その次に、バッチ、ストリーミング、およびリアルタイム推論のためのモデルデプロイを実装します。最後に、継続的なインテグレーション、継続的なデプロイメント、モニタリングおよびアラートを含めた本番環境で必要な取組もカバーします。\n このコースを修了すると、機械学習モデルのログ記録、デプロイ、監視を含めたエンドツーエンドのパイプラインの構築が可能になります。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeとは？","response":"Feature Storeは、 企業における全ての特徴量を管理する中央管理型リポジトリ になります。これにより、組織内で 特徴量の共有と探索 が可能になり、モデルのトレーニングと推論をするために、特徴量を算出するための同じコードが使用されていることが確保できます。"}
{"category":"closed_qa","context":"","instruction":"Feature（特徴量）をリアルタイムサービングに使用する必要の場合はどうすればいいですか？","response":"Feature（特徴量）をリアルタイムサービングに使用する必要の場合は、online storeにFeatureをpublishすることができます。"}
{"category":"closed_qa","context":"","instruction":"なぜデータ管理が重要ですか?","response":"データ管理は、エンドツーエンドMLの再現性において見過ごされがちな重要なことです。"}
{"category":"closed_qa","context":"","instruction":"Delta tableでデータのバージョンをどのように管理するのですか?","response":"Delta tableは、新しいデータが書き込まれるたびに自動的にバージョン管理されます。旧バージョンのテーブルへのアクセスは、 display(spark.sql(f'DESCRIBE HISTORY delta.{delta_path}')) を使って戻してほしいバージョンを見つけ、そのバージョンのデータをロードするぐらいに簡単にできます。タイムスタンプを使用して以前のバージョンに戻すこともできます。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeはどのような課題を解決するのに役立ちますか?","response":"MLパイプラインの開発によく直面する重要な課題は、特徴量の再現性とデータの共有です。Feature Storeを利用することで、組織内のユーザーが同じ特徴量計算コードを使用できるようになります。"}
{"category":"closed_qa","context":"","instruction":"データセットのハッシュ化は何に役立ちますか?","response":"2つのデータセットが同じかどうかを確認するのに役立ちます。これはデータの再現性の確認に役立ちます。ただし、2つのデータセットの完全な差分を示すことはできず、スケーラブルなソリューションでもありません。"}
{"category":"closed_qa","context":"","instruction":"Delta tableの詳細を知りたいですが、どこを見ればいいですか？","response":"Himanshu Rajによるこの講演をチェックしてください。https://www.youtube.com/watch?v=DVUwIpIlNss"}
{"category":"closed_qa","context":"","instruction":"Feature Storeの詳細を知りたいですが、どこを見ればいいですか？","response":"このドキュメント（https://docs.databricks.com/machine-learning/feature-store/index.html）では、Feature Storeを利用してパイプラインに対して何ができるかを詳しく説明しています。"}
{"category":"closed_qa","context":"","instruction":"データとMLの再現性とその重要性についてもっと知りたいですが、どこを見ればいいですか？","response":"Mary Grace MoestaとSrijith Rajamohanによるこのブログ記事は、再現可能なデータとモデルを作成するための基礎知識を提供します。https://www.databricks.com/blog/2021/04/26/reproduce-anything-machine-learning-meets-data-lakehouse.html"}
{"category":"closed_qa","context":"","instruction":"MLflow Trackingとは？","response":"MLflow Trackingとは、\n 1. 機械学習に特有のLogging API。\n 2. トレーニングを行うライブラリや環境に縛られない。\n 3 データサイエンスコードの実行であるrunの概念を中心に整備されている。\n 4 Runは実験(Experiments)の中に集約され、一つの実験に多数のrunが含まれます。\n 5 MLflow Serverは多数の実験をホストできます。"}
{"category":"closed_qa","context":"","instruction":"MLflow Trackingでどういう情報が記録されるか？","response":"MLflow Trackingで各runに関して次の情報を記録できます。\n ・Parameters: 入力パラメータのキーと値のペア。例：ランダムフォレストモデルの木の本数。\n ・Metric: 評価指標。例：RMSE、AUC。\n ・Artifacts: 任意形式の任意の出力ファイルで、実験の過程で出力された作成物。 例：画像、モデルのpickleファイル、データファイル。\n ・Source: runした時のソースコード。"}
{"category":"closed_qa","context":"","instruction":"MLflowで過去の実験と実行をどのように検索するか？","response":"ログデータをPythonで再利用するために、過去のrunをプログラム的にクエリします。 クエリするには、 MlflowClient オブジェクトを利用します。"}
{"category":"closed_qa","context":"","instruction":"MLflow trackingでは何をログとして記録できますか？","response":"MLflowは以下のものをログとして取得できます。\n ・Parameter(パラメータ): モデルへの入力　\n ・Metrixs(評価指標): モデルのパフォーマンス　\n ・Artifact(作成物): データ、モデル、画像などあらゆるオブジェクト　\n ・Source（ソースコード）: runした時のコード。gitにリンクされている場合はコミットハッシュを含む"}
{"category":"closed_qa","context":"","instruction":"実験はどのようにLogをとることができますか?","response":"実験では、まずrunを作成し、そのrunオブジェクトにLogging method(例えば run.log_param('MSE', .2) ) を使用することによってログを取得できます。"}
{"category":"closed_qa","context":"","instruction":"ログとして出力されたartifactはどこに保存されますか?","response":"ログとして出力されたartifactは、選択したディレクトリに保存されます。 Databricksでは、DBFS (Databricks ファイルシステム) に保存されます。"}
{"category":"closed_qa","context":"","instruction":"過去のrunを照会するにはどうしたらいいですか?","response":"MlflowClient オブジェクトを使用して照会できます。 これでUIでできることはすべてプログラム的に実行できるため、プログラミング環境から出る必要はありません。"}
{"category":"closed_qa","context":"","instruction":"MLflowのハイレベルな説明がありますか?","response":"2019年のSpark SummitでのSparkとMLflowのクリエイターであるMatei Zahariaの講演を見てください。https://www.youtube.com/watch?v=QJW_kkRWAUs"}
{"category":"closed_qa","context":"","instruction":"様々な機械学習ツールのより大きな背景に関する情報ソースはありますか?","response":"Roaring Elephantポッドキャストのこのエピソードをチェックしてください。https://roaringelephant.org/2019/06/18/episode-145-alex-zeltov-on-mlops-with-mlflow-kubeflow-and-other-tools-part-1/#more-1958"}
{"category":"closed_qa","context":"","instruction":"MLflowに関連するドキュメントはどこにありますか?","response":"ドキュメントはここにあります。https://www.mlflow.org/docs/latest/index.html"}
{"category":"closed_qa","context":"","instruction":"機械学習を学ぶために一般的な学習材料はありますか?","response":"An Introduction to Statistical Learning (和訳：Rによる統計的学習入門)は、機械学習のテーマと基本的なアプローチを理解する入門書としてお薦めします。https://www.statlearning.com/"}
{"category":"closed_qa","context":"","instruction":"Sparkを使った機械学習に関する詳細情報はどこで入手できますか?","response":"機械学習に特化したDatabricks Blogをご覧ください。https://www.databricks.com/blog/category/engineering/data-science-machine-learning"}
{"category":"closed_qa","context":"","instruction":"Hyperoptとは？","response":"Hyperoptはハイパーパラメーターチューニングを自動化するためのツールです。以下のものを介してApache Sparkと統合できるようになりました。\n ・Trials: シングルノードまたは分散のMLモデルの逐次トレーニング (例:MLlib) \n ・SparkTrials: シングルノードモデルの並列トレーニング (例: sklearn)。並列処理の数は、 parallelism パラメータで制御されます。"}
{"category":"closed_qa","context":"","instruction":"MLflowでどのように実行を階層的に管理するか？","response":"MLflowが Nested Run という便利な実行の整理機能を提供しています。Nested Runは、親Runと子Runをツリー構造で整理します。MLflow UIでは、親Runをクリックして展開したら、子Runが表示されます。"}
{"category":"closed_qa","context":"","instruction":"どうやって自動的にメトリック、パラメータおよびモデルをログに記録するか？","response":"MLflow Autologgingを使って自動的に記録することができます。オートロギングを使用すると、明示的なログステートメントを必要とせずに、メトリック、パラメータ、およびモデルをログに記録できます。"}
{"category":"closed_qa","context":"","instruction":"オートロギングをどうやって有効にするか？","response":"オートロギングを有効にするには、次の2つの方法があります。1. トレーニングコードの前にmlflow.autolog()を呼び出します。これにより、インストールしたサポート対象のライブラリをインポートするとすぐに、オートロギングが有効になります。サポートされているライブラリのリストはここにあります。2. コードで使用するライブラリごとに、ライブラリ固有のオートロギング機能を使用します。例えば、 mlflow.sklearn.autolog() を使用してsklearnのMLflow Autologging 機能を有効にします。"}
{"category":"closed_qa","context":"","instruction":"MLflowモデルのフレーバーは何ですか？","response":"MLflow modelsは、機械学習モデルをパッケージングするための約束事のようなものであり、自己完結するコード、環境、モデルを提供します。\n このパッケージに主な抽象化されたのはフレーバーという概念です。フレーバーとはモデルの異なる使い方です。たとえば、TensorFlowモデルはTensorFlow DAG またはPython関数として読み込むことができます。MLflow modelsとして使用すると、両方のフレーバーが利用可能になります。\n python_functionまたはpyfuncというモデルのフレーバーは、モデルを使用する一般的な方法を提供します。\n これにより、モデルのフォーマットを気にせず、Python関数を使用してモデルをデプロイできます。"}
{"category":"closed_qa","context":"","instruction":"MLflow projectはモデルとどう違うのですか?","response":"MLflow projectの役割は、runの再現性とコードのパッケージ化です。 MLflow modelsは、さまざまな環境へデプロイする役割を持っています。"}
{"category":"closed_qa","context":"","instruction":"ML モデルフレーバーって何ですか?","response":"フレーバーは、デプロイツールがモデルを理解するために使用する約束事です。これらフレーバーにより、各ライブラリを実装しなくても、任意のMLライブラリのモデルを適用できるデプロイツールを開発できます。 各トレーニング環境をデプロイ環境にマッピングする代わりに、MLモデルフレーバーがこのマッピングを管理・実施してくれます。"}
{"category":"closed_qa","context":"","instruction":"どうやってモデルに前処理と後処理ロジックを追加すればいいですか?","response":"mlflow.pyfunc.PythonModel を使って拡張したモデルクラスでは、データのロード、前処理、および後処理のロジックを実装することが可能です。"}
{"category":"closed_qa","context":"","instruction":"MLflow modelsの詳細情報はどこで入手できますか?","response":"MLflow のドキュメントをチェックしてください。https://www.mlflow.org/docs/latest/models.html"}
{"category":"closed_qa","context":"","instruction":"MLflow Model Registry (モデル・レジストリ)は何？","response":"MLflow Model Registry (モデル・レジストリ)は、MLflow Modelの完全なライフサイクルを共同で管理するための一元化されたモデルストア、APIのセット、およびUIを揃えているコンポーネントです。モデル・リネージ(どのMLflow Experiment、どのRunで構築されたモデルか)、モデルのバージョン管理、ステージの移行(「Staging」から「Production」への移行など)、アノテーション(コメント、タグなど)、デプロイ管理(どの本番用ジョブが特定のモデルバージョンをリクエストしたかなど)を提供します。"}
{"category":"closed_qa","context":"","instruction":"モデル・レジストリの機能を教えてください。","response":"モデルレジストリには以下の機能があります。\n ・中央リポジトリ: モデルをMLflowモデル・レジストリに登録します。登録済みモデルには、一意の名前、バージョン、ステージ、およびその他のメタデータがあります。\n ・モデルのバージョン管理: 更新時に、登録済みモデルのバージョンを自動的に追跡します。\n ・モデルのステージ: モデルのライフサイクルを表す「Staging」や「Production」など、各モデルバージョンに割り当てられたものです。\n ・モデルステージの移行: モデルを別のステージへ変更することです。その際に、新しいモデルの登録またはモデルのステージの変更を行動履歴として記録し、ユーザー名、変更点、およびコメントなどの追加メタデータを自動的にログに記録します。\n ・CI/CDワークフロー統合: より良い制御とガバナンスを行うため、ステージの移行・変更・レビュー・承認をCI/CDパイプラインの一部として記録します。"}
{"category":"closed_qa","context":"","instruction":"モデルステージはいつく定義されているか。それぞれの意味は何か？","response":"MLflow model registryでは、 None , Staging , Production と Archived という4つのモデルステージが定義されています。各ステージには固有の意味があります。例えば、 Staging はモデルテスト用、 Production はテストまたはレビュープロセスを完了し、アプリケーションにデプロイされたモデル用です。"}
{"category":"closed_qa","context":"","instruction":"MLflow tracking (モデル・トラッキング)とモデル・レジストリはどこか違いますか?","response":"トラッキングは、実験と開発のプロセスの追跡です。 モデル・レジストリは、トラッキングからモデルを取得し、ステージングを経て本番稼働に移行するように設計されています。 これは、データエンジニアや機械学習エンジニアがよくデプロイプロセスに担当することです。"}
{"category":"closed_qa","context":"","instruction":"モデル・レジストリが必要なのはなぜですか?","response":"MLflow trackingが機械学習トレーニング・プロセスのエンドツーエンドの再現性を提供するのと同じように、モデル・レジストリはデプロイプロセスの再現性とガバナンスを提供します。 本番システムはミッションクリティカルなので、コンポーネントはACLで分離できるため、特定のユーザーしかが本番モデルを変更できません。 バージョン管理とCI/CDワークフロー統合は、モデルを本番環境にデプロイする際の重要な役割でもあります。"}
{"category":"closed_qa","context":"","instruction":"UIを使用する場合と比較して、プログラム的に何を実行できますか?","response":"ほとんどの操作はUIまたは純粋なPythonで行うことができます。 モデルのトラッキングはPythonで実施する必要がありますが、それ以降の処理はどちらの方法でも実行できます。 例えば、MLflow tracking APIを使用してログに記録されたモデルをUIで登録し、「Production」にプロモートすることができます。"}
{"category":"closed_qa","context":"","instruction":"MLflow modelの詳細情報はどこで入手できますか?","response":"MLflowのドキュメントをチェックしてください。https://www.mlflow.org/docs/latest/models.html"}
{"category":"closed_qa","context":"","instruction":"Webhookとは？","response":"Webhookとは、イベントが発生した際に、HTTP リクエストを介してアクションを呼び出す一般的なメカニズムです。"}
{"category":"closed_qa","context":"","instruction":"モデル・レジストリのWebhookでできることは？","response":"モデル・レジストリのWebhookにより、テストやデプロイ・パイプラインの実行、通知を送信するプッシュメカニズムの提供が可能になり、CI/CDプロセスを円滑に実行できます。モデルレジストリのWebhookは、新たなモデルバージョンの作成、新規コメントの追加、モデルバージョンのステージ移行などによって起動します。"}
{"category":"closed_qa","context":"","instruction":"MLflowモデル・レジストリのWebhookは何種類あるか？","response":"MLflowモデル・レジストリのWebhookは以下2つのタイプがあります。1. ジョブトリガー付きのWebhook: Databricksワークスペースでジョブをトリガーします。2. HTTPエンドポイントを持つWebhook: 任意のHTTPエンドポイントにトリガーを送信します。"}
{"category":"closed_qa","context":"","instruction":"モデルのバッチ・デプロイにおけて、考慮する必要な事項はなんでしょうか？","response":"以下の事項か、推論結果をバッチ処理でデプロイする最適な方法を決定するのに役に立ちます。1. データがどのようにクエリされるか。 2. データがどのように書き込まれるか。3. トレーニングとデプロイ環境。4. 最終モデルがどのようなデータでトレーニングされるか"}
{"category":"closed_qa","context":"","instruction":"推論の読み込みと書き込みはどのように最適化することができますか？","response":"書き込みは並列分散することで最適化することができます。 Sparkでは、これは、DataFrameのパーティションを管理することを意味しており、作業が均等に分散され、書き込み先のデータベースに最も効率的に接続します。"}
{"category":"closed_qa","context":"","instruction":"モデルのリアルタイム・デプロイの方法は何？","response":"モデルをデプロイする方法はいくつもあります。1. 多くはRESTを使用。2. 基本的なプロトタイプの場合、MLflowは開発時のデプロイ用のサーバーとし利用することができます。MLflowの実装はPythonのライブラリであるFlaskによって支えられています。本番環境での利用を想定したものではありません。 \n さらに、Databricksはマネージドなサービスとして　MLflow Model Serving を提供しています。このソリューションでは、モデルのバージョンとそのステージの可用性に基づいて自動的に更新されるRESTエンドポイントとして、Model Registryから機械学習モデルをホストすることができます。"}
{"category":"closed_qa","context":"","instruction":"リアルタイム・デプロイに最適なツールは何ですか？","response":"これは、何を望むかに大きく依存します。 考慮すべき主なツールは、コードをコンテナ化する方法と、RESTエンドポイントまたは埋め込みモデルのいずれかです。 これらの方法で、リアルタイムのデプロイメントオプションの大部分をカバーしています。"}
{"category":"closed_qa","context":"","instruction":"最適なRESTfulサービスは何ですか？","response":"主要なクラウドプロバイダーはすべて、それぞれのデプロイに関する選択肢を持っています。 Azure環境では、Azure MLは、Dockerイメージを使用してデプロイメントを管理します。これは、あなたのインフラの様々なサブシステムから問い合わせることができるRESTエンドポイントを提供します。"}
{"category":"closed_qa","context":"","instruction":"RESTデプロイのレイテンシーに影響を与える要因は何ですか？","response":"応答時間は、いくつかの要因の関数と考えられます。 バッチ予測は、REST接続のオーバーヘッドを下げることによってスループットを向上させるので、必要なときに使用されるべきです。 地理的な位置も、サーバーの負荷と同様に問題です。 これは、どの地理的場所にデプロイするかと、より多くのリソースによるロードバランシングで対処できます。"}
{"category":"closed_qa","context":"","instruction":"機械学習モデルのドリフト監視で何を監視すべきか？","response":"ドリフトを監視するためには、ドリフトの原因毎にを監視する必要があります。ドリフトはいくつかの種類があります。1. データのドリフト：データ変化、特徴量のドリフト、ラベルのドリフト、予測値のドリフト。2. コンセプトのドリフト: 入力変数と目的変数ラベルの相関の変化、P(Y∣X)の分布の変化、現行モデルが無効になる可能性が高い"}
{"category":"closed_qa","context":"","instruction":"CI/CDとは？","response":"継続的インテグレーションと継続的デプロイは、一般的にCI/CDと呼ばれ、開発と運用の間のギャップを埋めるものです。"}
{"category":"closed_qa","context":"","instruction":"機械学習におけるCI/CDのメリットは何ですか？","response":"機械学習でCI/CDを実践すると、完全に自動化された開発とデプロイのループを閉じることができ、様々な利点が得られます。1. 時間短縮 (Time Savings)。CI/CDがなければ、本番の機械学習アプリケーションの更新は、時間がかかり、人に依存したプロセスになります。\n ・データサイエンティストが新しいモデルを構築することを決定します。\n ・データサイエンティストが手作業で新しいモデルを構築します。\n ・データサイエンティストによるモデルの性能評価を行います。\n ・データサイエンティストが新しいモデルを本番に稼動させるべきと判断します。\n ・データサイエンティストは、機械学習エンジニアに新しいモデルを本番稼動させるべきと伝えます。\n ・機械学習エンジニアが手作業でモデルをテスト環境に移動させます。\n ・機械学習エンジニアがテスト環境内にモデルをデプロイします。\n ・機械学習エンジニアは、テスト環境内でモデルに対して一連の手動テストを実行します。\n ・機械学習エンジニアが手動でモデルを本番環境に移行させます。\n ・機械学習エンジニアが本番環境にモデルを展開します。\n ・データサイエンティストは、モデルのパフォーマンスを繰り返しテストし、アップデートが必要なタイミングを判断します。\n これには、工数がかかります。CI/CDプロセスに従うことで、この時間のかかるワークフローを自動化することができます。これにより、より速いアップデートサイクルができるようになります。\n ・もっと多い最新モデル。\n ・モデル不良などの不具合による悪影響を抑えることができます。\n 2. 一貫性 (Consistency)。自動化された統合とデプロイに従うことで、プロセスの決定ポイントは、一貫性と再現性になります。これはつまり、\n ・各モデルは同じターゲットで作られています。\n ・各モデルに全く同じテストを実施します。\n ・各モデルは、ステージング環境と本番環境に同じように統合されます。\n ・各モデルは全く同じ方法でデプロイされます。\n ・各モデルは、ドリフトを検出するために同じ基準で継続的に評価されます。\n これにより、担当のデータサイエンティストと機械学習エンジニアが異なる（または同じ担当者で工数が異なる）ことによるバイアスが、機械学習アプリケーションに悪影響を与えないようにすることができます。\n 3. ホットフィックスとロールバック (Hotfixes and Rollbacks)　\n ・新コードの統合とデプロイを継続的に行うことのもう一つの利点は、ミスや問題を迅速に修正できることです。これには2つの方法があります。\n ・ホットフィックス(Hotfixes): 実稼働中のソフトウェアアプリケーションのバグを迅速に修正するために書かれた小さなコードの断片。\n ・ロールバック(Rollbacks)：ソフトウェアアプリケーションを、正しく機能した最後のバージョンに戻すこと。"}
{"category":"closed_qa","context":"","instruction":"モデルCI/CDのため役に立つツールはどんなものですか？","response":"機械学習にCI/CDを採用する場合、役立つツールがいろいろとあります。一般に、次のようなカテゴリに分類されます。\n・オーケストレーション：アプリケーションのフローを制御します。\n・Git Hooks: Gitリポジトリで特定のイベントが発生したときに自動的にコードを実行します。\n・アーティファクト管理：パッケージソフトや機械学習モデルなどのアーティファクトを管理します。\n・環境管理：アプリケーションで利用可能なソフトウェアリソースを管理します。\n・テスト：モデルの妥当性と有効性を評価するためのテストを開発します。\n・アラート：特定のイベントやテスト結果が発生したときに、該当のステークホルダーに通知します。"}
{"category":"closed_qa","context":"","instruction":"アラートに使用するツールはどういうものがありますか？","response":"アラートツールには、さまざまなレベルのものがあります。1. PagerDuty。本番システムの停止を監視する最も人気なツールの1つです。テキストメッセージや電話で問題をチーム間にエスカレーションすることができます。2. SlackまたはMicrosoft。3. Teams。4. Twilio。5. メール通知。ほとんどのアラートフレームワークは、REST統合によってカスタムアラートを出すことができます。"}
{"category":"closed_qa","context":"","instruction":"アラートツールの代表的なものは何ですか？","response":"PagerDutyは、本番環境で最も使用されるツールである傾向があります。 また、アラートをメールで送信するSMTPサーバーや、テキストメッセージでアラートを送信するTwilioも人気があります。 Slackのwebhookやbotも簡単に作ることができます。"}
{"category":"closed_qa","context":"","instruction":"Spark Streamingでできることを教えてください。","response":"Spark Streamingでできることは主に以下の２つです。1. スケーラブルかつ耐障害性な運用により、入力データに対して継続的に推論を行います。2. ストリーミングアプリケーションは、ETLやその他のSparkの機能を組み込んで、リアルタイムにアクションを起こすことも可能です。"}
{"category":"closed_qa","context":"","instruction":"ストリーミングに関する特別な課題が何か？","response":"ストリーミングには、特別な課題がいくつもあります。具体的には次のようなものがあります。1. エンドツーエンドの信頼性と正確性： ネットワークの問題、トラフィックの急増、および/またはハードウェアの誤動作によって引き起こされるパイプラインの任意の要素の障害に対する回復力がなければなりません。2. 複雑な変換を処理する： よく複雑なビジネスロジックかつ形式の異なるデータを入力として処理しなければなりません。3. データの遅延と到着順序の外れ： ネットワークの問題により、データの到着が遅れたり、意図した順序から外れたりすることがあります。4. 他のシステムとの統合： アプリケーションは、データインフラストラクチャの他の部分と統合する必要があります。"}
{"category":"closed_qa","context":"","instruction":"一般的なデータストリームは何でしょうか？","response":"Apache Kafkaや、AWS KinesisやAzure Event Hubsなどのクラウド管理型ソリューションが一般的なデータストリームです。 さらに、受信ファイルが入ってくるディレクトリを監視するのもよくあります。 新しいファイルが入ると、そのファイルをストリームに取り込んで処理します。"}
{"category":"closed_qa","context":"","instruction":"SparkはどのようにExactly Once（厳密に1回）」のデータ配信を行い、ストリームのメタデータを維持するのでしょうか？","response":"チェックポイントは、クラスターの状態を維持する能力を通じ、耐障害性を実現します。"}
{"category":"closed_qa","context":"","instruction":"Sparkのストリーミングは、他のSparkの機能とどのように統合されているのですか？","response":"Spark Streamingはバッチ処理と同じDataFrame APIを使用しているため、他のSpark機能との統合が容易に行えます。"}
{"category":"closed_qa","context":"","instruction":"Databricks AutoMLについて","response":"Databricks AutoML は、UIとプログラムのいずれで機械学習モデルを自動的に構築するのに役立ちます。モデル トレーニング用のデータセットを準備し、一連の試行 (HyperOpt を使用して) を実行して記録し、複数のモデルを作成、チューニング、および評価します。"}
{"category":"closed_qa","context":"","instruction":"Databricks File System (DBFS)とは？","response":"Databricks File System (DBFS) は、クラウド オブジェクト ストレージをクラスター上のローカル ファイルおよびディレクトリであるように扱うことができる仮想ファイル システムです。"}
{"category":"closed_qa","context":"","instruction":"SparkSessionとは？","response":"SparkSession クラスは、DataFrame API を使用する Spark のすべての機能への唯一のエントリです。Databricks ノートブックでは、既にSparkSession が作成され、 spark という変数に格納されています。"}
{"category":"closed_qa","context":"","instruction":"DataFramesとは？","response":"DataFrame は、名前付き列にグループ化されたデータの分散コレクションです。"}
{"category":"closed_qa","context":"","instruction":"DataFrameの結果を表示するのに使うメソッドは何？","response":"DataFrameの結果を表示するのにdisplay()を使います。"}
{"category":"closed_qa","context":"","instruction":"データフレームのスキーマを表示するのに使うメソッドは何？","response":"データフレームのスキーマを表示するのにprintSchema() メソッドを使用します。データフレームのschema属性を使用して、DataFrameのスキーマにもアクセスできます。"}
{"category":"closed_qa","context":"","instruction":"データフレームのTransformationsは何？","response":"データフレームのTransformationsは DataFrame を操作して返すため、変換メソッドを連鎖させて新しい DataFrame を構築できます。 ただし、変換メソッドは 遅延評価されるため、これらの操作は単独では実行できません。"}
{"category":"closed_qa","context":"","instruction":"DataFrameActions(アクション)","response":"DataFrame アクションは、計算をトリガーするメソッドです。 DataFrame 変換の実行をトリガーするには、アクションが必要です。"}
{"category":"closed_qa","context":"","instruction":"DataFramesとSQL間の変換はどうすればいいですか？","response":"一つの方法はcreateOrReplaceTempViewで、DataFrame (データフレーム)から一時的なビューを作成します。一時ビューの有効期間は、DataFrame の作成に使用された SparkSession に関連付けられています。"}
{"category":"closed_qa","context":"","instruction":"なぜDelta Lakeなのか？","response":"一言で言えば、Delta Lakeは、データレイクに信頼性とパフォーマンスをもたらす、オープンソースのストレージレイヤーです。Delta Lakeは、ACIDトランザクション、スケーラブルなメタデータ処理を提供し、ストリーミングとバッチデータを統一的に処理します。"}
{"category":"closed_qa","context":"","instruction":"DeltaのTransaction Log(トランザクション・ログ)は何？","response":"DeltaのTransaction Logは、ユーザーがDelta Lakeテーブルを作成すると、そのテーブルのトランザクションログは自動的に_delta_logというサブディレクトリに作成されるものです。そのテーブルを変更すると、全ての変更はトランザクションログとして、順序付きでアトミックにコミットとして記録されます。各コミットは、000000.jsonで始まるJSONファイルとして書き出されます。テーブルを追加変更すると、さらにJSONファイルが生成されます。"}
{"category":"closed_qa","context":"","instruction":"Delta Time Travelを使用して何かできるか？","response":"Delta Time Travelを使用して、デルタテーブルの以前のバージョンにアクセスすることができます。Delta Lakeは、デフォルトで30日間のバージョン履歴を保持しますが、必要であればより長い履歴を保持することもできます。"}
{"category":"closed_qa","context":"","instruction":"データフレームの要約統計を出力するのに使うメソッドは何？","response":"データフレームの要約統計を出力する2つのメソッドがあります。1. describe : count, mean, stddev, min, max 。2. summary : 上記のdescribeの項目 + 四分位範囲(IQR : interquartile range)"}
{"category":"closed_qa","context":"","instruction":"欠損値を扱うには、どうな方法がありますか？","response":"欠損を処理するいくつかの方法があります。1. 欠損を含むレコードはすべて削除する。2. 数値型の場合は平均値/中央値/ゼロ/その他で置き換える。3. カテゴリカル変数の場合は最頻値に置き換えるか欠損を表す表現を用意する。4. 欠損値補完のために設計された ALS (Alternating Least Squares) のようなテクニックを使用する。"}
{"category":"closed_qa","context":"","instruction":"maxBins は、どのようなパラメータでしょうか？","response":"Sparkでは、データは行で分割されます。そのため、分割を行う必要がある場合、各Workerは分割点ごとに各特徴の要約統計量を計算する必要があります。そして、分割するためにこれらの統計情報を（tree reduceによって）集約する必要があります。考えてみてください。Worker1が値 32 を持っているが、他のどのWorkerもその値を持っていなかったとしたらどうなるでしょうか。どれだけ良い分割になるのかどうやって分かりますか。そこで、Sparkには連続変数を離散化してバケットにするためのmaxBinsパラメータを使います。しかし、バケット数は最も基数の多いカテゴリ型変数と同じ大きさでなければなりません。"}
{"category":"closed_qa","context":"","instruction":"Hyperoptとは？","response":"Hyperoptは、「実数値、離散、条件付き次元を含む、厄介な探索空間上でのシリアルおよびパラレル最適化」のためのPythonライブラリです。機械学習ワークフローにおいて、hyperoptは、他のライブラリで利用可能なものより高度な最適化戦略を用いてハイパーパラメータ最適化プロセスを分散/並列化するために使用することができます。"}
{"category":"closed_qa","context":"","instruction":"Apache Sparkでhyperoptをスケールさせる方法を教えてください。","response":"Apache Sparkでhyperoptをスケールさせるには、2つの方法があります。1. シングルマシンのhyperoptで、分散学習アルゴリズム（MLlibなど）を使う。2. 分散hyperoptで、SparkTrialsクラスと一緒にシングルマシンの学習アルゴリズム（scikit-learnなど）を使う。"}
{"category":"closed_qa","context":"","instruction":"hyperoptのハイパーパラメータ探索アルゴリズムは何か？","response":"Hyperoptでは、ランダムサーチまたはTree of Parzen Estimators（TPE）を用いて、ハイパーパラメータのチューニングを並行して行うことができます。TPEはハイパーパラメータ空間を繰り返し探索します。テストされる新しいハイパーパラメータ設定は、過去の結果に基づいて選択される。なのでtpe.suggest はベイジアンの探索方法です。"}
{"category":"closed_qa","context":"","instruction":"hyperoptでのハイパーパラメータ・チューニングをMLflowで追跡することができますか？","response":"はい、MLflowはHyperoptと統合されているため、ハイパーパラメータのチューニングの一環として、学習させたすべてのモデルの結果とその結果を追跡することができます。"}
{"category":"closed_qa","context":"","instruction":"Feature storeのバッチ推論メソッドは何？","response":"Feature Storeに登録したMLflowモデルを使ってバッチ推論をするために、score_batchを使用します。"}
{"category":"closed_qa","context":"","instruction":"Feature Storeの既存特徴量テーブルにカラムを追加することができるか？","response":"はい、できます。overwrite(上書き)で新規特徴量の追加が可能です。"}
{"category":"closed_qa","context":"","instruction":"Apache Arrowは何？","response":"Apache Arrowは、Sparkで使用されてJVM と Python プロセス間のデータをほぼゼロの（デ）シリアライズコストで効率的に転送するためのインメモリ列型データ形式です。詳しくはこちらをご覧ください。https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html"}
{"category":"closed_qa","context":"","instruction":"MLflowでモデルのサービング機能を提供していますか？","response":"はい、MLflow Model Serving機能を提供しています。MLflowでは、モデルをリアルタイムのREST APIとしてデプロイすることができます。"}
{"category":"closed_qa","context":"","instruction":"SparkにおけるPandas API (Pandas API on Spark)は何？","response":"pandas API on Sparkプロジェクトは、Apache Sparkにおけるpandas DataFrame APIを実装することにより、データサイエンティストがビッグデータを扱う際の生産性を高めることを目的としています。2つのエコシステムを使い慣れたAPIに統一することで、pandas API on Sparkは小規模データと大規模データの間のシームレスな切り替えを提供します。PySpark 3.2にマージされたKoalasプロジェクトについてご存知の方もいるかもしれません。Apache Spark 3.2以降では、スタンドアローンのKoalasプロジェクトがメンテナンスモードに入ったため、PySparkを直接ご利用ください。"}
{"category":"closed_qa","context":"","instruction":"Pandas DataFrames、Spark DataFrame、Pandas API on Spark DataFrameの違いは？","response":"Pandas DataFrames は ミュータブル(変更可能) で、強制的に評価され、行の順序を維持します。これらは1台のマシンに限定され、a)に示すようにデータセットが小さい場合に非常に高い性能を出します。\n Spark DataFrameは、分散的、遅延評価され、不変であり、行の順序を維持しません。b)とc)に示したように、大規模データの場合の性能は非常に高いです。\n Pandas API on Sparkは、pandas APIとSparkの性能上の利点という、両者の利点を提供します。しかし、Sparkでネイティブにソリューションを実装するのに比べれば、速度は劣ります。"}
{"category":"closed_qa","context":"","instruction":"Spark DataFrameとpandas API on Spark DataFrameの変換方法を教えてください。","response":"PySpark DataFrameからpandas API on Spark DataFrameに変換するのは、spark_df.to_pandas_on_spark()とps.DataFrame(spark_df)の２つ方法があります。\n Pandas API on Spark DataFrameからSpark DataFrameへ変換するには'df.to_spark()'を使用します。"}
{"category":"closed_qa","context":"","instruction":"Pandas API on Sparkデータフレームの可視化機能は特別なところがありますか？","response":"はい、あります。Pandas API on Sparkでは、可視化の種類に応じて、プロットの実行方法が最適化されています。"}
{"category":"closed_qa","context":"","instruction":"pandas API on Spark DataFramesをSQLでどのように操作するか？","response":"pandas API on Spark DataFramesをSQLで操作する場合、以下の例のように実施することができます。例：「ps.sql('SELECT distinct(property_type) FROM {df}')」"}
{"category":"closed_qa","context":"","instruction":"モデルの主なデプロイ方法は何種類がありますか？","response":"モデルのデプロイ方法は主に4つあります。1. バッチ・プリ・コンピュート, 2. 構造化ストリーミング, 3.低レイテンシーモデルサービング, 4. モバイル／エッジ推論"}
{"category":"closed_qa","context":"","instruction":"時系列 (Time Series)とは？","response":"時系列とは、時間順に索引付け（またはリスト化、グラフ化）された一連のデータポイントのことです。最も一般的な時系列は、時間的に等間隔に連続して取得されたデータ系列です。したがって、これは離散時間データのシーケンスです。時系列の例：海洋潮汐の高さ、太陽黒点の数、ダウ・ジョーンズ工業の平均株価の日次終値"}
{"category":"closed_qa","context":"","instruction":"Prophetとは？","response":"ProphetはFacebookのProphetパッケージです。Prophetは、ユーザーに代わってヘビーな作業を行ってくれるため、時系列予測タスクによく採用されています。"}
{"category":"closed_qa","context":"","instruction":"ARIMAとは？","response":"ARIMAとは、Auto-Regressive (AR) Integrated (I) Moving Average (MA)の略です。ARIMAモデルは回帰分析の一種で、データ系列のある点とその直近時点の値との関係性を分析する手法です。\n ARIMAは、Prophetと同様に、データセットの過去の値に基づいて将来の値を予測します。ARIMAはプロフェットと違い、設定作業が多くなりますが、様々な時系列に適用することが可能です。\n ARIMAモデルを作成するために、以下のパラメータを求める必要があります。\n ・p (自己回帰パラメータ) :モデルに含まれるラグ観測の数で、ラグ次数とも呼ばれる。\n ・d (差分の階数):生の観測値が差分される回数で、差分化の度合いとも呼ばれます。\n ・q (移動平均パラメータ):移動平均の窓の大きさで、移動平均の次数とも呼ばれる。"}
{"category":"closed_qa","context":"","instruction":"指数平滑化 (Exponential Smoothing)","response":"指数平滑化とは、指数窓関数を用いて時系列データを平滑化する経験則の手法です。単純移動平均では過去の観測値が均等に重み付けされるのに対し、指数関数は時間の経過とともに指数的に減少する重みを割り当てるために使用されます。季節性など、ユーザーによる事前の想定に基づいて何らかの判断を行うための手順で、簡単に習得でき、簡単に適用できます。時系列データの解析には、指数平滑化がよく使われます。"}
{"category":"closed_qa","context":"","instruction":"指数平滑化には何種類がありますか？","response":"指数平滑化には3つのタイプがあります。1. 単純指数平滑化(SES), トレンドや季節性のないデータセットに使用します。2. 二重指数平滑化（別名：ホルトの線形平滑化）, トレンドはあるが季節性がないデータセットに使用します。3. 三重指数平滑化（ホルト・ウィンタース指数平滑化とも呼ばれる）, トレンドと季節性の両方を持つデータセットに使用されます。"}
{"category":"closed_qa","context":"","instruction":"ジョブの実行速度が遅い場合はどういう原因がありますか？","response":"ジョブの実行速度が遅い場合の最も一般的な要因：1. Spill (スピル、メモリ溢れ): メモリーからディスクにデータが溢れること。解決策：より多くのメモリを持つクラスタを使用する。2. Shuffle : 大量データがクラスタ間で転送されていること。 解決策：Joinを最適化するか、シャッフルを回避するためにコードをリファクタリングする。3. スキュー/ストラグラー : パーティショニングされたデータ（ファイルまたはメモリ内）が均等に分散されていない、一部のパーティションの実行に時間がかかる「最後のreducerの呪い」が発生します。 解決策：使用可能なコアの倍数に再分割するか、スキューヒントを使用する。4. Small/Large Files : 小さなファイルが多すぎて、各ファイルの読み込みに専用のスレッドが必要なため、クラスタのリソースが枯渇しているか、大きなファイルのせいで未使用のスレッドが発生しています。 解決策：より最適な方法でデータを書き直すか、Deltaファイルのコンパクションを実行する。"}
{"category":"closed_qa","context":"","instruction":"ジョブの実行速度が遅いなどの場合のデバッグのツールは何？","response":"ジョブの実行速度が遅いなどの場合のデバックツールはいくつあります。1. CPU、ネットワーク、メモリのリソースをクラスタまたはノードレベルで管理するGanglia, 2. その他を監視するSpark UI（特にストレージとエグゼキュータータブ）, 3. ドライバやワーカーのログにエラーがないか（特にバックグラウンドプロセスの場合）, 4. クラスタ・セクションのノートブックタブで、インターンが再びクラスタを占拠しているかどうか"}
{"category":"closed_qa","context":"","instruction":"クラスタータイプをどう選択するか？","response":"クタスターの構成について一般的な経験則は以下です。1. 機械学習用に大きなマシンタイプの小型クラスタ, 2. 本番ワークロードごとに1クラスタ, 3. ML学習用のクラスタを共有しない（開発中であっても）。詳しくはドキュメントをご覧ください。https://docs.databricks.com/clusters/configure.html"}
{"category":"closed_qa","context":"","instruction":"ライブラリインストールのベストプラクティスを教えてください。","response":"以下はライブラリインストールのベストプラクティスです。1. Notebook-scoped Python libraries は、同じクラスタのユーザが異なるライブラリを持つことができることを保証します。 また、ライブラリに依存するノートブックを保存するのにも便利です。2. 初期化スクリプトは、JVMの起動前にコードが実行されることを保証します（特定のライブラリまたは環境構成に適しています）。3. いくつかの構成変数は、クラスタ起動時に設定する必要があります。"}
{"category":"closed_qa","context":"","instruction":"ジョブのベストプラクティスを教えてください。","response":"以下はジョブのベストプラクティスです。1. ノートブックワークフローを使用します。2. ウィジェットはパラメータ渡しのために使用します。3. jars や wheelsも実行することができます。4. オーケストレーションツール（Airflowなど）のCLIを利用します。5. 詳しくはドキュメントをご覧ください。https://docs.databricks.com/workflows/jobs/jobs.html 6. 無限にジョブが実行されるのを防ぐため、必ずタイムアウト時間を指定します。"}
{"category":"closed_qa","context":"","instruction":"Gitの統合方法は？","response":"Gitの統合は、いくつかの方法で実現できます:1. CLIを使ってノートブックをインポート/エクスポートし、gitに手動でチェックインする。2. ビルドインの git統合機能を使用する。3. プロジェクト統合の代替に次世代ワークスペースを使用する。"}
